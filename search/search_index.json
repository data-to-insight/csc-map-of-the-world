{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CSC Knowledge Base Network","text":"<p>A structured, open-source knowledge base and ecosystem map for the Children\u2019s Social Care (CSC) sector. This project brings together [Documentation], [Relationships], [Services], [Sector_Tools], [Rules], [Plans], [Events] using a flexible YAML-based data model and aiming for alignment with the Smart City Concept Model (SCCM) towards sector data interoperability.</p> SCCM Concept (Category) Suggested example(s) (in progress) Community South East fostering cluster Documentation CSC Independent Review Events Children\u2019s Social Care Review, ILACS Inspections, Public Inquiries Organization Data to Insight, LIIA Plans Kinship Care Strategy, Children\u2019s Social Care National Framework Relationships LA-1 \u2194 Supports \u2194 SSD Tests, DfE \u2194 Pilots \u2194 API Data Flows Rules Statutory Guidance, Keeping Children Safe in Education 2025 Sector Tools PATCH, ChAT Services <p>It aims to support full-text search, (filtered)graph-based relations visualisation, and YAML schema validation across structured <code>.yml</code> records. Supporting documents (PDF, Markdown, HTML, Python, JS). Development is scaffolded/designed to be extensible, transparent, and Git-native.</p>"},{"location":"#current-dev-phase-discovery-alpha","title":"Current Dev Phase: Discovery-Alpha","text":""},{"location":"#what-is-this-for","title":"What is this for?","text":"<p>This proof of concept(PoC) project explores how a structured, searchable map of Children\u2019s Social Care (CSC) data, tools, and activity could support work across the sector. While exact use-cases are still emerging, our goal is to create a shared resource that brings together:</p> <ul> <li>published reports  </li> <li>pre-defined data objects  </li> <li>web data (e.g. from the DfE, local authorities, third parties)  </li> <li>sector-developed tools and frameworks  </li> </ul> <p>All of this would be made accessible through a visual or navigable interface, allowing users to explore connections between people, projects, standards, and services.</p> <p>We think this could help:</p> <ul> <li>Make relationships clearer \u2014 between local and national CSC initiatives, policies, systems, and data sources  </li> <li>Show who\u2019s doing what \u2014 helping users track new tools, updates to frameworks, or structural changes in services  </li> <li>Bring siloed or under-the-radar work into view \u2014 so efforts can align, build on each other, or avoid duplication  </li> <li>Support local teams \u2014 by contributing to a more joined-up picture of activity across the sector  </li> </ul> <p>We see this as a collaborative mapping tool, developed potentially with input from local authority teams, analysts, service leads, academic partners, and national bodies.</p>"},{"location":"#who-is-this-for","title":"Who is this for?","text":"<p>We envisage use-cases from:</p> <ul> <li>Local authority data and performance teams</li> <li>Children\u2019s social care service managers and strategic leads</li> <li>Academic researchers and national analysts</li> <li>Project leads, developers and architects working in CSC data or digital delivery</li> </ul>"},{"location":"#plandev-scope","title":"Plan/Dev Scope?","text":"<ul> <li>Interactive network map: Navigate to Network to view entities, relationships, and systems as a live graph</li> <li>Structured data records: Underpinning the map is a growing library of structured YAML records, aligned to a SCCM concept framework(BSI as PAS 182) that describe:</li> <li>Tools and systems (e.g. PATCH, Validator 903)</li> <li>Frameworks and inspections (e.g. Ofsted ILACS, JTAI)</li> <li>Relationships and service models</li> <li>Rules, plans, events and guidance</li> <li>Searchable resource: The search page enables you to explore the structured data model directly.</li> <li>This is separate from the standard MkDocs search (top-right), which only covers page text within this site.</li> <li>The CSC knowledge search indexes structured YAML content as well as <code>.md</code>, <code>.pdf</code>, <code>.py</code>, <code>.js</code> and <code>.html</code> files, and supports keyword relevance, match scoring, and metadata extraction.</li> <li>(in dev)The search index currently takes a data sample direct from local authority web sites. At the moment this is throttled to ~10, but with the potential to extract simplistic reference resource(s) directly from all ~153</li> <li>Documentation hub: Local documentation from D2I projects(Git repos) is also indexed to provide technical context</li> </ul>"},{"location":"#how-is-this-structured","title":"How is this structured?","text":"<p>Records in this tool are aligned with the Smart City Concept Model (SCCM), an open framework for describing public service ecosystems. Every entity towards the documented network(diagram) is represented as a YAML file using one or more of the following types:</p> <ul> <li><code>@type: AGENT</code> \u2013 people, teams, or organisations</li> <li><code>@type: SERVICE</code> \u2013 a system, service or tool</li> <li><code>@type: EVENT</code> \u2013 events such as inspections, launches, reviews</li> <li><code>@type: RULE</code>, <code>@type: PLAN</code>, <code>@type: COLLECTION</code> \u2013 policy elements, datasets or strategies</li> <li><code>@type: RELATIONSHIP</code> \u2013 links between entities (e.g. oversight, supply, influence)</li> </ul> <p>These files are validated, searchable, and designed to be easy to contribute to.</p>"},{"location":"#how-can-i-get-involved","title":"How can I get involved?","text":"<p>This project is being developed with and for the sector. We welcome:</p> <ul> <li>Feedback on what\u2019s useful or missing(or broken)</li> <li>Contributions of local projects or documentation</li> <li>Suggestions for how the tool could better support planning and analysis</li> </ul> <p>To contribute or get involved, please contact the Data to Insight team or fork from/visit the GitHub repo.</p>"},{"location":"#foundations-and-inspiration","title":"Foundations and Inspiration","text":"<p>This tool builds on the thinking behind platforms like the Children\u2019s Services Network and is grounded in open modelling approaches like the Smart City Concept Model.</p> <p>It is designed to be lightweight, transparent, and openly extensible \u2014 enabling others to adopt or adapt it for their own contexts.</p>"},{"location":"#whats-next","title":"What\u2019s next?","text":"<ul> <li>Ongoing expansion of linked tools, rules, and frameworks</li> <li>Live|scheduled scrapes from key web resources or published docs/framesworks</li> <li>Search and filter interface (in beta, but aiming to implement network diagram filters)</li> <li>Option for local teams to submit structured entries or link live repositories</li> <li>Export options for integration into other data tools</li> </ul> <p>Thanks for the interest in CSC Knowledge Base We hope it supports your work, and welcome your feedback as we continue to improve and expand it.</p>"},{"location":"network/","title":"CSC Network Graph (in dev)","text":""},{"location":"search/","title":"CSC Search","text":"Search the knowledge base <p>JavaScript is required to use search function.</p>"},{"location":"D2I_Contacts_Processing/","title":"Contacts processing to maintain d2i members","text":"<p>A browser-based process towards enabling and maintaining a clean(er) and reliable members and contact record; reducing|removing our need to use Wix as core contacts management. Uses stlite to run Streamlit/pyodide(in stlite) directly in browser and embeds views via <code>&lt;iframe&gt;</code> containers into the html page(s). Currently only one tool is defined/requested but this might be extended to include others as needed. </p> <p>Tool(s) available via : Contacts Tool(s)</p>","tags":["d2i","loader","validation","internal"]},{"location":"D2I_Contacts_Processing/#problem","title":"Problem","text":"<p>Our contacts/members continue to sign up via our web platform Wix, and through the signup process gain access to both the main tools, and if relevant seperate access to such as the Early Help/Apprenticeships groups. Our outgoing contact/updates/newsletters have historically/typically gone out to our entire contacts list, or a (Wix)identifiable group within. However, we increasingly want to reduce both unneccessary/excess/invalid contact with some members and improve individual update quality and relevance of it. Our current data(and Wix limited processing) is proving to be problematic, uncertain and has a bloated time-cost overhead for simple tasks(incl. filtering). </p> <p>One solution is to clean the existing data and address the shortfalls within Wix itself. However within this approach is the potential for introducing new problems specifically around tool members access tied to Wix internals incl. contact labelling, and the poor time-efficiency when handling contacts via the Wix interface(e.g. paginated views only) as well as the difficulty around reliably grouping contacts - and access to user defined groupings in exports. Alongside ongoing conversations with Wix to identify and enable work-arounds to improve legacy contacts handling/enrichment/sepertion, we have made a long-term plan to move away from Wix to a clean d2i site build(most likely using Django). As such this tool supports any interim move to manage contacts outside of Wix, concurrently maintain/refresh members details (as Wix remains our members' single sign-up entry point).</p>","tags":["d2i","loader","validation","internal"]},{"location":"D2I_Contacts_Processing/#issues-summarised","title":"Issues summarised","text":"<ul> <li>Searches in Wix limited to complete-email|name only</li> <li>Use of custom labelling in Wix is problematic on export/import</li> <li>Wix forces a paginated view of members, which as d2i members base expands is increasingly slow to work with </li> <li>New/Legacy labelling within Wix means we now have unreliable single source of truth</li> <li>Filtering some specific groups e.g. Members-NVEST only has heavy time-cost overhead, unreliable</li> <li>Enriching members data is not easily possible as export/import limits fields</li> <li>Data fixes/corrects are record by record </li> <li>Ineffective Wix members overview enables duplicate records</li> <li>Newsletter rejections can only be handled 1:1 with manual deletion in Wix (we have PoC scripted versions towards this)</li> <li>Re-imports back into Wix problematic, custom field loss</li> </ul>","tags":["d2i","loader","validation","internal"]},{"location":"D2I_Contacts_Processing/#repo-structure","title":"Repo structure","text":"<ul> <li><code>index.html</code>:   Landing page, currently with: </li> <li> <p>Left: <code>upload_tool.html</code> (iframe)  </p> </li> <li> <p><code>upload_tool.html</code>:   HTML file embedding a stlite-powered Streamlit app</p> </li> <li> <p><code>assets/</code>:   Img assets (e.g., <code>images/d2i_logo.png</code>)   Styling assets (e.g., <code>css/styles.css</code>)</p> </li> </ul>","tags":["d2i","loader","validation","internal"]},{"location":"D2I_Contacts_Processing/#tech-stack","title":"Tech stack","text":"<ul> <li>Streamlit for data interface  </li> <li>stlite to run Streamlit app directly in browser (Pyodide)  </li> <li>Bootstrap 5 for layout  </li> </ul>","tags":["d2i","loader","validation","internal"]},{"location":"D2I_Contacts_Processing/#dev-notes-testing","title":"Dev notes &amp; testing","text":"<ul> <li>At the moment py code is embedded into index/upload_tool page, so can't be tested via usual streamlit local running. Instead run: python -m http.server 8000, or select a different port/py: python3 -m http.server 8501</li> </ul>","tags":["d2i","loader","validation","internal"]},{"location":"D2I_Contacts_Processing/#deploymentdevelopment-notes","title":"Deployment/Development notes","text":"<p>Built to run in GitHub Pages.  Note: avoid direct browser refresh on subpages (e.g., <code>/upload_tool.html</code>) unless served (e.g., via iframe or root-relative path) - as i've not added # or 404 handling</p>","tags":["d2i","loader","validation","internal"]},{"location":"D2I_Linux_IMG/","title":"D2I Linux Alpha Build (v0.1.0)","text":"<p>Ubuntu-based ISO for D2I tooling and testing. This is an early attempt to build, deploy(to anyone who'd like to try it) and test a Linux desktop environment preconfigured with key apps towards a data-orientated workflow(incl. D2I's) straight out of the box. An exploratory and very much in-progress investigation into what a D2I open source plug-and-play data-suite might look like for our own team and for any interested data colleagues in local authorities. </p> <p>Now a working build from release v0.1.0 Download D2I Linux v0.1.2</p>"},{"location":"D2I_Linux_IMG/#why-build-a-custom-linux-iso-for-d2i","title":"Why Build a Custom Linux ISO for D2I?","text":"<p>Across lots of sectors teams like ours are increasingly exploring more sustainable, cost-effective and open approaches to tooling. This Linux build reflects that shift. We're asking ourselves, can we:</p> <ul> <li>Put together a free, open-source alternative for D2I use and potentially local authority (data)colleagues</li> <li>Package key D2I tools in a ready-to-use format (ideally that will run on any hardware)</li> <li>Make it easier for colleagues to try, adopt, (enjoy?) and understand open-source tools like Python, Anaconda, Jupyter, and LibreOffice</li> <li>Encourage open standards, data transparency, and independence from software lock-in</li> </ul>"},{"location":"D2I_Linux_IMG/#common-open-source-benefits","title":"Common Open Source Benefits","text":"<ul> <li>Cost saving: No licence fees or platform lock-in  </li> <li>Transparency: Code and tools can be audited and understood  </li> <li>Security: Community-tested, widely deployed systems  </li> <li>Portability: Run on almost any hardware  </li> <li>Adaptability: Customise, rebuild and redistribute legally  </li> <li>Shared knowledge: Aligns with our principles of collaboration and insight-sharing  </li> </ul> <p>Some public teams are already taking steps to switch away from proprietary platforms, e.g.:</p> <ul> <li>\u201cWe\u2019re done with Teams\u201d \u2014 German state uninstalls Microsoft tools</li> <li>Denmark exploring open source alternatives to Microsoft</li> </ul> <p>By offering a Linux-based, D2I-ready environment, we provide a small but practical step toward possible wider open adoption \u2014 while also improving our own portability and resilience. We're learning by doing on this one; but there is potential future value in putting together an in-one desktop solution for both D2I or local authority use. Feedback welcomed accross the board. </p>"},{"location":"D2I_Linux_IMG/#what-the-buildsoftware-includes","title":"What the build/software includes","text":"<ul> <li>Ubuntu 24.04 LTS (Noble Numbat) base</li> <li>XFCE desktop environment (fast/ minimal </li> <li>Live Mode: XFCE GUI with auto-login (<code>d2iuser</code>) on initial use)</li> <li>D2I wallpaper (placeholder atm)</li> </ul>"},{"location":"D2I_Linux_IMG/#preinstalled-apps","title":"Preinstalled Apps","text":"Application Purpose Firefox ESR Web browser LibreOffice Office suite (Calc as initial Excel replacement) Slack Team comms (installed via <code>.deb</code>) (autostart on live boot or first login) Thunderbird Email client (autostart on live boot or first login) Anaconda Python data environment, includes Jupyter (installed via custom script) Zenity Simple GUI dialogs for scripts curl, wget, git Core CLI tools for dev and data use Element (Matrix) Decentralised chat app, alternative to Teams or Slack OpenVPN Support VPN client support via NetworkManager plugins (setup guide)"},{"location":"D2I_Linux_IMG/#preinstalled-tweaks","title":"Preinstalled tweaks","text":"Feature Description Wallpaper (<code>d2i-wallpaper.png</code>) in <code>/usr/share/backgrounds/</code> Welcome Msg First login popup: \u201cYour new D2I build has installed.\u201d Autostart Apps Slack and Thunderbird autostart on live boot or first login Firefox Homepage https://www.datatoinsight.org Live Session Autologin User <code>d2iuser</code> logged in automatically in live session"},{"location":"D2I_Linux_IMG/#data-science-environment","title":"Data Science Environment","text":"<p>D2I Linux build includes Python tools. Access via a post-install hook using <code>pip</code> inside the Anaconda environment. Anaconda is preinstalled in <code>/opt/anaconda3</code> and added to the system PATH. To start working with notebooks <code>jupyter notebook</code>(this opens the browser as per ESCC current set up)</p>"},{"location":"D2I_Linux_IMG/#preinstalled-python-packages","title":"Preinstalled Python Packages","text":"Package Purpose <code>pandas</code> Data manipulation and analysis <code>numpy</code> Numerical ops <code>matplotlib</code> Plots and visualisation <code>seaborn</code> Statistical visuals <code>openpyxl</code> Excel file reading/writing (xlsx) <code>scikit-learn</code> Machine learning tools <code>statsmodels</code> Stats modelling and tests <code>jupyter</code> Notebooks for data workflows <p>Available by default via <code>python</code> or <code>jupyter notebook</code> from terminal or XFCE menu.</p>"},{"location":"D2I_Linux_IMG/#further-notes","title":"Further notes","text":"<ul> <li> <p>Add to or change the build   Outlines how to add packages, troubleshoot common issues, and release updated ISO versions</p> </li> <li> <p>Our/D2I learning   We've experienced issues during our learn-by-doing process up to release 1.0, and we've added/formatted some of that for ref here in case useful to others.  </p> </li> </ul>"},{"location":"D2I_Linux_IMG/#how-to-quick-view","title":"How to (Quick view)","text":"<ol> <li>Download latest release/ver (see below for intructions about Downloading ISO)</li> <li>Write ISO file to usb stick using a tool like Rufus, Etcher or <code>dd</code> command</li> <li>Boot PC or virtual machine from usb or ISO file</li> <li>Use in live Mode (no changes saved) or install to disk for full access</li> </ol>"},{"location":"D2I_Linux_IMG/#how-to-expanded-view","title":"How to (Expanded view)","text":"<p>A mini-guide shoiuld help you run D2I Linux build from USB stick in Live Mode (i.e. try it without installing). </p>"},{"location":"D2I_Linux_IMG/#1-download-the-iso","title":"1. Download the ISO","text":"<p>Go to the Latest Release and download the ZIP or TAR.GZ file. Inside you'll find the ISO file (e.g. <code>d2i-custom.iso</code>)</p> <p>Unzip or extract it so you can access <code>.iso</code> file directly.</p>"},{"location":"D2I_Linux_IMG/#2-write-the-iso-to-a-usb-stick","title":"2. Write the ISO to a USB stick","text":"<p>Choose one of the following methods:</p>"},{"location":"D2I_Linux_IMG/#opt-a-use-gui-recommended-for-beginners","title":"Opt A: Use GUI (recommended for beginners)","text":"<ul> <li>Rufus (Windows)  </li> <li>Balena Etcher (Windows, macOS, Linux)</li> </ul> <p>Steps: 1. Plug in a USB stick (least 4GB) 2. Open tool and select ISO file 3. Choose USB device 4. Click \"Flash\" or \"Start\" to write</p>"},{"location":"D2I_Linux_IMG/#opt-b-use-dd-command-linuxmacos-only-advanced","title":"Opt B: Use <code>dd</code> command (Linux/macOS only - advanced)","text":"<pre><code>sudo dd if=d2i-custom.iso of=/dev/sdX bs=4M status=progress &amp;&amp; sync\n</code></pre> <p>Replace <code>/dev/sdX</code> with correct USB device name (be careful \u2013 will erase disk).</p>"},{"location":"D2I_Linux_IMG/#3-boot-from-usb","title":"3. Boot from USB","text":"<ol> <li>Insert USB stick into target machine.</li> <li>Reboot and enter BIOS/boot menu (usually by pressing F12, F2, ESC, or DEL).</li> <li>Select your USB stick as boot device.</li> </ol>"},{"location":"D2I_Linux_IMG/#4-use-d2i-linux","title":"4. Use D2I Linux","text":"<p>Options then visible: - Live Mode: Try system without making changes to your computer (for testing) - Install Mode: Permanently install system on hard drive (for full-time use)</p>"},{"location":"D2I_Linux_IMG/#downloading-iso","title":"Downloading ISO","text":"<p>Download D2I Latest version Note: ISO file located inside archive (either ZIP or TAR.GZ) and is not shown as separate download file.</p> <p>To extract ISO:</p> <ol> <li>Download the ZIP or TAR.GZ file from release (Git doesnt allow direct upload of <code>.iso</code> files, so ISO is bundled inside <code>.zip</code>|<code>.tar.gz</code> archive for compatibility)</li> <li>Extract it on your pc</li> <li>Locate ISO at <code>output/d2i-custom.iso</code> inside extracted folder</li> <li>You can boot this ISO in VirtualBox, write it to USB with BalenaEtcher, or install it on pc</li> </ol>"},{"location":"D2I_Linux_IMG/#whats-an-iso","title":"What's an ISO?","text":"<p>An ISO file is a single archive that contains the entire contents of a disc or bootable installation image. In this case, it includes the full D2I Linux operating system \u2013 preconfigured with desktop, tools(increasing as we decide on what our core set is), and settings.</p> <p>You can use the ISO to:</p> <ul> <li>Boot a virtual machine (e.g. VirtualBox, QEMU)</li> <li>Create a bootable live USB (using BalenaEtcher, Rufus, etc)(try without installing)</li> <li>Install the system on any PC </li> <li>Give us an open source, consistent, customisable, ready-to-go Linux environment for the d2i team</li> </ul>"},{"location":"D2I_Linux_IMG/#first-boot","title":"First Boot","text":"<p>When run via VirtualBox, QEMU, or direct install, you (should) see:</p> <ul> <li>XFCE GUI with D2I wallpaper</li> <li>App menu incl Firefox, LibreOffice, Slack, Thunderbird</li> <li>Slack and Thunderbird autostart</li> <li>D2I welcome popup on first boot</li> </ul>"},{"location":"Demand_Model/","title":"Children's Social Care Demand Model","text":"<p>This is the Python implementation of the Children's Social Care Demand Model.</p> <p>It can be used as a library, as a command line tool, or as part of a web application. </p> <p>It can also be used within Pyodide and has a partnering front-end application here: </p> <p>https://github.com/data-to-insight/cs-demand-model-web</p>"},{"location":"Demand_Model/#principles","title":"Principles","text":"<p>The model is designed to run of the SSDA903 returns, and requires several years' worth of  data to be able to build a model for the system behaviour. </p> <p>To create the model we take the standard SSDA903 Headers and Episodes files and merge several years of these to create a single dataset. This dataset is then analysed to give a longitudinal view of the children's experience within the system. Most notably we look at transfers between different types of placement categories (fostering, residential etc.), as well as grouping the children by age. </p> <p>We then build a Stock and Flow model of the system. We use the episode start and end  dates to calculate the number of children in care at any given stage (stock), and when one episode ends and  another begins, we look at the type of placement the children transfer between (flow), also taking into account new children entering the system, as well as children leaving care, either returning home or ageing out of the system. </p> <p>You can read more about the Data Analysis and how the data is transformed and analysed.</p>"},{"location":"Demand_Model/#technical-components","title":"Technical Components","text":"<p>Model components:</p> <ul> <li>Configuration - How to configure the tool</li> <li>File Loader - How to load files into the tool</li> <li>Data Container - How we enrich and access the data from the model</li> <li>Predictor - takes the model and uses it to predict the number of children in care at a given point in time.</li> </ul> <p>The components are designed to be re-usable and extensible. </p> <p>In addition there are a couple of abstraction layers to handle loading of files in different environments. </p>"},{"location":"Demand_Model/#quickstart","title":"Quickstart","text":"<p>Want to get started straight away? You can install from the GitHub repo and run from the command line:</p> <pre><code>\npip install 'cs-demand-model[cli]'\n\n</code></pre> <p>The part after the # is optional, but will install a few extra dependencies to make the experience better when  running from the command line.</p> <p>You can now view the command line options by running:</p> <pre><code>demand-model\n````\n\nFor example, you can run a quick predictive model using a sample dataset with:\n\n```bash\ndemand-model predict sample://v1.zip\n</code></pre> <p>In this case we have used a sample dataset, but you can also use a local folder by specifying the path to the folder:</p> <pre><code>demand-model predict path/to/my/folder\n</code></pre> <p>The folder currently needs to have quite a specific structure with sub-folders for each year. You can make sure your folder is read correctly by running:</p> <pre><code> demand-model list-files sample://v1.zip \n</code></pre> <p>(obviously replacing the path with your own)</p> <p>You can get more information about passing options to the command line tool by adding <code>--help</code> after the command, e.g.</p> <pre><code>demand-model analyse --help\n</code></pre>"},{"location":"Demand_Model/#launching-with-jupyter","title":"Launching with Jupyter","text":"<p>You can also launch the model with Jupyter. Install the library with the jupyter extension:</p> <pre><code>pip install 'cs-demand-model[jupyter]'\n</code></pre> <p>Then launch Jupyter:</p> <pre><code>jupyter-lab\n</code></pre> <p>(or <code>jupyter notebook</code> if you prefer)</p>"},{"location":"Demand_Model/#launching-with-jupyter-lite-pyodide","title":"Launching with Jupyter-Lite (Pyodide)","text":"<p>Using Jupyter-Lite you can run the model in the browser without installing anything. This is a great way to get started quickly, and you can also analyse sensitive data without having to share it with 3rd parties.</p> <p>We have a custom build of jupyter-lite that includes the model. </p> <p>You can try it from here:</p> <p>https://sfdl.org.uk/cs-demand-model-jupyter-lite/</p>"},{"location":"Demand_Model/#launching-on-binder","title":"Launching on Binder","text":"<p>You can also launch the model on Binder.  Click to launch the sample repository on binder.</p>"},{"location":"FOI_Scrape/","title":"FOI Whisper Network","text":"","tags":["FOI","Scrape"]},{"location":"FOI_Scrape/#foi-scrape-tools-and-related-data-processing","title":"FOI scrape tool(s) and related data processing","text":"","tags":["FOI","Scrape"]},{"location":"FOI_Scrape/#conceptualinitial-problem-brief","title":"Conceptual/initial problem brief","text":"<p>FOI requests submitted to local authorities by both private individuals and organisations are increasing in frequency. Though valid, the short time-frame requirements, and added overheads on already stretched (data)teams comes at a time-cost to those affected; more so where data data reporting is a small or single person team or that time is allocated on a part-time basis only.   </p> <p>Is it possible to succinctly monitor submitted FOI requests? If yes, it might be possible to (pre-emptively) develop coded responses to these for open use or gain insights from patterns of requested data. This potentially allowing solutions/efforts to be shared collaboratively between impacted local authorities. Should LA colleagues wish, Analysts could also upload/submit both requests they have recieved directly, and|or their responses to recieved FOI requests. </p> <p>In combination with deployment of the [Standard Safeguarding Dataset (SSD)] (https://github.com/data-to-insight/ssd-data-model), where data points for Child Social Care are both known and standardised, it would be possible to codify FOI solutions that could be distributed|utilised by any LA who has received the same or similar FOI request(s). By accessing a central FOI resource within the SSD Git repo, analysts could save hours of unpredictable time and effort. </p> <p>Published: data-to-insight.github.io/foi-scrape-tool</p>","tags":["FOI","Scrape"]},{"location":"FOI_Scrape/#project-repo-file-overview","title":"Project| Repo File Overview","text":"<p>tbc</p>","tags":["FOI","Scrape"]},{"location":"FOI_Scrape/#mkdocs-commands-ref","title":"mkdocs commands ref","text":"<ul> <li><code>mkdocs serve --help</code>     - see list of options including the below</li> <li><code>mkdocs build --clean</code>    - build docs site</li> <li><code>mkdocs serve</code>            - live-reload docs server</li> <li> <p><code>mkdocs serve -a 127.0.0.1:8080</code>  - serve on new port if blocked</p> </li> <li> <p><code>mkdocs gh-deploy</code>        - push new changes(only) to Gitpage front-end(public)</p> </li> <li><code>mkdocs gh-deploy --force</code>- push full rebuild and redeployment to Gitpage front-end(public)</li> </ul>","tags":["FOI","Scrape"]},{"location":"FOI_Scrape/#mkdocs-problem-solving","title":"mkdocs problem solving","text":"<ul> <li><code>pkill mkdocs</code>            - kill any running MkDocs process</li> <li><code>lsof -i :8000</code>           - kill running </li> <li><code>kill -9 12345</code>           - kill process (Replace 12345 with PID)</li> </ul>","tags":["FOI","Scrape"]},{"location":"FOI_Scrape/#features","title":"Features","text":"<ul> <li>Scrapes details from FOI requests from public source(s)   </li> <li>Outputs data in structured HTML amd CSV for download </li> <li>Setup and execution automated via <code>./setup.sh</code> </li> <li>Pre-release \u2013 still in development, feedback welcome!  </li> </ul>","tags":["FOI","Scrape"]},{"location":"FOI_Scrape/#setup-running","title":"Setup &amp; Running","text":"<p>To install dependencies and run the scraper, run (might need file permissions set but details in the file header):  </p> <pre><code>./setup.sh\n</code></pre> <p>This will:  </p> <ul> <li>Install required Python libraries </li> <li>Run scraper to Collect/process data </li> <li>Generate an Current summary to markdown </li> </ul>","tags":["FOI","Scrape"]},{"location":"FOI_Scrape/#future-adaptability","title":"Future Adaptability","text":"<p>The scraper currently pulls data primarily from the whattheyknow site, but could be extended to cover other available sources.</p>","tags":["FOI","Scrape"]},{"location":"FOI_Scrape/#feedback-contributions","title":"Feedback &amp; Contributions","text":"<p>This tool is still in early dev/alpha, and changes/improvements are ongoing. If you encounter any issues, incorrect data extraction, or have suggestions, feel free to:  </p> <ul> <li>Add ticket|feature request to backlog </li> <li>Email us at datatoinsight.enquiries@gmail.com </li> </ul>","tags":["FOI","Scrape"]},{"location":"ILACS_Scrape/","title":"Ofsted-ILACS-Scrape-Tool","text":"<p>On demand Ofsted ILACS results summary via inspection reports scrape from the Ofsted.gov pages Published: https://data-to-insight.github.io/ofsted-ilacs-scrape-tool/ -</p>"},{"location":"ILACS_Scrape/#the-inspection-reports-output-summary-is-refreshed-daily-and-timestamped-for-reference","title":"The inspection reports output summary is refreshed daily and timestamped for reference.","text":""},{"location":"ILACS_Scrape/#initial-problem-brief","title":"Initial problem brief","text":"<p>D2I and some local authorities use the ADCS published Ofsted ILACS inspections Excel summary as part of their internal data workflow(s). However the use of this data is restricted by the limited frequency that the summary sheet is (re-)published. Given that Ofsted inspection reports are published much more often(with an irregular publishing pattern), could we access the data/results directly ourselves and re-create a similar or more useful summary. Concurrently, are there also any other data elements that we could bring in to increase the potential use-cases for such a data summary. </p>"},{"location":"ILACS_Scrape/#solution-overview","title":"Solution overview","text":"<p>This project is based on a proof-of-concept, 'can we do this' basis. As such it's supplied very much with the disclaimer of 'please check the vitals' if you're embedding it into something more critical, and likewise pls feel free to feedback into the project with suggestions. The structure of the code and processes have much scope for improvement, but some of the initial emphasis was on maintaining a level of code readability so that others might have an easier time of taking it further. That said, we needed to take some of the scrape/cleaning processes further than anticipated due to inconsistencies in the source site/data; this has ultimately impacted the intended 're-usable mvp' approach to codifying a solution for the original problem. </p> <p>The results structure and returned data is based almost entirely on the originating ILACS Summary produced/refreshed periodically by the ADCS; the use of which has previously underpinned several D2I projects. We're aware of several similar collections of longer-term work on and surrounding the Ofsted results theme, and would be happy to hear from those who perhaps also have bespoke ideas for changes here that would assist their own work. </p> <p>The scrape process is completed by running a single Python script: ofsted_ilacs_scrape.py</p>"},{"location":"ILACS_Scrape/#exports","title":"Export(s)","text":"<p>There are currently three exports from the script. </p>"},{"location":"ILACS_Scrape/#results-html-page","title":"Results HTML page","text":"<p>Generated (as ./index.html) to display a refreshed subset of the ILACS results summary. </p>"},{"location":"ILACS_Scrape/#results-overview-summary","title":"Results Overview Summary","text":"<p>The complete ILACS overview spreadsheet, exported to the git project root ./ as an .xlsx file for ease and also accessible via a download link from the generated web-site results page (index.html)</p>"},{"location":"ILACS_Scrape/#all-cs-inspections-reports","title":"All CS inspections reports","text":"<p>During the scrape process, because we scan all the related CS inspection pdf reports for each LA; these can be/are packaged up into tidy LA named folders (urn_LAname) within the git repo (./export_data/inspection_reports/). There is a lot of data here, but if you download the entire export_data folder after the script has run, with the overview summary sheet then the local_inspection_reports column active links will work and you can then easily access each LA's previous reports all in once place via the supplied hyperlink(s). Note: This is currently not an option when viewing the results on the web page/Git Pages; but we are happy to clarify how to access/use this element if you get in touch. </p>"},{"location":"ILACS_Scrape/#known-bugs","title":"Known Bugs","text":"<p>Some LA's inspection reports have PDF encoding or inconsistent data in the published reports that is causing extraction issues &amp; null data.  We're working to address these, these are: - southend-on-sea, [overall, help_and_protection_grade,care_leavers_grade] - nottingham, [inspection_framework, inspection_date] - redcar and cleveland, [inspection_framework, inspection_date] - knowsley, [inspector_name] - stoke-on-trent, [inspector_name]</p>"},{"location":"ILACS_Scrape/#smart-city-concept-model-sccm","title":"Smart City Concept Model (SCCM)","text":"<p>The terminology and relations shown here might not be fully alligned with the SCCM standard(s), this is a work-in-progress. </p> Key Entities (click to expand)  | Entity Label                                    | Type         | Description | |------------------------------------------------|--------------|-------------| | **Department for Education (DfE)**             | `ORGANIZATION` | UK government(body) overseeing children's services | | **Ofsted**                                     | `ORGANIZATION` | Inspectors for children\u2019s services(CSC) in England | | **Local Authority**                            | `ORGANIZATION` | Responsible for children\u2019s services delivery. Has unique `la_code` | | **LA Children\u2019s Social Care**                  | `SERVICE`      | Team within LA delivering social care services | | **Children Within Social Care**                | `PERSON`       | Children receiving support from the CSC service | | **ILACS Inspection**                     | `EVENT`        | Official Ofsted inspection of a local authority.Properties: `start_date`, `end_date`, `published_date` | | **ILACS Judgement**                            | `OBSERVATION`  | Ofsted\u2019s findings from an inspection.Properties: `judgement_type`, `judgement_grade` | | **ILACS Inspection Summary**                   | `STATE`        | Summary of findings for a specific ILACS inspection event | | **All ILACS Summary**                          | `COLLECTION`   | Combined ILACS inspections summaries(All LAs).Properties: `generated_date`, `version` | | **ILACS Scraper Tool**                         | `METHOD`       | Python tool extracting inspection summaries(via Ofsted) into structured summary dataset | | **London Innovation and Improvement Alliance (LIIA)** | `COMMUNITY` | Regional group using inspection data for (strategic) analysis | | **LIIA Power BI Dashboards**                   | `FUNCTION`     | Dashboards built from the ILACS summary to inform regional | | **Local Authority Area**                       | `PLACE`        | Geographic/admin area for each Local Authority | | **Region in England**                          | `PLACE`        | Statistical/admin region made up of multiple Local Authorities |   Key Relationships (click to expand)  | Subject                          | Predicate         | Object                        | |----------------------------------|-------------------|-------------------------------| | `Department for Education`       | **oversees**      | `Ofsted`                      | | `Local Authority`                | **is_located_in** | `Local Authority Area`        | | `Local Authority Area`           | **is_part_of**    | `Region in England`           | | `Local Authority`                | **provides**      | `LA Children\u2019s Social Care`   | | `LA Children\u2019s Social Care`     | **serves**        | `Children Within Social Care` | | `Ofsted`                         | **performs**      | `ILACS Inspection Event`      | | `ILACS Inspection Event`         | **hasOutcome**    | `ILACS Inspection Summary`    | | `ILACS Inspection Summary`       | **stateOf**       | `LA Children\u2019s Social Care`   | | `ILACS Inspection Event`         | **records**       | `ILACS Judgement`             | | `ILACS Scraper Tool`            | **extracts_from** | `ILACS Inspection Summary`    | | `ILACS Scraper Tool`            | **produces**      | `All ILACS Summary`           | | `LIIA`                           | **uses**          | `All ILACS Summary`           | | `All ILACS Summary`             | **informs**       | `LIIA Power BI Dashboards`    | | `LIIA`                           | **operates**      | `LIIA Power BI Dashboards`    | | `LIIA`                           | **defines**       | `Region in England`           | | `LIIA`                           | **operates_in**   | `Region in England`           | | `Local Authority`                | **compares_with** | `Local Authority`             | | `Local Authority`                | **learns_from**   | `ILACS Inspection Summary`    |   <p>References: istanduk.org Initial SCCM Project &amp; smartcityconceptmodel.com Smart Cities Concept Model </p>"},{"location":"ILACS_Scrape/#importss","title":"Imports(s)","text":"<p>There are currently two flat file(.csv) imports used. (/import_data/..)</p>"},{"location":"ILACS_Scrape/#la-lookup-import_datala_lookup","title":"LA Lookup (/import_data/la_lookup/)","text":"<p>Allows us to add further LA related data including such as the historic LA codes still in use for some areas, but also enablers for further work, for example ONS region identifiers, and which CMS system LA's are using.</p>"},{"location":"ILACS_Scrape/#geospatial-import_datageospatial","title":"Geospatial (/import_data/geospatial/)","text":"<p>This part of some ongoing work to access data we can use to enrich the Ofsted data with location based information, thus allowing us to visualise results on a map/choropleth. Some of the work towards this is completed, however because LA's geographical deliniations don't always map to ONS data, we're in the process of finding some work-arounds. The code and the reduced GeoJSON data are there if anyone would like to fork the project and suggestion solutions. GeoJSON data has been pre-processed to reduce the usually large file size and enable it within this repo/processing. </p>"},{"location":"ILACS_Scrape/#future-work","title":"Future work","text":"<ul> <li> <p>We have started some early placeholder work on sentiment analysis of the inspection reports. At the moment, this is only shown in the Excel download summary (not on the web based summary to improve readability). The positive/negative sentiment within the inspection reports needs some further work developing both the thresholds and more detailed work on better understanding the report text. </p> </li> <li> <p>Some of the in-progress efforts are included as a point of discuss or stepping stone for others to develop within the download .xlsx file. For example a set of columns detailing simplistic inspection sentiment analysis based on the language used in the most recent report (ref cols: sentiment_score, inspectors_median_sentiment_score, sentiment_summary, main_inspection_topics). Note that the inclusion of these columns does not dictate that the scores are accurate, these additions are a starting point for discussion|suggestions and development!!</p> </li> <li> <p>Geographical/Geospatial visualisations of results by region, la etc. are in progress. The basis for this is aready in place but some anomolies with how LA/counties boundary data is configured is an issue for some and thus the representation requires a bit more thought. </p> </li> <li> <p>Improved automated workflow. We're currently still running the script manually until fixes can be applied to enable the Git Workflow(s) to run automatically/on a daily basis. We have the needed workflow scripts in place, but there is an ongoing issue in getting the py script to auto-run. Manual runs of the py script(+push/pull action) do correctly initiate the refresh of the html/GitPage.</p> </li> <li> <p>Provide active link access to all previous reports via the web front end. This currently only available when all post-script run files/folders are downloaded(this a v.large download if all LA folders included).</p> </li> <li> <p>Further development|bespoke work to improve potential tie-in with existing LA work that could use this tool or the resultant data. </p> </li> </ul>"},{"location":"ILACS_Scrape/#contact-via-datatoinsightenquiries-at-gmailcom","title":"Contact via : datatoinsight.enquiries AT gmail.com","text":""},{"location":"ILACS_Scrape/#script-admin-notes","title":"Script admin notes","text":"<p>Simplified notes towards repo/script admin processes and enabling/instructions for non-admin running. </p>"},{"location":"ILACS_Scrape/#script-run-intructions-user","title":"Script run intructions (User)","text":"<p>If looking to obtain a full instant refresh of the ilacs output, the ofsted_childrens_services_inspection_scrape.PY should be run. These instructions for running in the cloud/Github.  - Create a new Codespace (on main) - Type run the following bash script at Terminal prompt to set up './setup.sh' - Run the script (can right click script file and select 'run in python....' - Download the now refreshed ofsted_childrens_services_inspection_scrape.XLSX (Right click, download) - Close codespace (Github will auto-remove unused spaces later)</p>"},{"location":"ILACS_Scrape/#run-notes-admin","title":"Run notes (Admin)","text":"<p>If you experience a permissions error running the setup bash file. </p> <p>/workspaces/ofsted-ilacs-scrape-tool (main) $ ./setup.sh bash: ./setup.sh: Permission denied</p> <p>then type the following, and try again:  chmod +x setup.sh</p>"},{"location":"JTAI_Scrape/","title":"Ofsted-JTAI-Scrape-Tool","text":"<p>On demand Ofsted JTAI results summary via inspection reports scrape from the Ofsted.gov pages Published: https://data-to-insight.github.io/ofsted-jtai-scrape-tool/ -</p>"},{"location":"JTAI_Scrape/#updates-to-the-jtai-summary-page-occur-on-at-least-a-weekly-basis-but-are-timestamped-for-reference","title":"Updates to the JTAI summary page occur on at least a weekly basis, but are timestamped for reference.","text":""},{"location":"JTAI_Scrape/#brief-overview","title":"Brief overview","text":"<p>This project is based on a proof-of-concept, 'can we do this' basis. As such it's supplied very much with the disclaimer of please check the vitals if you're embedding it into something more critical, and likewise pls feel free to feedback into the project with suggestions. The structure of the code and processes have much scope for improvement, but some of the initial emphasis was on maintaining a level of readability so that others might have an easier time of taking it further. That said, we needed to take some of the scrape/cleaning processes further than anticipated due to inconsistencies in the source site/data and this has ultimately impacted the intended 're-usable mvp' approach to codifying a solution for the original problem. </p> <p>The results structure and returned data is based almost entirely on the originating JTAI Summary produced/refreshed periodically by the ADCS; the use of which has previously underpinned several D2I projects. We're aware of several similar collections of longer-term work on and surrounding the Ofsted results theme, and would be happy to hear from those who perhaps also have bespoke ideas for changes here that would assist their own work. </p> <p>The scrape process is completed by running a single Python script: ofsted_jtai_scrape.py</p>"},{"location":"JTAI_Scrape/#exports","title":"Export(s)","text":"<p>There are currently three exports from the script. </p>"},{"location":"JTAI_Scrape/#results-html-page","title":"Results HTML page","text":"<p>Generated (as ./index.html) to display a refreshed subset of the JTAI results summary. </p>"},{"location":"JTAI_Scrape/#results-overview-summary","title":"Results Overview Summary","text":"<p>The complete JTAI overview spreadsheet, exported to the git project root ./ as an .xlsx file for ease and also accessible via a download link from the generated results page (index.html)</p>"},{"location":"JTAI_Scrape/#all-cs-inspections-reports","title":"All CS inspections reports","text":"<p>During the scrape process, because we scan all the related CS inspection pdf reports for each LA; these can be/are packaged up into tidy LA named folders (urn_LAname) within the git repo (./export_data/inspection_reports/). There is a lot of data here, but if you download the entire export_data folder after the script has run, with the overview summary sheet then the local_inspection_reports column active links will work and you can then easily access each LA's previous reports all in once place via the supplied hyperlink(s). Note: This is currently not an option when viewing the results on the web page/Git Pages.</p>"},{"location":"JTAI_Scrape/#known-bugs","title":"Known Bugs","text":"<p>Some LA's inspection reports have PDF encoding or inconsistent data in the published reports that is causing extraction issues &amp; null data.  We're working to address these, current known  issues are: JTAI reports have no consistent|common structure to extract content from. JTAI report 'Themes' have no consistent structure. Publication date currently duplicates inspection date, as extracting this from the inspection report text isn't possible and will require an additional process to access the main Ofsted inspections listings. </p>"},{"location":"JTAI_Scrape/#importss","title":"Imports(s)","text":"<p>There are currently two flat file(.csv) imports used. (/import_data/..)</p>"},{"location":"JTAI_Scrape/#la-lookup-import_datala_lookup","title":"LA Lookup (/import_data/la_lookup/)","text":"<p>Allows us to add further LA related data including such as the historic LA codes still in use for some areas, but also enablers for further work, for example ONS region identifiers, and which CMS system LA's are using.</p>"},{"location":"JTAI_Scrape/#geospatial-import_datageospatial","title":"Geospatial (/import_data/geospatial/)","text":"<p>This part of some ongoing work to access data we can use to enrich the Ofsted data with location based information, thus allowing us to visualise results on a map/choropleth. Some of the work towards this is completed, however because LA's geographical deliniations don't always map to ONS data, we're in the process of finding some work-arounds. The code and the reduced GeoJSON data are there if anyone would like to fork the project and suggestion solutions. GeoJSON data has been pre-processed to reduce the usually large file size and enable it within this repo/processing. </p>"},{"location":"JTAI_Scrape/#future-work","title":"Future work","text":"<ul> <li> <p>Some of the in-progress efforts are included as a point of discuss or stepping stone for others to develop within the download .xlsx file. For example a set of columns detailing simplistic inspection sentiment analysis based on the language used in the most recent report (ref cols: sentiment_score, inspectors_median_sentiment_score, sentiment_summary, main_inspection_topics). Note that the inclusion of these columns does not dictate that the scores are accurate, these additions are a starting point for discussion|suggestions and development.</p> </li> <li> <p>Geographical/Geospatial visualisations of results by region, la etc. are in progress. The basis for this is aready in place but some anomolies with how LA/counties boundary data is configured is an issue for some and thus the representation requires a bit more thought. </p> </li> <li> <p>Improved automated workflow. We're currently still running the script manually until fixes can be applied to enable the Git Workflow(s) to run automatically/on a daily basis. We have the needed workflow scripts in place, but there is an ongoing issue in getting the py script to auto-run. Manual runs of the py script(+push/pull action) do correctly initiate the refresh of the html/GitPage.</p> </li> <li> <p>Provide active link access to all previous reports via the web front end. This currently only available when all post-script run files/folders are downloaded(this a v.large download if all LA folders included).</p> </li> <li> <p>Further development|bespoke work to improve potential tie-in with existing LA work that could use this tool or the resultant data. </p> </li> </ul>"},{"location":"JTAI_Scrape/#contact-via-datatoinsightenquiries-at-gmailcom","title":"Contact via : datatoinsight.enquiries AT gmail.com","text":""},{"location":"JTAI_Scrape/#script-admin-notes","title":"Script admin notes","text":"<p>Simplified notes towards repo/script admin processes and enabling/instructions for non-admin running. </p>"},{"location":"JTAI_Scrape/#script-run-intructions-user","title":"Script run intructions (User)","text":"<p>If looking to obtain a full instant refresh of the JTAI output, the ofsted_childrens_services_inspection_scrape.PY should be run. These instructions for running in the cloud/Github.  - Create a new Codespace (on main) - Type run the following bash script at Terminal prompt to set up './setup.sh' - Run the script - can right click script file and select 'run in python....' - Download the now refreshed ofsted_childrens_services_jtai_overview.XLSX (Right click, download) - Close codespace (Github will auto-remove unused spaces later)</p>"},{"location":"JTAI_Scrape/#run-notes-admin","title":"Run notes (Admin)","text":"<p>If you experience a permissions error running the setup bash file. </p> <p>/workspaces/ofsted-jtai-scrape-tool (main) $ ./setup.sh bash: ./setup.sh: Permission denied</p> <p>then type the following, and try again:  chmod +x setup.sh</p>"},{"location":"NVEST/","title":"NVEST Tools(Contacts processing) &amp; Regional Members Overview","text":"<p>In-progress collection of tools towards NVEST related processing.</p> <p>Tool(s) available via : NVEST Tools</p>","tags":["NVEST","loader","validation"]},{"location":"NVEST/#contacts-processing-tool","title":"Contacts processing tool","text":"<p>A browser-based process for handling NVEST contacts and events data alongside a (quasi)live regional overview map.  Uses stlite to run Streamlit/pyodide(in stlite) directly in browser and embeds both views via <code>&lt;iframe&gt;</code> containers into the html page(s). </p>","tags":["NVEST","loader","validation"]},{"location":"NVEST/#instructions","title":"Instructions","text":"","tags":["NVEST","loader","validation"]},{"location":"NVEST/#step1-pre-processing-steps","title":"Step1: Pre-processing steps","text":"<p>The master contacts worksheet. will only be up to date if the source data is. Confirm/do the following before running the contacts processing tool.</p> <ol> <li>Download the entire Wix NVEST EOI form submissions. The NVEST contacts, download should be accessed only via Forms and Submissions. Download this data into : <ul> <li><code>NVEST/Contacts/NVEST_Contacts_Master_Processing/dddmmmyyy.csv</code></li> </ul> </li> <li>If you have any contacts that have been sent seperately(i.e they might not have yet registered via Wix form), they should be added to the list in <ul> <li><code>NVEST/Contacts/NVEST_Contacts_Master_Processing/contacts_other_external.csv</code> </li> </ul> </li> <li>NVEST event/workshop invitations and attendance data is stored on event sheets within <ul> <li><code>NVEST/Contacts/NVEST_Contacts_Master_Processing/Events/event1.csv + event2.csv + event3.csv + .....</code> </li> </ul> </li> </ol>","tags":["NVEST","loader","validation"]},{"location":"NVEST/#step2-tool-processing-steps","title":"Step2: Tool Processing steps","text":"<p>Using NVEST Contacts Processing Tool</p> <p>There are 4 buttons/actions   - Three for uploading the needed source contacts &amp; events data(in this order)</p> <pre><code>1. Single file: `NVEST/Contacts/NVEST_Contacts_Master_Processing/&lt;wix_nvest_downloaded_contacts&gt;.csv`\n\n2. Single file: `NVEST/Contacts/NVEST_Contacts_Master_Processing/contacts_other_external.csv`\n\n3. **ALL**/Multiple-files in: `NVEST/Contacts/NVEST_Contacts_Master_Processing/Events/*.csv`\n</code></pre> <ul> <li> <p>One to download the resultant processed contacts data</p> <ul> <li>Data downloads as .csv file but MUST be saved/overwrite <code>NVEST/Contacts/NVEST_Contacts_Master_Processing/processed_contacts_refreshed.csv</code> </li> </ul> </li> </ul> <p>The processing occurs immeadiately that the 3rd upload source files are added. There is nothing to press or do in order to initiate the actual processing and the download file is available instantly after the above 3 steps are completed.</p>","tags":["NVEST","loader","validation"]},{"location":"NVEST/#step3-update-working-contacts-sheet","title":"Step3: Update working contacts sheet","text":"<p>Now that the contacts have been processed (i.e. cleaned, de-duped, other data added, steering grp flagged etc) and downloaded into <code>NVEST\\Contacts\\4 - contacts_processed\\processed_contacts_refreshed.csv</code>. We are going to refresh the contacts sheet with this data. It's a simple 1 button update. </p> <ul> <li>Open the main Contacts Excel sheet/file on your PC. This is usually located: <code>NVEST\\Contacts\\</code> </li> <li>From the ribbon, navigate to the 'Data' tab </li> <li>Click 'Refresh All' from the 'Queries and Connections' block on the ribbon </li> </ul> <p>That's it. Whether you notice any changes or not, all the data in the contacts (and events) page(s) is now updated and current. </p>","tags":["NVEST","loader","validation"]},{"location":"NVEST/#important-further-information","title":"Important further information","text":"<p>NVEST_Contacts_Master.xls The master contact sheet itself is NOT where contacts data should be edited/updated. The data sources within <code>\\NVEST\\Contacts\\NVEST_Contacts_Master_Processing\\</code> are where any changes/edits should be made. The reason is that we refresh all the data within the contacts workbook each time we run this process. So the source data and not the contacts workbook is our single data source for the actual contacts related data. IF wanting to store person/individual specific notes, or additional data against specific records, this can be done using a user defined lookup page in which email address, and those related additional columns/data is stored against them. </p>","tags":["NVEST","loader","validation"]},{"location":"NVEST/#nvest-events-files","title":"NVEST Events files","text":"<p>The NVEST events invites/attended are recorded to assist event planning and checking we've correctly contacted all those that have registered their interest. When a new event is planned, the full list of current Contacts emails should be copied from the master contact sheet overview (column <code>email</code>), and pasted afresh into into the next free <code>event1, 2, 3,... file</code>. Do not change the file name, or file type here as it will break the onward data flow. These are csv files so no formatting can be stored in the file. Instead, if desired such formatting can be added in the NVEST_Contacts_Master.xls.... in this file Event1, 2, 3 heading CAN then be changed to reflect the date or another decriptor for each event. To be clear.... col headers can only be modified in the NVEST_Contacts_Master.xls file. </p> <p>Key take-aways for managing events files</p> <ul> <li>Events files cant be renamed</li> <li>Events files are csv so not for annotating with colour formatting</li> <li>Additional columns CAN be added into Events files - but those will NOT come through into the master contacts</li> <li>New individual emails can be added to Event file at any point (but those emails/contacts should already exist in Wix OR contacts_other_external.csv)</li> </ul>","tags":["NVEST","loader","validation"]},{"location":"NVEST/#related-tech-notes","title":"Related Tech Notes","text":"","tags":["NVEST","loader","validation"]},{"location":"NVEST/#process-logic","title":"Process Logic","text":"<ol> <li>Upload CSVs:</li> <li>Wix signups: <code>NVEST/Contacts/NVEST_Contacts_Master_Processing/&lt;latest date file&gt;.csv</code></li> <li>Other external contacts: <code>NVEST/Contacts/NVEST_Contacts_Master_Processing/contacts_other_external.csv</code></li> <li>NVEST events attendees: <code>NVEST/Contacts/NVEST_Contacts_Master_Processing/Events/&lt;upload &lt;ALL&gt; files in this folder&gt;.csv</code></li> <li>Clean + de-duplicate each:</li> <li>Drop blanks, rename/standardise columns </li> <li>Parse <code>submission_date</code> (empty dates get today())</li> <li>Normalise/lower <code>email</code> </li> <li>Keep latest record by <code>email</code> (web sign up record takes priority if duplicates identified)</li> <li>We need to remove dups before merge, as might not have submission date/time on external records - so determining most recent record not possible; so we assume web sign up is more recent as members directed to this input route after having shown interest via another route, e.g. workshop.</li> <li>Ensure column order matches requested output order </li> <li>Apply basic text normalisation on agreed columns (currently: titlecase on <code>name</code>(not yet effective for such as McGregor), <code>role</code>, <code>local_authority</code>) </li> <li>Sort on <code>name</code> (asc)       </li> <li>Added fields:</li> <li><code>domain</code> (captured from email address - as this will be more reliable|consistent than inputted LA name text)  </li> <li><code>steering_grp</code> (default: <code>\"N\"</code>)  </li> <li> <p><code>source</code>: <code>\"Web\"</code> for Wix, <code>\"External\"</code> for others (these labels explain where the contact has originated)</p> </li> <li> <p>Merge contacts datasets</p> </li> <li>Bring together Wix contacts, and those coming in from other sources(e.g. via email, or requested)</li> <li>Should a contact be added into the other sources, but later register via the web site the de-dup process will retain the web site registered record</li> <li>Flag steering group uses hard-coded email match (repo needs to stay private, as those emails now in code not upload via file)</li> <li>Preview rows, &amp; option of timestamped CSV download</li> </ol>","tags":["NVEST","loader","validation"]},{"location":"NVEST/#repo-structure","title":"Repo structure","text":"<ul> <li><code>index.html</code>:   Landing page, currently with: </li> <li>Left: <code>upload_tool.html</code> (iframe)  </li> <li> <p>Right: Embedded Google Map (iframe) </p> </li> <li> <p><code>upload_tool.html</code>:   HTML file embedding a stlite-powered Streamlit app</p> </li> <li> <p><code>assets/</code>:   Img assets (e.g., <code>images/d2i_logo.png</code>)   Styling assets (e.g., <code>css/styles.css</code>)</p> </li> </ul>","tags":["NVEST","loader","validation"]},{"location":"NVEST/#tech-stack","title":"Tech stack","text":"<ul> <li>Streamlit for data interface  </li> <li>stlite to run Streamlit app directly in browser (WebAssembly + Pyodide)  </li> <li>Bootstrap 5 for layout  </li> <li>Google My Maps for embedded regional visual</li> </ul>","tags":["NVEST","loader","validation"]},{"location":"NVEST/#deploymentdevelopment-notes","title":"Deployment/Development notes","text":"<p>Built to run in GitHub Pages and other static hosting providers. Ensure:</p> <ul> <li>avoid direct browser refresh on subpages (e.g., <code>/upload_tool.html</code>) unless served (e.g., via iframe or root-relative path) - i've not added # or 404 handling</li> <li>Test : python3 -m http.server 8501</li> </ul>","tags":["NVEST","loader","validation"]},{"location":"PATCH/","title":"PATCh","text":"<p>This is a directory of web applications built with Python. The apps run entirely in the  browser. It uses mainly two libraries:</p> <ul> <li>streamlit - web applications using simple python code. Check the documentation to learn  how to write apps.</li> <li>stlite - allows to use stramlit in the browser (using pyodide). If you're just  writing apps, you don't have to worry about this.</li> </ul> <p>All you need to do is to write your own streamlit apps and they will be accessible from the directory of apps.</p>"},{"location":"PATCH/#how-to-start","title":"How to start","text":"<p>Here are some quick video guides on how to contribute via GitHub and how to write a basic app if you prefer that to reading.</p> <p>If not, let's go ahead and create, run and publish a small testing app. </p> <p>If you just want to see the final result, check this python file which creates  this live app.</p>"},{"location":"PATCH/#setup-the-editor","title":"Setup the editor","text":"<ol> <li> <p>Start by creating a new branch. This is where your code will live until you are ready to publish your app.</p> </li> <li> <p>Within that branch, go back to the repository main page (where you can read this document) and open the web-based editor on this repository by pressing the . key on the keyboard.</p> </li> <li> <p>Ensure you have the suggested extensions installed - a popup should open on the lower right side of your screen a  few seconds after you open this editor. If it doesn't, go to the sidebar of the editor, click on the <code>extensions</code> icon  and check the <code>recommended</code> section. You should see a social finance extension called <code>sf-nova</code> (if not, search for it). Install it.</p> </li> </ol> <p>:warning: please ensure you have installed the <code>sf-nova</code> extension and not the original <code>stlite</code> extension - the latter currently doesn't work while the former is an adaptation written by Social Finance that works properly.</p>"},{"location":"PATCH/#create-an-app","title":"Create an app","text":"<ol> <li> <p>Go to the apps directory and make a new directory for your app (for example, <code>apps/my_very_first_app</code>)  and create your main app python file within it (it can be <code>apps/my_very_first_app/app.py</code> or  <code>apps/my_very_first_app/my_very_first_app.py</code> or whatever you want).</p> </li> <li> <p>Let's create a sample app that asks for a username and says hello back to the user. In the file  <code>apps/my_very_first_app/app.py</code> add the following code and save it:</p> <p>```python import streamlit as st</p> <p>name = st.text_input(\"What's your name?\")</p> <p>if name:     st.write(f'Hello {name}!') ```</p> <p>In here we:</p> <ul> <li>Import the streamlit package</li> <li>ask for the users name with a text input</li> <li>say hello to the user once they submit their name</li> </ul> </li> </ol>"},{"location":"PATCH/#preview-the-app","title":"Preview the app","text":"<ol> <li> <p>With your app's python file opened (and focused), click on the <code>sf-nova</code> icon on the sidebar (it should be the one  bellow the <code>github</code> icon) and press \"Launch preview\". </p> <p> </p> </li> <li> <p>You can also run it from vscode command palette: <code>ctrl</code> + <code>shift</code> + <code>P</code> and search for the command  <code>launch streamlit preview</code>. You should now see a preview on the right side of your editor, while your app's code is in  your left side:</p> <p></p> </li> <li> <p>You can now edit the code and, once you save the python file, it will update the app's preview accordingly. If it  doesn't, edit the settings of your streamlit preview screen (on the top right button) and check the \"Run on save\"  option - that will ensure your preview reloads every time you change something in your code (and save the file(s)).</p> </li> <li> <p>If you need to use external libraries (such as <code>matplotlib</code> to render charts or <code>openpyxl</code> to read excel files)  ensure that those libraries are listed in a <code>requirements.txt</code> file in the same directory as your python file is.  Check this example for guidance. </p> </li> <li> <p>Once you are happy with your changes, you are ready to publish your app (pending on approval) by making a pull  request with your new app. Once your code is approved and merged, it will be displayed in the directory of apps in https://patch.datatoinsight.org.</p> </li> </ol>"},{"location":"SEND_Scrape/","title":"Ofsted-SEND-Scrape-Tool","text":"<p>On demand Ofsted SEND results summary via inspection reports scrape from the Ofsted.gov pages Published: https://data-to-insight.github.io/ofsted-send-scrape-tool/ -</p>"},{"location":"SEND_Scrape/#the-inspection-reports-output-summary-is-refreshed-daily-and-timestamped-for-reference","title":"The inspection reports output summary is refreshed daily and timestamped for reference.","text":""},{"location":"SEND_Scrape/#brief-overview","title":"Brief overview","text":"<p>Note: The scrape only pulls those LA's that have a published report within the revised, post 2023 framework. This project is based on a proof-of-concept, 'can we do this' basis. As such it's supplied very much with the disclaimer of please check the vitals if you're embedding it into something more critical, and likewise pls feel free to feedback into the project with suggestions. The structure of the code and processes have much scope for improvement, but some of the initial emphasis was on maintaining a level of readability so that others might have an easier time of taking it further. That said, we needed to take some of the scrape/cleaning processes further than anticipated due to inconsistencies in the source site/data and this has ultimately impacted the intended 're-usable mvp' approach to codifying a solution for the original problem. </p> <p>The results structure and returned data is based almost entirely on the originating SEND Summary produced/refreshed periodically by the ADCS; the use of which has previously underpinned several D2I projects. We're aware of several similar collections of longer-term work on and surrounding the Ofsted results theme, and would be happy to hear from those who perhaps also have bespoke ideas for changes here that would assist their own work. </p> <p>The scrape process is completed by running a single Python script: ofsted_send_scrape.py</p>"},{"location":"SEND_Scrape/#exports","title":"Export(s)","text":"<p>There are currently three exports from the script. </p>"},{"location":"SEND_Scrape/#results-html-page","title":"Results HTML page","text":"<p>Generated (as ./index.html) to display a refreshed subset of the SEND results summary. </p>"},{"location":"SEND_Scrape/#results-overview-summary","title":"Results Overview Summary","text":"<p>The complete SEND overview spreadsheet, exported to the git project root ./ as an .xlsx file for ease and also accessible via a download link from the generated results page (index.html). </p>"},{"location":"SEND_Scrape/#all-csc-inspections-reports","title":"All CSC inspections reports","text":"<p>During the scrape process, because we scan all the related CSC inspection pdf reports for each LA; these can be/are packaged up into tidy LA named folders (urn_LAname) within the git repo (./export_data/inspection_reports/). There is a lot of data here, but if you download the entire export_data folder after the script has run, with the overview summary sheet then the local_inspection_reports column active links will work and you can then easily access each LA's previous reports all in once place via the supplied hyperlink(s). Note: This is currently not an option when viewing the results on the web page/Git Pages.</p>"},{"location":"SEND_Scrape/#known-bugs","title":"Known Bugs","text":"<p>Some LA's inspection reports have PDF encoding or inconsistent data in the published reports that is causing extraction issues &amp; null data.  We're working to address these, current known  issues are: - The scrape only pulls those LA's that have a published report within the revised, post 2023 framework. We're considering whether/how best to include those inspections based both on previous frameworks and those with a written statement of action. </p>"},{"location":"SEND_Scrape/#importss","title":"Imports(s)","text":"<p>There are currently two flat file(.csv) imports used. (/import_data/..)</p>"},{"location":"SEND_Scrape/#la-lookup-import_datala_lookup","title":"LA Lookup (/import_data/la_lookup/)","text":"<p>Allows us to add further LA related data including such as the historic LA codes still in use for some areas, but also enablers for further work, for example ONS region identifiers, and which CMS system LA's are using.</p>"},{"location":"SEND_Scrape/#geospatial-import_datageospatial","title":"Geospatial (/import_data/geospatial/)","text":"<p>This part of some ongoing work to access data we can use to enrich the Ofsted data with location based information, thus allowing us to visualise results on a map/choropleth. Some of the work towards this is completed, however because LA's geographical deliniations don't always map to ONS data, we're in the process of finding some work-arounds. The code and the reduced GeoJSON data are there if anyone would like to fork the project and suggestion solutions. GeoJSON data has been pre-processed to reduce the usually large file size and enable it within this repo/processing. </p>"},{"location":"SEND_Scrape/#future-work","title":"Future work","text":"<ul> <li> <p>Some of the in-progress efforts are included as a point of discuss or stepping stone for others to develop within the download .xlsx file. For example a set of columns detailing simplistic inspection sentiment analysis based on the language used in the most recent report (ref cols: sentiment_score, inspectors_median_sentiment_score, sentiment_summary, main_inspection_topics). Note that the inclusion of these columns does not dictate that the scores are accurate, these additions are a starting point for discussion|suggestions and development!!</p> </li> <li> <p>Geographical/Geospatial visualisations of results by region, la etc. are in progress. The basis for this is aready in place but some anomolies with how LA/counties boundary data is configured is an issue for some and thus the representation requires a bit more thought. </p> </li> <li> <p>Improved automated workflow. We're currently still running the script manually until fixes can be applied to enable the Git Workflow(s) to run automatically/on a daily basis. We have the needed workflow scripts in place, but there is an ongoing issue in getting the py script to auto-run. Manual runs of the py script(+push/pull action) do correctly initiate the refresh of the html/GitPage.</p> </li> <li> <p>Provide active link access to all previous reports via the web front end. This currently only available when all post-script run files/folders are downloaded(this a v.large download if all LA folders included).</p> </li> <li> <p>Further development|bespoke work to improve potential tie-in with existing LA work that could use this tool or the resultant data. </p> </li> </ul>"},{"location":"SEND_Scrape/#contact-via-datatoinsightenquiries-at-gmailcom","title":"Contact via : datatoinsight.enquiries AT gmail.com","text":""},{"location":"SEND_Scrape/#script-admin-notes","title":"Script admin notes","text":"<p>Simplified notes towards repo/script admin processes and enabling/instructions for non-admin running. </p>"},{"location":"SEND_Scrape/#script-run-intructions-user","title":"Script run intructions (User)","text":"<p>If looking to obtain a full instant refresh of the SEND output, the ofsted_childrens_services_inspection_scrape.PY should be run. These instructions for running in the cloud/Github.  - Create a new Codespace (on main) - Type run the following bash script at Terminal prompt to set up './setup.sh' - Run the script (can right click script file and select 'run in python....' - Download the now refreshed ofsted_childrens_services_inspection_scrape.XLSX (Right click, download) - Close codespace (Github will auto-remove unused spaces later)</p>"},{"location":"SEND_Scrape/#run-notes-admin","title":"Run notes (Admin)","text":"<p>If you experience a permissions error running the setup bash file. </p> <p>/workspaces/ofsted-send-scrape-tool (main) $ ./setup.sh bash: ./setup.sh: Permission denied</p> <p>then type the following, and try again:  chmod +x setup.sh</p>"},{"location":"Validator_903/","title":"Quality LAC data beta: Python validator","text":"<p>We want to build a tool that improves the quality of data on Looked After Children so that Children\u2019s Services Departments have all the information needed to enhance their services.</p> <p>We believe that a tool that highlights and helps fixing data errors would be valuable for:</p> <ol> <li>Reducing the time analysts, business support and social workers spend cleaning data.</li> <li>Enabling leadership to better use evidence in supporting Looked After Children.</li> </ol>"},{"location":"Validator_903/#about-this-project","title":"About this project","text":"<p>The aim of this project is to deliver a tool to relieve some of the pain-points of reporting and quality in children's services data. This project focuses, in particular, on data on looked after children (LAC) and the SSDA903 return.</p> <p>The project consists of a number of related pieces of work:</p> <ul> <li>Hosted Tool</li> <li>React &amp; Pyodie Front-End</li> <li>Python Validator Engine &amp; Rules [this repo]</li> <li>Local Authority Reference Data</li> <li>Postcode Reference Data</li> </ul> <p>The core parts consist of a Python validator engine and rules using Pandas with Poetry for dependency management. The tool is targeted to run either standalone, or in pyodide in the browser for a zero-install deployment with offline capabilities.</p> <p>It provides methods of finding the validation errors defined by the DfE in 903 data. The validator needs to be provided with a set of input files for the current year and, optionally, the previous year. These files are coerced into a common format and sent to each of the validator rules in turn. The validators report on rows not meeting the rules and a report is provided highlight errors for each row and which fields were included in the checks.</p>"},{"location":"Validator_903/#data-pipeline","title":"Data pipeline","text":"<ul> <li>Loading of files</li> <li>Identification of tables - currently matched on exact filename</li> <li>Conversion of CSV to tabular format - no type checking</li> <li>Enrichment of provided data with Postcode distances</li> <li>Evaluation of rules</li> <li>Report</li> </ul>"},{"location":"Validator_903/#project-structure","title":"Project Structure","text":"<p>These are the key files</p> <pre><code>project\n\u251c\u2500\u2500\u2500 pyproject.toml           - Project details and dependencies\n\u251c\u2500\u2500\u2500 validator903\n\u2502    \u251c\u2500\u2500\u2500 config.py           - High-level configuration\n\u2502    \u251c\u2500\u2500\u2500 ingress.py          - Data ingress (handling CSV and XML files)\n\u2502    \u251c\u2500\u2500\u2500 types.py            - Classes used across the work\n\u2502    \u251c\u2500\u2500\u2500 validator.py        - The core validator process\n\u2502    \u2514\u2500\u2500\u2500 validators.py       - All individual validator codes\n\u2514\u2500\u2500\u2500 tests                    - Unit tests\n</code></pre> <p>Most of the work from contributors will be in <code>validators.py</code> and the associated testing files under tests. Please do not submit a pull-request without a comprehensive test.</p>"},{"location":"Validator_903/#development","title":"Development","text":"<p>To run in codespaces, you need to run in a virtual environment, this information can be found in a .txt file in the documentation.</p> <p>To install the code and dependencies, from the main project directory run:</p> <pre><code>poetry install\n</code></pre> <p>If this does not work, it might be because you're running the wrong version of Python, the version of Numpy used by the 903 validator is locked at 3.9. The devcontainer and dockerfile should ensure you are running 3.9 and you may simply require a rebuild. If not, ensure you are working in an environment or venv with Python 3.9 as your interpreter.</p>"},{"location":"Validator_903/#adding-validators","title":"Adding validators","text":"<p>Validators are contained in <code>rule_XXX()</code> files in the rules folder, where <code>xxx</code> is the code of the validation rule. Each file contains a <code>validate</code> which defines the rule logic and a <code>test_validate</code> function which runs the validate function on some test data to check that the rule works as expected. The validator takes a single argument, the datastore, which is a Mapping (a dict-like) following the structure below.</p> <p>The following is the expected structure for the input data that is given to each validator (the <code>dfs</code> object). You should assume that not all of these keys are present and handle that appropriately.</p> <p>Any XML uploads are converted into CSV form to give the same inputs.</p> <pre><code>{\n    # This years data\n    'Header':   # header dataframe\n    'Episodes': # episodes dataframe\n    'Reviews':  # reviews dataframe\n    'UASC':     # UASC dataframe\n    'OC2':      # OC2 dataframe\n    'OC3':      # OC3 dataframe\n    'AD1':      # AD1 dataframe\n    'PlacedAdoption':  # Placed for adoption dataframe\n    'PrevPerm': # Previous permanence dataframe\n    'Missing':  # Missing dataframe\n    'SWEpisodes': # Social Worker Episodes dataframe (new from 2023/24 return) \n    # Last years data\n    'Header_last':   # header dataframe\n    'Episodes_last': # episodes dataframe\n    'Reviews_last':  # reviews dataframe\n    'UASC_last':     # UASC dataframe\n    'OC2_last':      # OC2 dataframe\n    'OC3_last':      # OC3 dataframe\n    'AD1_last':      # AD1 dataframe\n    'PlacedAdoption_last':  # Placed for adoption dataframe\n    'PrevPerm_last': # Previous permanence dataframe\n    'Missing_last':  # Missing dataframe\n    'SWEpisodes_last': # Social Worker Episodes dataframe (new from 2023/24 return)\n    # Metadata\n    'metadata': {\n        'collection_start': # A datetime with the collection start date (year/4/1)\n        'collection_end':   # A datetime with the collection end date (year + 1/4/1)\n        'postcodes':        # Postcodes dataframe, columns laua, oseast1m, osnrth1m, pcd\n        'localAuthority:    # The local authority code entered (long form, e.g. E07000026)\n        'collectionYear':   # The raw collection year string - unlikely to need this (e.g. '2019/20')\n    }\n}\n</code></pre>"},{"location":"Validator_903/#yearly-rule-updates","title":"Yearly rule updates","text":"<p>Each year, the DfE might release specifications of any rules which have been added, modified or deleted. Expanded guidance on how to incorporate these changes can be found in the landing page (readme.md file) of the CIN validator repo. The CIN and LAC validators have been refactored to resemble each other as much as possible so their overall documentation applies to both tool backends.</p>"},{"location":"Validator_903/#publishing-backend-changes-to-the-frontend-live-tool","title":"Publishing backend changes to the frontend live tool","text":"<p>When bugs are fixed or rules modified, it is necessary to update the tool so that users can have access to the improvements that have been made in the backend.  Detailed steps on how to do this are spelt out in the README of the children in need data validator. Read about making changes available to users</p>"},{"location":"Validator_903/#releases","title":"Releases","text":"<p>To build and release a new version, make sure all your unit tests pass.</p> <p>We use semantic versioning, so update the project version in pyproject.toml accordingly and commit, creating a PR. Once the release version is on GitHub, create a GitHub release naming the release with the  current release name, e.g. 1.0 and the tag with the release name prefixed with a v, i.e. v1.0. Alpha and beta releases  can be flagged by appending <code>-alpha.&lt;number&gt;</code> and <code>-beta.&lt;number&gt;</code>.</p>"},{"location":"Validator_CiN/","title":"CIN-validator","text":"<p>The CIN validator is an open source, volunteer built tool that allows users to validate CIN census data year round via the command line, or using the browser based front end (URL HERE). It also provides a framework which other validation tools can easily be built on top of.</p> <p>The functions are documented using sphinx format so that a docs website can be auto-generated if need be. Also, there is an increased use of python type-hints as a form of intrinsic documentation. This does not apply to test functions as they neither receive nor return data, in the strict sense. More extensive documentation can be found here: https://data-to-insight.github.io/CIN-validator/</p>"},{"location":"Validator_CiN/#setup","title":"Setup","text":"<p>This repo can be opened and run in a codespaces instance or cloned locally using <code>git clone https://github.com/data-to-insight/CIN-validator.git</code> If it is cloned locally, use <code>pre-commit install</code> to install the pre-commit hooks.</p>"},{"location":"Validator_CiN/#run","title":"Run","text":"<ul> <li>To test that all the rules pass their Pytests and will validate data as expected: <code>python -m cin_validator test</code></li> <li>To list all rules that are present: <code>python -m cin_validator list</code></li> <li>To run rules on a file and generate a table of error locations: <code>python -m cin_validator run &lt;path to test data&gt;</code> -To run rules on the sample data and explore the output of the CLI: <code>python -m cin_validator run path/to/your/cin/validator/CIN-validator/fake_data/fake_CIN_data.xml</code></li> <li>To run rules on a file and select an instance of an error based on its ID: <code>python -m cin_validator run &lt;path to test data&gt; -e \"&lt;ERROR_ID as string&gt;\"</code></li> <li>To convert a CIN XML file to it's respective CSV tables: <code>python -m cin_validator xmltocsv &lt;path to test data&gt;</code></li> </ul>"},{"location":"Validator_CiN/#yearly-tool-updates","title":"Yearly tool updates","text":""},{"location":"Validator_CiN/#update-rule-resources","title":"Update rule resources","text":"<ul> <li>Run <code>python get_uk_holidays.py</code> in the command line. This fetches the latest values of bank holidays into <code>cin_validator\\england_holidates.py</code> (don't edit this file directly) for the rules that need them. Remember to convert \\ to / if you are using a unix operating system.</li> </ul>"},{"location":"Validator_CiN/#update-rules","title":"Update rules","text":"<ul> <li>If any rules have been added or changed with respect to the previous year, create files for them in a rule folder named after the new validation year. For example, new or added rules for the 2023/24 validation year should be created in a folder named <code>cin2023_24</code>. Do not copy over rules that haven't changed.</li> <li>The init.py file contains the code that pulls in rules from the previous year and modifies them to meet the current year's specification. Copy across that init file whenever a folder for a new collection_year is created. Change the import to the name of the previous year's folder. </li> <li>If the new specifications require that some rules are deleted, add their codes as strings to the <code>del_list</code> array in the current year's init file. Do not delete the rules manually. </li> <li>Any new rules or modified rules should be added by creating a file for each rule and writing the modified code or new code. Even for small modifications, create a new file for the rule in the year where the modification was made instead of going backwards into the previous years and editing the original file.</li> <li>To run the modified set of rules from the command line interface, you can use the <code>-r</code> or <code>--ruleset</code> flag to specify the name of the rule folder that you wish to run. Otherwise, feel free to update the defaults of the commands so that they point to the new year's folder instead. For example, change <code>cin2022_23</code> to <code>cin2023_24</code>. </li> </ul>"},{"location":"Validator_CiN/#make-changes-available-to-user","title":"Make changes available to user","text":"<p>This part is a guide on how to update the frontend so that it reflects the changes that have been done in the backend. - Delete the <code>dist</code> folder completely. - Update the package version in the <code>pyproject.toml</code> file. (there is a section below to help you choose the new number.) - run <code>poetry install</code> (installs project dependencies) and then <code>poetry shell</code> (ensures project runs in controlled environment) in the command line. You might have already done this when updating the rules. - check that validation rules work as expected (pass) when you run <code>poetry python -m cin_validator test</code> in the command line. - Then run <code>poetry build</code> in the command line. You will see that this creates a new <code>dist</code> folder. - Push the pyproject.toml change to github and do a pull request.  - When the version-number-change pull request is merged in, do a <code>release</code> by navigating to the release page from the right hand control bar on the repo homepage (click on <code>Releases</code> then <code>draft new release</code> in the top right hand corner) - The release tag is created by including a <code>v</code> before the version number which you put in the <code>pyproject.toml</code> file. For example, if you filled in <code>version = \"0.1.3\"</code> then on Github, write <code>v0.1.3</code> as your release tag. - Click on <code>generate release notes</code> at the top right of the main text box. Commit messages of all changes made since the last release will appear in the text box. - Create a release title that starts with a ddmmyyy pattern to indicate the date of the release and then you can write a few words to describe the changes made since the last release. - take the <code>.whl</code> file from the dist folder in this repo, go to the <code>public\\bin\\dist</code> location in the frontend repo, delete the previous cin..<code>.whl</code> file in it and add this one. - Search for the former wheel name on the frontend repo and update all locations where the wheel file name is referenced so that they now point to the new wheel file name with updated version number. Here is an example. - Do a pull request to the frontend repo, containing your changes.  - When the frontend pull request is merged in, the live tool will be updated automatically. You can confirm by checking that the version number in the footer of the web app has changed. - You can watch the deployment process in the <code>Actions</code> tab on Github when your pull request is merged to the frontend. - All done !</p>"},{"location":"Validator_CiN/#notes-about-choosing-version-numbers","title":"Notes about choosing version numbers","text":"<p>When changes are rules updates (add/delete/modify) or bug fixes, only the last part of the version number should be updated (e.g <code>v0.1.9</code> becomes <code>v0.1.10</code>).   The middle number is only updated when new features have been added (the changes enable user functionality that was not previously available). Finally, the first part of the version number is changed when breaking changes are present (new version of tool is incompatible with previous version e.g when functions in the api, <code>rpc_main.py</code>, change.) Read more about semantic versioning here.</p>"},{"location":"Validator_SEN/","title":"annex-a-sen-validator-be","text":"<p>Back end and CLI for the Annex A SEN validator.</p>"},{"location":"YouthJustice_Scrape/","title":"hmi-probation-youth-justice-scrape","text":"<p>This repository contains a scraper for HM Inspectorate of Probation Youth Justice inspection reports, extracting structured data from published PDFs. The extracted data is compiled into a summary dataset, which is automatically updated and published as an HTML report. Note: We are reviewing with colleagues atm regarding how best to potentially combine historic/mis-matched grading columns to improve the summary output usefulness and display without compromising the expected gradings. </p>"},{"location":"YouthJustice_Scrape/#features","title":"Features","text":"<ul> <li>Scrapes only Youth Justice inspection reports from HMI Probation  </li> <li>Extracts inspection ratings and outcomes directly from PDF reports  </li> <li>Outputs data in structured CSV and HTML formats </li> <li>Setup and execution automated via <code>./setup.sh</code> </li> <li>Alpha release \u2013 still in development, feedback welcome!  </li> </ul>"},{"location":"YouthJustice_Scrape/#setup-running","title":"Setup &amp; Running","text":"<p>To install dependencies and run the scraper, run (might need permissions set but details in the file header):  </p> <pre><code>./setup.sh\n</code></pre>"},{"location":"YouthJustice_Scrape/#setup-running_1","title":"Setup &amp; Running","text":"<p>This will:  </p> <ul> <li>Install required Python libraries </li> <li>Run scraper to Collect/process data </li> <li>Generate an Current HTML summary </li> </ul>"},{"location":"YouthJustice_Scrape/#future-adaptability","title":"Future Adaptability","text":"<p>The scraper currently focuses on Youth Justice inspections, but could be extended to cover other available report types, such as:  </p> <ul> <li>Probation services </li> <li>Joint Targeted Area Inspections (JTAI) </li> <li>Serious Further Offence Reviews </li> <li>Thematic reports </li> </ul>"},{"location":"YouthJustice_Scrape/#feedback-contributions","title":"Feedback &amp; Contributions","text":"<p>This tool is still in early dev/alpha, and improvements are ongoing. If you encounter any issues, incorrect data extraction, or have suggestions, feel free to:  </p> <ul> <li>Open an issue in this GitHub repo  </li> <li>Email us at datatoinsight.enquiries@gmail.com </li> </ul>"}]}