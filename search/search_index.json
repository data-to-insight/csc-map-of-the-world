{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"About this project","text":"<p>Map of the World is an open-source knowledge base for Children\u2019s Social Care (CSC). It brings together key documents, tools, services, and guidance into one structured, searchable map.</p>"},{"location":"#what-youll-find","title":"What you\u2019ll find","text":"<ul> <li>A visual network showing how projects, organisations, and frameworks connect  </li> <li>A smart search tool for quick access to relevant information  </li> <li>Simple, human-readable data formats so anyone can contribute  </li> </ul> <p>The aim is to make the CSC landscape clearer and more connected. Built on open technology, this project is designed to be transparent, flexible, and collaborative, helping the sector work smarter, not harder.</p> <p>This proof of concept (PoC) forms part of the work toward a Centre of Excellence for CSC, building on thinking behind platforms like the Children\u2019s Services Network and using open modelling approaches such as the Smart City Concept Model. It is designed to be lightweight, transparent, and openly extensible so others can adopt or adapt it for their own contexts.</p>"},{"location":"#purpose","title":"Purpose","text":"<p>This PoC is the first step toward creating a clear, searchable map of everything happening in CSC. It will bring together reports, tools, frameworks, guidance, and projects into one place so people can easily find and understand what is out there. We aim to make it simple to:</p> <ul> <li>See how initiatives, policies, and systems connect  </li> <li>Find out who is working on what and track updates  </li> <li>Bring hidden or siloed work into view to align efforts and avoid duplication  </li> <li>Give local teams a clearer picture of sector-wide activity  </li> </ul>"},{"location":"#whats-included","title":"What\u2019s included","text":"<ul> <li>Published reports and official guidance  </li> <li>Data from government and local authority websites  </li> <li>Sector-developed tools and frameworks  </li> <li>Links to people and organisations (where consent or public information allows)  </li> </ul> <p>All accessible through an intuitive interface with a visual map and search tools.</p>"},{"location":"#who-will-use-it","title":"Who will use it","text":"<p>Local authority data teams, service managers, researchers, analysts, and project leads working on CSC data or digital delivery. Ultimately, this is a collaborative tool, built with input from local authorities, analysts, service leads, academic partners, and national bodies.</p>"},{"location":"#how-this-helps","title":"How this helps","text":"<p>This tool will make life easier across the CSC sector by:</p> <ul> <li>Showing the bigger picture, clarifying how local and national initiatives, policies, systems, and data sources fit together  </li> <li>Highlighting who is doing what, enabling you to track new tools, framework updates, and service changes  </li> <li>Bringing hidden work into view, helping teams align efforts, build on progress, and avoid duplication  </li> <li>Giving local authority data teams a more joined-up view of sector-wide activity  </li> </ul>"},{"location":"#the-plan","title":"The plan","text":"<ul> <li>Interactive network map, explore a live graph showing how entities, relationships, and systems connect across the CSC sector.  </li> <li>Structured data records, a growing library of YAML files (human-readable format) based on the Smart City Concept Model (SCCM, BSI PAS 182). These describe tools, frameworks, relationships, rules, plans, events, and guidance.  </li> <li>Documentation hub, technical documentation from Data to Insight (D2I) projects and Git repositories indexed for developers and analysts.  </li> <li>Searchable resource, a dedicated search page indexing YAML content and files (.md, .pdf, .py, .js, .html) with keyword relevance, match scoring, and metadata extraction.  </li> <li>In development, currently using a small sample of local authority data (around 10 sites), with plans to expand to all 153 authorities and scrape relevant CSC public sources (.gov, .edu).  </li> </ul>"},{"location":"#how-it-is-structured","title":"How it is structured","text":"<p>The tool is built around the Smart City Concept Model (SCCM), an open framework for describing public service ecosystems. Every item in the network diagram is stored as a YAML file, which is simple and easy to edit. Each file starts with a type, for example:</p> <ul> <li>AGENT, people, teams, or organisations  </li> <li>SERVICE, a system, service, or tool  </li> <li>EVENT, inspections, launches, or reviews  </li> <li>RULE / PLAN / COLLECTION, policies, datasets, or strategies  </li> <li>RELATIONSHIP, links between entities (for example oversight, supply, influence)  </li> </ul> <p>These YAML files are validated, searchable, and easier to edit than formats like JSON or CSV, making contributions simpler.</p>"},{"location":"#how-to-get-involved","title":"How to get involved","text":"<p>This project is being built with the sector, for the sector, and we would welcome your input. You can help by:</p> <ul> <li>Sharing feedback on what works, what is missing, or what needs fixing  </li> <li>Contributing local projects, tools, or documentation  </li> <li>Suggesting ideas for how this tool could better support the sector  </li> </ul> <p>To get involved, contact the Data to Insight team or visit our GitHub repository to explore, fork, or contribute.</p> <p>If you think that this tool might be useful within the sector and want to pass this on to colleagues, feel free to make use of the below infographic that we thought might offer the key headline insights more succintly.</p> <p></p> <p></p> <p>  This PoC forms part of the work towards a Centre of Excellence, building on the thinking behind platforms like the Children\u2019s Services Network and grounded in open modelling approaches like the already mentioned Smart City Concept Model.</p> <p> </p> <p> </p>"},{"location":"#soft-systems-conceptual-mapping","title":"Soft Systems Conceptual Mapping","text":"<p>System of Interest </p> <p>Shared|public data platform and ecosystem used within Children\u2019s Social Care to connect people|LA colleagues, data, tools, and services</p> <p>Purpose </p> <p>To enable shared sector understanding, validation, discovery, and collaboration between local authorities, tool development, projects and other initiatives in CSC</p> <p>Worldview (Weltanschauung) </p> <p>Fragmented data landscapes transformed into a collaborative, open ecosystem using lightweight, transparent structures like SCCM, JSON, YAML + MkDocs</p> <p>Owner(s) </p> <p>Likely data platform stewards: D2I, local authority data teams, ecosystem developers</p> <p>Environment (External Constraints) </p> <p>GitHub Pages (no backend), data security and ethics, evolving standards, distributed maintenance, changing ecosystem, frameworks and statuatory guidance, browser-only deployments</p>"},{"location":"#whats-next","title":"What\u2019s next?","text":"<ul> <li>Ongoing expansion of linked tools, rules, and frameworks</li> <li>Live|scheduled scrapes from key web resources or published docs/framesworks</li> <li>Search and filter interface (in beta, but aiming to implement network diagram filters)</li> <li>Option for local teams to submit structured entries or link live repositories</li> <li>Export options for integration into other data tools</li> </ul> <p>Thanks for the interest in CSC Knowledge Base Network We hope it supports your work, and welcome your feedback as we continue to improve and expand it.</p>"},{"location":"#possible-sccm-mapping-to-csc-eco-system","title":"Possible SCCM Mapping to CSC Eco-System","text":"SCCM Concept (Category) Suggested example(s) (in progress) Community South East fostering cluster Documentation CSC Independent Review Events Children\u2019s Social Care Review, ILACS Inspections, Public Inquiries Organization Data to Insight, LIIA Persons Organisational/sector tools linked where consent given or public record Plans Kinship Care Strategy, Children\u2019s Social Care National Framework Relationships LA-1 \u2194 Supports \u2194 SSD Tests, DfE \u2194 Pilots \u2194 API Data Flows Rules Statutory Guidance, Keeping Children Safe in Education 2025 Sector Tools PATCH, ChAT Services"},{"location":"data_source_optimisation/","title":"Optimising Search for \u201cMap of the World\u201d","text":"<p>From PDFs to compact, fast, portable indexes</p>"},{"location":"data_source_optimisation/#why-this-matters-for-childrens-social-care-data","title":"Why this matters for children\u2019s social care data","text":"<p>Children\u2019s social care information is extensive and often spread very wide across government and partner websites, (long)PDFs, and research portals. Our approach focuses(in-part) on making that material accessible and searchable without moving heavy files around or duplicating storage.</p> <ul> <li>Fast, lightweight search: we extract text once and create a minimised search index (no PDFs, just raw optimised data)</li> <li>Clarity and provenance: chunk-level search surfaces the exact clauses (e.g. thresholds, Section 47, kinship care) with links back to the authoritative source and licence</li> <li>Low overhead, high resilience: compact Parquet/FAISS artefacts are inexpensive to store (e.g. in R2), simple to mirror, and easy to reuse locally by LAs/sector stakeholders</li> <li>Keeps pace with change: incremental builds pick up updates. Reprocessing just what changed, so the map stays current as guidance, published reports or sector research evolves</li> <li>Defined scope boundaries: Defined licensing metadata, and explicit source boundaries ensure ring-fenced sector scope</li> </ul>"},{"location":"data_source_optimisation/#who-this-is-for-and-what-youll-get","title":"Who this is for (and what you\u2019ll get)","text":"<ul> <li>Product / content folks: how we take lots of PDFs and turn them into a tiny, fast search index that still finds the right things.</li> <li>Engineers / data people: the design choices, math, and scaling numbers so you can reuse this pipeline on your own document sets.</li> </ul> <p>One\u2011line summary: we extract text -&gt; split into semantically sensible chunks -&gt; embed each chunk as a vector -&gt; store compactly in Parquet using uint8 quantisation -&gt; build a FAISS similarity index (HNSW by default). The public search can then use a small JSON keyword index for near-instant client\u2011side search; deeper semantic search uses the vector index as needed.</p>"},{"location":"data_source_optimisation/#why-this-approach-works","title":"Why this approach works","text":"<ol> <li>We do heavy lifting once, locally. PDFs are slow to parse and (comparitively)large to store especially at scale; we extract the text and avoid shipping raw files within the platform. </li> <li>We chunk long documents. Searching against topically coherent chunks(minimised to the most relevant) beats whole\u2011document matching for retrieval quality. </li> <li>We use sentence embeddings. Similarity in embedding space approximates \u201cis this about the same thing?\u201d</li> <li>We store vectors compactly. Quantising embeddings to uint8 shrinks storage ~4\u00d7 with negligible impact on retrieval quality for our use case(s). - Although we are still defining/asking about the sector's possible use-cases. </li> <li>We separate UX search from deep search. The site ships a tiny keyword index for instant(or at least faster) results; we're also aiming for external LA workflows being able to use the vector index offline.</li> </ol>"},{"location":"data_source_optimisation/#what-tiny-recall-loss-means-plain-english","title":"What \u201ctiny recall loss\u201d means (plain English)","text":"<p>When we save embeddings as uint8 instead of 32\u2011bit floats, we\u2019re using a lossy compression scheme. That slightly perturbs numbers. If someone later rebuilds a vector index from those saved numbers, the nearest\u2011neighbour results may differ a bit from a float32 baseline.</p> <p>In practice with normalised sentence embeddings:</p> <ul> <li>Cosine similarity between original and dequantised vectors is usually \u2265 0.995 (to around ~0.999)</li> <li>End\u2011to\u2011end recall@10 typically changes by 0-2% on common text corpora</li> <li>Our live FAISS index is built directly from float32 in memory, so website retrieval quality is unaffected by how we store vectors in Parquet</li> </ul>"},{"location":"data_source_optimisation/#the-quick-math","title":"The quick math","text":"<p>L2\u2011normalise each embedding vector <code>x</code> (so values lie roughly in [-1, 1]) and map each component to 8 bits:</p> <ul> <li>Quantise: <code>q = round((x + 1) * 127.5)</code> -&gt; <code>q \u2208 {0,\u2026,255}</code> </li> <li>Dequantise: <code>x' = (q / 127.5) - 1.0</code> </li> <li>Re\u2011normalise: <code>x'' = x' / ||x'||</code> (keeps cosine similarity meaningful)</li> </ul> <p>Because most information in sentence embeddings is in direction (not exact magnitudes), cosine similarity is very stable after this process.</p>"},{"location":"data_source_optimisation/#the-pipeline-what-and-why","title":"The pipeline (what and why)","text":"<p>This a reference for devs and those managing the platform/repo back-end stuff, probably not relevant to most stakeholders. </p> <ol> <li>Text extraction (PyMuPDF; pdfminer fallback)  </li> <li>Why: PyMuPDF is 2-5\u00d7 faster on many PDFs  </li> <li>Chunking (default ~1,800 chars with ~150 overlap)  </li> <li>Why: balances context and index size; fewer, richer chunks -&gt; faster builds and smaller artefacts  </li> <li>Embedding (<code>all-MiniLM-L6-v2</code>, 384\u2011dim, normalised)  </li> <li>Why: Small, fast, good quality; normalisation makes cosine = inner product and improves quantisation robustness  </li> <li>Storage </li> <li>Per\u2011doc Parquet (text + metadata) -&gt; easy incremental updates  </li> <li>Vectors in Parquet as <code>uint8</code> -&gt; ~4\u00d7 smaller than float32, plus schema metadata describing de/quantisation  </li> <li>Combined Parquet (optional) for simple downstream use if LAs have a use for  </li> <li>Indexing (FAISS)  </li> <li>HNSW default: excellent recall, easier to tune  </li> <li>IVF\u2011PQ optional: order\u2011of\u2011magnitude smaller index for very large corpora; small recall trade\u2011off, tunable with <code>nprobe</code></li> <li>Incremental builds </li> <li><code>state.json</code> records per\u2011document SHA\u2011256; unchanged docs are skipped. Rebuilds are proportional to change, not corpus size</li> <li>Website search index (keyword JSON) </li> <li>From Parquet, not PDFs. Cleaned text, remove stop\u2011words and over\u2011common terms, derive per\u2011doc keywords -&gt; a very small JSON file for client\u2011side search</li> </ol>"},{"location":"data_source_optimisation/#realworld-impact-for-users","title":"Real\u2011world impact for users","text":"<ul> <li>Fast search: small on\u2011site JSON makes type\u2011ahead and filters (appear) near-instant  </li> <li>Better hits: chunk\u2011level indexing can return relevant text blocks from extensive PDFs  </li> <li>Stable URLs: Not yet in use. But would be backlinks within the platform to object specific detail(s)  </li> <li>Scales: adding thousands of documents has less of impact on site response and repo bloat  </li> </ul>"},{"location":"data_source_optimisation/#realworld-impact-for-devsengineersplatform","title":"Real\u2011world impact for devs/engineers/platform","text":"<p>This a reference for devs and those managing the platform/repo back-end stuff, probably not relevant to most stakeholders. </p> <ul> <li>Artefacts, not assets: Git &amp; Pages store compact Parquet/JSON, not PDFs  </li> <li>Cheap(er) storage: vectors as <code>uint8</code> and HNSW/IVF\u2011PQ keep Git/R2 costs low(er)  </li> <li>Reproducible builds: manifest + hashes = deterministic rebuilds  </li> <li>Interoperability: Parquet + FAISS are standard; others can reuse the corpus locally  </li> </ul>"},{"location":"data_source_optimisation/#sizing-scaling-rules-of-thumb","title":"Sizing &amp; scaling (rules of thumb)","text":"<p>This a reference for devs and those managing the platform/repo back-end stuff, probably not relevant to most stakeholders. </p> <p>Let C = number of chunks. With 384\u2011dim embeddings:</p> <ul> <li>Float32 vector size (raw): <code>C \u00d7 384 \u00d7 4</code> bytes  </li> <li>UInt8 vector size (raw): <code>C \u00d7 384 \u00d7 1</code> bytes (~4\u00d7 smaller)  </li> <li>HNSW index size: typically ~1-2 KB per chunk (depends on <code>M</code>, dataset)  </li> <li>Parquet overhead: +10-30% over raw, but ZSTD helps especially for text  </li> </ul>"},{"location":"data_source_optimisation/#example-projections","title":"Example projections","text":"<p>Assuming current chunking yields ~74 chunks/doc (observed ~1,928 chunks for 26 docs) -&gt; 3,000 docs ~ 222k chunks </p> Component Formula 222k chunks (est.) Parquet vectors (float32 raw) <code>C \u00d7 384 \u00d7 4</code> ~341 MB (+overhead) Parquet vectors (uint8 raw) <code>C \u00d7 384 \u00d7 1</code> ~85 MB (+overhead) HNSW index ~1-2 KB \u00d7 C ~220-440 MB Parquet text+meta compressed tens of MB (depends on text volume) <p>If HNSW index starts feeling large to juggle, flip to IVF\u2011PQ: you can expect tens of MB with recall@10 often 95-99%, tunable via <code>nprobe</code>.</p>"},{"location":"data_source_optimisation/#keyword-index-whats-in-tiny-json","title":"Keyword index (what\u2019s in tiny JSON)","text":"<p>A compact, derived per\u2011document keyword list from the Parquet text:</p> <ul> <li>Normalise punctuation/quotes and remove boilerplate (e.g., page headers)  </li> <li>Lowercase, keep words of 4+ letters  </li> <li>Remove stop\u2011words (scikit\u2011learn English) and domain\u2011common words (CSC custom list of high freq/low importance words)  </li> <li>Filter too\u2011common terms with <code>max_df</code> (e.g., drop tokens appearing in &gt;85% of docs) and too\u2011rare with <code>min_df</code> (e.g., keep tokens in \u22652 docs)</li> </ul> <p>This makes <code>docs/search_index.json</code> tiny and fast(er) for lookup/searches.</p>"},{"location":"data_source_optimisation/#why-hnsw-now-ivfpq-later","title":"Why HNSW now, IVF\u2011PQ later?","text":"<ul> <li>HNSW gives near\u2011exact recall with simple knobs (<code>M</code>, <code>efConstruction</code>, <code>efSearch</code>), great for up to a few hundred thousand vectors on a single machine.</li> <li>IVF\u2011PQ compresses vectors and partitions the space. It is useful when you need smaller indexes and faster queries at scale. We trade some/a little recall, but can dial it back with <code>nprobe</code> and codebook size.</li> </ul> <p>Switching rule of thumb: if chunks exceed ~200-250k or HNSW file becomes problematic (&gt;~300 MB), toggle IVF\u2011PQ.</p>"},{"location":"data_source_optimisation/#interoperability-reuse","title":"Interoperability &amp; reuse","text":"<ul> <li>Parquet is easy to read from Python, R, Spark, DuckDB, Polars, etc.</li> <li>FAISS is a standard ANN engine; the index can be loaded locally or by a lightweight service.</li> <li>The corpus is self\u2011describing: Parquet schema metadata indicates whether embeddings are <code>float32</code> or quantised (<code>uint8</code> with dequant rule).</li> </ul>"},{"location":"data_source_optimisation/#frequently-asked-but-will-it","title":"Frequently asked \"but will it\u2026?\"","text":"<ul> <li> <p>\u2026find the right stuff in compressed vectors?   Yes-for docs and model, differences are tiny. We've normalised vectors, which preserves cosine structure.</p> </li> <li> <p>\u2026scale to thousands of PDFs?   Yes. The heavier corpus build, is seperate and done externally, size via incremental hashing, hence only compact artefacts. </p> </li> <li> <p>\u2026lock me into specific, or paid 3rd-party|software?   No. We've aimed for an entirely open/Git-centric approach on this + open formats (Parquet, FAISS) + standard Python libs</p> </li> <li> <p>\u2026support OCR/scanned PDFs?   No. But it could. WE just dont see the immeadiate need for this within the sector. Could later add Tesseract or cloud OCR  if needed. Can add later if any colleagues are aware of such docs. </p> </li> </ul>"},{"location":"data_source_optimisation/#key-libraries-and-why-we-chose-them","title":"Key libraries and why we chose them","text":"<ul> <li>PyMuPDF (<code>pymupdf</code>): fast, robust PDF text extraction  </li> <li>Sentence\u2011Transformers: high\u2011quality sentence embeddings + easy APIs  </li> <li>FAISS: full vector search (HNSW, IVF\u2011PQ) with scalable performance  </li> <li>PyArrow/Parquet: portable column based storage; perfect for text + vectors + metadata  </li> <li>scikit\u2011learn: light\u2011weight keyword vocab building with stop\u2011word &amp; frequency filters</li> </ul>"},{"location":"data_source_optimisation/#takeaways","title":"Takeaways","text":"<ul> <li>By separating heavy preprocessing from what's hosted in the main/mkdocs site here, plus by quantising vectors, we massively cut the needed storage and bandwidth without compromising colleagues' experience in the search etc</li> <li>The site\u2019s search is able to be near-instant with smaller footprint, and an underlying corpus remains portable and reusable for future or onward data projects - although we're not yet sure of the use-cases on this.</li> </ul>"},{"location":"data_source_optimisation/#faiss-and-parquet-artefacts-structure-and-reference","title":"FAISS and Parquet artefacts: structure and reference","text":"<p>A bit about how derived files are organised within the project, what's inside Parquet tables, and what the FAISS index stores. </p>"},{"location":"data_source_optimisation/#repo-layout-derived-artefacts","title":"Repo layout (derived artefacts)","text":"<pre><code>artifacts/\n  state.json                        # per-document hashes, build config, last_built\n  chunks/                           # per-document Parquet (text + metadata)\n    1a2b3c4d5e6f7a8b.parquet\n    9c0d1e2f3a4b5c6d.parquet\n    ...\n  vectors/                          # per-document Parquet (text + vectors)\n    1a2b3c4d5e6f7a8b.parquet\n    9c0d1e2f3a4b5c6d.parquet\n    ...\n  motw_chunks.parquet               # combined (optional)\n  motw_vectors.parquet              # combined (optional)\n  motw_index.faiss                  # FAISS HNSW index (default)\n  motw_index_ivfpq.faiss            # FAISS IVF-PQ index (optional)\n</code></pre> <ul> <li><code>doc_id</code> is the first 16 hex of the SHA-256 of the raw file content.</li> <li>Per-document Parquet files enable incremental builds and quick reloads.</li> <li>Combined Parquet files are optional and exist for convenience.</li> </ul>"},{"location":"data_source_optimisation/#parquet-schemas","title":"Parquet schemas","text":""},{"location":"data_source_optimisation/#per-document-chunks-parquet-artifactschunksdoc_idparquet","title":"Per-document chunks Parquet (<code>artifacts/chunks/&lt;doc_id&gt;.parquet</code>)","text":"<p>Schema:</p> <pre><code>doc_id:      string          # 16-char hex id (content address)\nchunk_id:    int32           # 0..N-1 per document\nsource_path: string          # original path on disk\nsource_name: string          # original filename (for example My_File.pdf)\ntext:        string          # chunked, cleaned text\n</code></pre> <p>Example row:</p> doc_id chunk_id source_name text (truncated) 1a2b3c4d5e6f7a8b 0 ADCS_Safeguarding_...pdf Local authorities should..."},{"location":"data_source_optimisation/#per-document-vectors-parquet-artifactsvectorsdoc_idparquet","title":"Per-document vectors Parquet (<code>artifacts/vectors/&lt;doc_id&gt;.parquet</code>)","text":"<p>Same columns as chunks, plus one embedding column:</p> <ul> <li>If stored as float32:   <code>embedding: list&lt;float32&gt;    # length 384</code></li> <li>If stored as uint8 (space-saving default):   <code>embedding_q: list&lt;uint8&gt;    # length 384, quantised</code></li> </ul> <p>Parquet schema metadata (present on the vectors file) indicates storage:</p> <pre><code>motw.embedding.storage = \"float32\"      # or \"uint8_sym\"\nmotw.embedding.dequant  = \"x = (q / 127.5) - 1.0\"\n</code></pre> <ul> <li>For <code>embedding_q</code>, clients can reconstruct an approximate float vector as:</li> <li><code>x = (q / 127.5) - 1.0</code>, then L2-normalise <code>x</code> before cosine similarity.</li> <li>Vectors were normalised during embedding, which keeps cosine behaviour stable after dequantisation.</li> </ul>"},{"location":"data_source_optimisation/#combined-parquet-files-optional","title":"Combined Parquet files (optional)","text":"<p><code>artifacts/motw_chunks.parquet</code> and <code>artifacts/motw_vectors.parquet</code> have the same schemas as above, but hold all docs together. They are convenient for one-shot analysis and for users/LA colleagues that do not want to iterate per-doc files.</p>"},{"location":"data_source_optimisation/#faiss-index-files","title":"FAISS index files","text":"<p>The pipeline can build either:</p>"},{"location":"data_source_optimisation/#hnsw-index-default-artifactsmotw_indexfaiss","title":"HNSW index (default) - <code>artifacts/motw_index.faiss</code>","text":"<p>Stored properties inside the binary:</p> <ul> <li><code>d</code> -&gt; 384 (embedding dimension)</li> <li><code>metric</code> -&gt; inner product (cosine when vectors are normalised)</li> <li><code>ntotal</code> -&gt; number of vectors (equals total chunks)</li> <li>Graph parameters saved with the index:</li> <li><code>M</code> -&gt; graph degree used at build time (for example 32)</li> <li><code>efConstruction</code> -&gt; build breadth (for example 80)</li> <li>Query-time parameter you set after loading:</li> <li><code>efSearch</code> -&gt; search breadth (for example 64 or 128). Higher -&gt; better recall, slower</li> </ul> <p>Notes: - Excellent recall and speed up to a few hundred thousand vectors - File size typically ~1-2 KB per vector, depending on <code>M</code> and data</p>"},{"location":"data_source_optimisation/#ivf-pq-index-optional-artifactsmotw_index_ivfpqfaiss","title":"IVF-PQ index (optional) - <code>artifacts/motw_index_ivfpq.faiss</code>","text":"<p>Stored properties:</p> <ul> <li><code>d</code> -&gt; 384</li> <li><code>metric</code> -&gt; inner product</li> <li><code>ntotal</code> -&gt; number of vectors</li> <li>Coarse quantiser and codebooks:</li> <li><code>nlist</code> -&gt; number of coarse centroids (for example 1024)</li> <li><code>m</code> -&gt; number of subquantisers (for example 16)</li> <li><code>nbits</code> -&gt; bits per subvector (for example 8)</li> <li>Trained centroids and PQ codebooks included in the file</li> <li>Query-time parameter (set after loading):</li> <li><code>nprobe</code> -&gt; how many coarse lists to search (for example 16, 32, 64). Higher -&gt; better recall, slower.</li> </ul> <p>Notes: - Much smaller on disk than HNSW (often tens of MB), with a small recall trade-off - tune via <code>nprobe</code>.</p>"},{"location":"data_source_optimisation/#minimal-load-examples-for-ref","title":"Minimal load examples (for ref)","text":"<pre><code># Load FAISS index\nimport faiss\nindex = faiss.read_index(\"artifacts/motw_index.faiss\")  # or motw_index_ivfpq.faiss\n# For HNSW, can increase recall at query time:\ntry:\n    index.hnsw.efSearch = 64\nexcept AttributeError:\n    pass\n# For IVF-PQ:\ntry:\n    index.nprobe = 32\nexcept AttributeError:\n    pass\n</code></pre> <pre><code># Load vectors from Parquet (handles float32 or uint8 storage)\nimport pyarrow.parquet as pq\nimport numpy as np\n\nt = pq.read_table(\"artifacts/motw_vectors.parquet\")  # or per-document under artifacts/vectors/\ncols = set(t.schema.names)\n\nif \"embedding\" in cols:  # float32\n    embs = np.array([np.array(x) for x in t.column(\"embedding\").to_pylist()], dtype=\"float32\")\nelif \"embedding_q\" in cols:  # uint8\n    q = np.array([np.array(x, dtype=\"uint8\") for x in t.column(\"embedding_q\").to_pylist()], dtype=\"uint8\")\n    x = (q.astype(\"float32\") / 127.5) - 1.0\n    # re-normalise to unit length\n    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-6\n    embs = x / n\nelse:\n    raise ValueError(\"No embedding column found\")\n</code></pre>"},{"location":"data_source_optimisation/#how-these-pieces-fit-the-platform","title":"How these pieces fit the platform","text":"<ul> <li>Parquet carries all chunk text and metadata for analytics, auditing, and lightweight keyword indexing.</li> <li>FAISS provides high-quality semantic retrieval over the same chunks.</li> <li>Storing Parquet embeddings as uint8 keeps long-term storage small while preserving near-identical search behaviour when vectors are dequantised and normalised.</li> <li>HNSW is the default for accuracy and simplicity. IVF-PQ is available when a much smaller index is needed, with tunable recall.</li> </ul>"},{"location":"data_source_optimisation/#glossary","title":"Glossary","text":"<ul> <li>Parquet: a columnar file format for tables. Efficient to store and quick to scan. Works well with Pandas, DuckDB, Spark and friends.</li> <li>FAISS: Facebook AI Similarity Search - a library for fast similarity search over vectors (numerical representations of text). Lets you find \"things like this\" quickly. </li> <li>HNSW: Hierarchical Navigable Small World graph. A very fast index structure used by FAISS for high\u2011quality approximate nearest\u2011neighbour search.</li> <li>Artefacts: the files we produce from processing (e.g. Parquet tables and FAISS indexes) that you can store, publish, or reuse.</li> <li>L2\u2011normalise: scale a vector so its length is 1. This keeps cosine similarity meaningful and makes quantisation behave well.</li> </ul>"},{"location":"dev_log/","title":"Development Log \u2014 MapOfTheWorld","text":""},{"location":"dev_log/#2025-12-02","title":"2025-12-02","text":"<ul> <li>Quite a gap since last working on this... although tbh lots of entries missing since last update here. </li> <li>Fix a number of minor problems that i left hanging last time, namely:</li> <li>explorer search was not doing anything, </li> <li>explorer search kept showing the full results list even after selecting one, wasnt ideal. Now the rest disappear   </li> <li>explorer info panel, this was only showing minimum data, not the fuller data from node_Details.json   </li> <li>explorer was duplicating edges when multi-press add 1 hop... (we needed to store bi-directional edge indexes)  </li> <li>Added 2 hops option and made buttons more visible instead of just text (bit more css)  </li> <li>Alison joins us and re-worked the intro page text, and nice infographic  </li> <li>did some work on data_source_optimisation.md content  </li> <li>Finding it really difficult to recall all the needed back-end processing and the process flow for all this now... maybe need to both tone down some of the docs, or just do my own idiots guide to refresh/remind myself how to work this. In particular Need to re-check how to externally process and which files come up(search_index.json, state.json, + other /csc_artifacts or into docs/data/ ? - when i ran the external notebook.... it only updated some files and not the large parquet/fais files? )</li> <li>Timed the graph load on network_fullscreen.md, so i could add 'it's gonna take Xminutes to load, grab a drink :)' (was ~50secs in off-peak time)</li> </ul>"},{"location":"dev_log/#2025-08-06-review-the-savvi-model-concepts-agin","title":"2025-08-06 - Review the SAVVI model concepts agin","text":"<ul> <li>Landed back on the SAVVI model, wondering if that was perhaps a better fit for htis project. Decided against. </li> </ul>"},{"location":"dev_log/#2025-08-05-input-form-logic-with-yaml-export","title":"2025-08-05 \u2014 Input form logic with YAML export","text":"<ul> <li>Finalised static HTML form for submitting new entries to the Map of the World</li> <li>Form supports(relevant) <code>@type</code> values defined in the SCCM-aligned data model: ORGANIZATION, SERVICE, EVENT, PLAN, COLLECTION, PERSON, RESOURCE, RELATIONSHIP</li> <li>Each type has type specific field section, dynamically shown/hidden on selection</li> <li>RELATIONSHIP includes source type dropdown to filter valid <code>relationship_type</code> options</li> <li>To help users, placeholder examples added (in grey text) for multi-value fields like Collaborators, Related Entities, Interests, etc</li> <li>Placeholders are auto-stripped from YAML output if unchanged(to make sure examples dont go into yml output)</li> <li>When <code>@type</code> changes, previously generated YAML box cleared to avoid confusion between entries</li> </ul>"},{"location":"dev_log/#implementation-rationale","title":"Implementation rationale","text":"<p>The form built using a lightweight HTML/CSS/JS stack - no frameworks or backend, relying only on js-yaml to handle YAML serialisation in browser. This driven by the following initial thinking:</p>"},{"location":"dev_log/#full-compatibility-with-github-pages","title":"Full compatibility with GitHub Pages","text":"<p>Form intended to be served directly from the repo \u2014 no server, no build process. Fully static ensures GitHub Pages compatibility, i.e. support for only static HTML/CSS/JS. This ensures all aspects of the form is human-readable, no reliance on 3rd party form, JS frameworks, or backend APIs that could disappear or break over time. </p>"},{"location":"dev_log/#git-native-and-open-source-aligned","title":"Git-native and open source aligned","text":"<p>The YAML output integrates directly with the project\u2019s Git-based workflow \u2014 contributors can download the file or submit via PR(not yet implemented), and the data remains version-controlled. This supports the vision of open, and collaborative mapping of the CSC ecosystem.</p>"},{"location":"dev_log/#problems-encountered","title":"Problems encountered","text":"<ul> <li>Type-specific sections not appearing: logic flaw, only <code>ORGANIZATION</code> and <code>RELATIONSHIP</code> type fields were initially showing. fixed by ensuring <code>toggleTypeSections()</code> logic applied on both load and change.</li> <li>YAML not generating or clearing the form unexpectedly: \"Generate YAML\" button reset the form and wipe data. caused by incorrect form handling, resolved by preventing the default submit behaviour.</li> <li>Type-specific fields missing from YAML output: type fields not always included in the final output. fixed by reintroducing and checking all relevant fields per type</li> <li>Placeholder values being treated as real input: <code>example_value</code> showing in YAML. We introduced <code>cleanList()</code> and <code>cleanTextList()</code> utilities to strip placeholder content automatically.</li> </ul>"},{"location":"dev_log/#2025-08-01-mailto-links-devlog-structure-and-public-facing-dev-log","title":"2025-08-01 \u2014 Mailto links, devlog structure, and public-facing dev log","text":"<ul> <li>Revised 'suggest improvement|fix' mailto to make it easier for direct contributions</li> <li>define a structure for this devlog. Using as a hybrid devlog/changelog</li> <li>Skipped tags and automation for now \u2014 Markdown works well and is fast to edit manually</li> </ul> <p>Notes: </p>"},{"location":"dev_log/#2025-07-31-cytoscape-class-filters-bug-stlite-graph-fixes","title":"2025-07-31 \u2014 Cytoscape class filters bug, stlite graph fixes","text":"<ul> <li>Found a stubborn bug where class-based filters in Cytoscape weren\u2019t applying on page load</li> <li>After investigating, I discovered that class assignment lagged layout rendering. A simple <code>setTimeout</code> workaround after graph layout stabilisation solved it</li> <li>Also fixed a mismatch in hardcoded colours and class names in the static legend \u2014 \u2018ORGANIZATION\u2019 nodes were showing grey due to casing</li> </ul>"},{"location":"dev_log/#2025-07-30-static-legend-switch-for-graph-ui","title":"2025-07-30 \u2014 Static legend switch for graph UI","text":"<ul> <li>The dynamic legend system was fragile, especially when class names didn\u2019t match types consistently</li> <li>I rebuilt the legend as a static block \u2014 hardcoding known entity types like <code>ORGANIZATION</code>, <code>SERVICE</code>, <code>PERSON</code> etc. with consistent colours</li> <li>Filter defaults now apply correctly on page load. The legend also starts collapsed, which feels cleaner</li> </ul>"},{"location":"dev_log/#2025-07-29-yaml-errors-and-edge-failures-in-cytoscape-build","title":"2025-07-29 \u2014 YAML errors and edge failures in Cytoscape build","text":"<ul> <li>Ran into YAML errors during <code>admin-build_cytoscape_json.py</code> \u2014 one malformed file broke the whole build</li> <li>Also noticed skipped edges due to missing nodes, traced to naming mismatches (e.g. <code>data_to_insight -&gt; d2i_excel_toolkit_maintenance</code>)</li> <li>Rebuilt one broken relationship YAML from a working template, which fixed the issue \u2014 reinforcing the need for stricter YAML validation tooling</li> </ul>"},{"location":"dev_log/#2025-07-22-to-2025-07-26-external-data-inclusion-and-web-scraping","title":"2025-07-22 to 2025-07-26 \u2014 External data inclusion and web scraping","text":"<ul> <li>Decided to expand the knowledge base by integrating published web content (PDFs, guidance, reports)</li> <li>Designed a strategy to store scraped content under <code>data_web/</code> and generate YAML metadata for indexing</li> <li>Early blockers included inconsistent text extraction (e.g. malformed GOV.UK PDFs) and parsing issues for structured headers in some <code>.txt</code> and <code>.pdf</code> files</li> </ul>"},{"location":"dev_log/#2025-07-19-pivot-away-from-streamlitstlite-to-mkdocs-js","title":"2025-07-19 \u2014 Pivot away from Streamlit/stlite to MkDocs + JS","text":"<ul> <li>After several attempts to render interactive network graphs in <code>stlite</code>, I hit repeated blockers</li> <li>Despite simplifying the data and testing basic layouts, Cytoscape.js would either not load properly or failed silently due to lack of support for external JavaScript modules and delayed layout rendering</li> <li>These limitations made <code>stlite</code> too fragile for graph-based exploration \u2014 especially with filters, tooltips, and legend controls</li> <li>Decided to switch fully to MkDocs as the primary documentation and frontend base, embedding custom HTML + JavaScript components directly</li> <li>This offered a much more stable and extensible foundation for publishing network diagrams, filtered views, and layered data exploration</li> </ul> <p>Notes: This was a significant architectural shift, but it unlocked better search integration, reusable visual components, and static hosting via GitHub Pages without relying on Python runtime hacks</p>"},{"location":"dev_log/#2025-07-18-migration-to-stlite-for-frontend-hosting","title":"2025-07-18 \u2014 Migration to <code>stlite</code> for frontend hosting","text":"<ul> <li>Shifted from standard Streamlit to <code>stlite</code> to support deployment via GitHub Pages \u2014 keeping everything browser-based</li> <li>Had to strip out unsupported modules like <code>pyvis</code>, <code>pathlib.Path(__file__)</code>, and Parquet I/O</li> <li>Rebuilt the visualisation to use JSON and embedded Cytoscape.js directly in HTML</li> </ul>"},{"location":"dev_log/#2025-07-17-parquet-removal-and-frontend-browser-shift","title":"2025-07-17 \u2014 Parquet removal and frontend browser shift","text":"<ul> <li>Began adapting the Streamlit app to run fully in-browser using <code>stlite</code></li> <li>Removed Parquet-based data loading due to Pyodide/browser compatibility</li> <li>Replaced with pre-generated JSON stored in <code>data/index_data.json</code> and loaded via HTTP</li> <li>Shifted from Python visualisation tools to pure JS (Cytoscape.js) for performance and portability</li> </ul>"},{"location":"dev_log/#2025-07-16-sccm-node-type-update-for-yaml-compatibility","title":"2025-07-16 \u2014 SCCM node type update for YAML compatibility","text":"<ul> <li>Switched all entity type declarations from <code>@type: PERSON</code> to <code>@type: 'PERSON'</code> (with quotes) to avoid YAML parsing issues</li> <li>This change rippled through the data layer and required updates to validation and the graph builder</li> <li>Highlighted the need for stricter YAML schema validation down the line</li> </ul>"},{"location":"dev_log/#2025-07-14-fixing-schema-to-graph-disconnects","title":"2025-07-14 \u2014 Fixing schema-to-graph disconnects","text":"<ul> <li>Spotted broken links and dangling edges in the graph caused by mismatches between service/organisation IDs and their relationship definitions</li> <li>Rewrote edge-building logic to verify both subject and object exist before drawing the edge</li> <li>Output clearer error messages when skipping invalid relationships</li> </ul>"},{"location":"dev_log/#2025-07-09-visual-and-navigational-structure-rethink","title":"2025-07-09 \u2014 Visual and navigational structure rethink","text":"<ul> <li>Reorganised the <code>/docs/</code> folder to avoid the sprawl of earlier MkDocs projects</li> <li>Added grouped navigation for tools, scrapes, and thematic areas like Early Help, SEND, and Benchmarking</li> <li>Refined the script that auto-generates <code>mkdocs.yml</code> navigation from the folder structure</li> </ul>"},{"location":"dev_log/#2025-07-03-csv-and-contact-data-integration","title":"2025-07-03 \u2014 CSV and contact data integration","text":"<ul> <li>Added ability to merge contact lists from separate sources (e.g. Wix exports and curated CSVs)</li> <li>Extracted email domains and lowercased fields for consistency</li> <li>Laid early groundwork for people-entity mapping within the broader ecosystem graph</li> </ul>"},{"location":"explore/","title":"Network Explorer (search-first)","text":"Search: Keep neighbours (append) Clear all"},{"location":"graph_filtering_guidance/","title":"Graph Filtering Guide","text":"<p>Make the most of the network graph using type filters, a search box, and an optional context mode that reveals nearby connections. This page explains how it works and gives practical examples.</p>"},{"location":"graph_filtering_guidance/#quick-start","title":"Quick start","text":"<ul> <li>Type filter (chips/select): show only Organizations, Events, Plans, etc.</li> <li>Search box: find nodes by name, tags, summary text, or slug (we index these into a <code>search_blob</code> for fast matching).</li> <li>Context mode: optionally keep neighbours of matches visible so you can see how results connect.</li> <li>Shareable views: your current filter state is encoded in the URL (types + query), so you can copy the link and share what you\u2019re seeing.</li> <li>Legend counts: the legend shows live totals for visible nodes by type.</li> <li>Reset: click Reset view to clear filters and fit the graph.</li> </ul>"},{"location":"graph_filtering_guidance/#how-filtering-works","title":"How filtering works","text":"<p>Filtering is intersection-based:</p> <ul> <li>The type filter (e.g. Organizations + Events) selects what kinds of nodes are considered.</li> <li>The search box narrows this further to nodes whose text matches your query.</li> <li>The result is Type AND Search.</li> </ul> <p>Edges remain visible only if both endpoints are visible. This offers scope to reduce less relevant showing, keep the visualisation cleaner and relevant.</p>"},{"location":"graph_filtering_guidance/#what-text-is-searchable","title":"What text is searchable?","text":"<p>Each node has a lightweight <code>search_blob</code> built at publish time, which includes: - Name/label - Tags - Summary/description (shortened) - Slug (path-like identifier) - Type (e.g. <code>organization</code>, <code>event</code>)</p> <p>The search matches plain words anywhere in that blob.</p>"},{"location":"graph_filtering_guidance/#debounced-typing","title":"Debounced typing","text":"<p>When you type in the search box, the filter doesn\u2019t run on every keystroke. It waits ~150 ms for a short pause before applying the filter. </p> <p>Why we've done this: As we scale up the visualised network(graph), filtering can get expensive (show/hide many nodes, recompute edges, update legend). Debouncing reduces unnecessary work and makes your searches smooth(er) but importantly reduces our background processing overheads - making your interactions more rapid. </p> <p>Example: typing <code>ilacs</code> would normally trigger 5 full filter runs. With debounce, you typically get just 1\u20132 runs total.</p>"},{"location":"graph_filtering_guidance/#context-mode-keep-neighbours-of-matches-visible","title":"Context mode (keep neighbours of matches visible)","text":"<p>If Context mode is ON, then when your search finds some nodes, we also show their immediate neighbours and the edges between them. That gives you a mini ego-network around each hit, which we think offers better network exploration/search relevance.</p> <p>Example: search <code>tag:ilacs</code>. You\u2019ll see the nodes that are tagged <code>ilacs</code>, plus the organizations that run them, related events, or connected plans\u2014so you understand each hit in context.</p> <p>Turn it OFF when you want a strict, minimal view showing only the direct matches.</p>"},{"location":"graph_filtering_guidance/#simple-search-operators-power-users","title":"Simple search operators (power users)","text":"<p>You can mix operators with plain keywords to express intent quickly:</p> <ul> <li><code>type:org</code> \u2014 show organizations (same as picking Organizations in the type filter).</li> <li><code>tag:ilacs</code> \u2014 show nodes with the tag <code>ilacs</code>.</li> <li><code>type:event tag:training</code> \u2014 events tagged <code>training</code>.</li> <li><code>tag:ilacs charity</code> \u2014 nodes tagged <code>ilacs</code> and whose text contains <code>charity</code>.</li> <li><code>type:org type:event</code> \u2014 organizations or events (operator values are ORed).</li> </ul> <p>Rules: - Within the same operator (e.g., multiple <code>type:</code>), values are ORed. - Across different buckets (type, tag, and plain text), conditions are ANDed. - Text tokens (non-operator words) must all appear somewhere in the node\u2019s <code>search_blob</code>.</p> <p>Aliases: <code>type:org</code>, <code>type:organisation</code>, and <code>type:organization</code> are treated the same.</p>"},{"location":"graph_filtering_guidance/#combining-filters-examples","title":"Combining filters: examples","text":"<ul> <li> <p>Only organizations with ILACS in the text   Type filter: Organizations; Search: <code>ilacs</code> Shows only orgs whose name/summary/tags include \u201cilacs\u201d.</p> </li> <li> <p>Any node tagged ILACS that mentions Ofsted   Type filter: All; Search: <code>tag:ilacs ofsted</code> Matches the ILACS tag AND the word \u201cofsted\u201d appears in the node\u2019s text.</p> </li> <li> <p>Events or Organizations related to training   Type filter: All; Search: <code>type:event type:org training</code> Matches if the node is an Event OR Organization, AND contains \u201ctraining\u201d.</p> </li> <li> <p>Strict list without neighbours   Toggle Context mode OFF; Search: <code>tag:procurement framework</code> Only nodes that match will display (no neighbours). Useful for tight lists.</p> </li> </ul>"},{"location":"graph_filtering_guidance/#url-state-and-sharing","title":"URL state and sharing","text":"<p>When you change filters or search, the page updates the URL with your current types and query. Copy the URL to share your exact view with someone else, including the existing zoom/pan context.</p>"},{"location":"graph_filtering_guidance/#legend-counts","title":"Legend counts","text":"<p>The legend shows the number of visible nodes per type. Counts update as you filter so you can see how your query impacts the mix.</p>"},{"location":"graph_filtering_guidance/#resetting","title":"Resetting","text":"<p>Use the Reset view button to: - Clear the type filter and search query - Remove match highlighting - Fit the graph to the viewport</p>"},{"location":"graph_filtering_guidance/#troubleshooting-tips","title":"Troubleshooting tips","text":"<ul> <li>Nothing shows up? Clear the search box, check the type chips, or hit Reset view.</li> <li>Too much disappears when I type? Turn Context mode ON to keep neighbours of hits visible.</li> <li>I can\u2019t find a node I know exists. Try broader terms, check spelling, or search by <code>tag:</code> if you know one.</li> <li>Sharing a view. Copy the page URL after you\u2019ve set filters; it contains your state.</li> </ul> <p>This graph is built from YAML entities (Organization, Event, Plan, etc.) with extra build-time fields for search (slug, search blob). The filters you see operate entirely in the browser for quick, privacy-friendly exploration.</p>"},{"location":"index_PREV/","title":"CSC Knowledge Base Network","text":"<p>A structured, extensible open-source data-eco-system map and 'knowledge base' for the Children\u2019s Social Care (CSC) sector. This project collates and creates an overview of key sector documentation, project relationships, data services, sector tools, rules, plans and events using a flexible YAML-based data model aiming to align with the Smart City Concept Model (SCCM) towards data interoperability.</p> <p>Alongside the (filtered)graph-based relations visualisation, it aims to support key-term search and YAML schema validation across the structured/human readable <code>.yml</code> structured records. Development is scaffolded/designed to be extensible, transparent, and Git-native.</p> <p>With a focus on ensuring an entirely open-source and Git-native tool, there are some interesting and potentially constraining problems to solve just within the available tech-stack(even before we address the arguably bigger issues of how to bring all the relvant data together, and what's a sustainable internal structure). We're particularly interested in the Git data limits vs CSC sector need on data storage towards such as optimised full text searches. The potential data-estate in this exploratory development, alongside how we might take or manage direct sector input into the YML structure(s) also ongoing factors. </p>"},{"location":"index_PREV/#current-dev-phase-discovery-alpha","title":"Current Dev Phase: Discovery-Alpha","text":""},{"location":"index_PREV/#what-is-this-for","title":"What is this for?","text":"<p>This proof of concept(PoC) project explores how a structured, searchable map of Children\u2019s Social Care (CSC) data, tools, data projects, key frameworks, statuatoary guidance and activity could enable collaboration and support or optimise efforts across the sector. While exact use-cases are still emerging, our goal is to create a shared resource that brings together:</p> <ul> <li>published reports  </li> <li>pre-defined data objects  </li> <li>web data (e.g. from the DfE, local authorities, third parties)  </li> <li>sector-developed tools and frameworks</li> <li>connected people (organisational/sector tools linked where consent given or public record)  </li> </ul> <p>All of this would be made accessible through a visual or navigable interface, allowing users to explore connections between people, projects, standards, and services.</p> <p>We think this could help:</p> <ul> <li>Make relationships clearer \u2014 between local and national CSC initiatives, policies, systems, and data sources  </li> <li>Show who\u2019s doing what \u2014 helping users track new tools, updates to frameworks, or structural changes in services  </li> <li>Bring siloed or under-the-radar work into view \u2014 so efforts can align, build on each other, or avoid duplication  </li> <li>Support local teams \u2014 by contributing to a more joined-up picture of activity across the sector  </li> </ul> <p>We envisage use-cases from:</p> <ul> <li>Local authority data and performance teams</li> <li>Children\u2019s social care service managers and strategic leads</li> <li>Academic researchers and national analysts</li> <li>Project leads, developers and architects working in CSC data or digital delivery</li> </ul> <p>We see this as a collaborative mapping tool, developed potentially with input from local authority teams, analysts, service leads, academic partners, and national bodies.</p>"},{"location":"index_PREV/#plan","title":"Plan","text":"<ul> <li>Interactive network map: Navigate to Network to view entities, relationships, and systems as a live graph</li> <li>Structured data records: Underpinning the map is a growing library of structured YAML records, aligned to a SCCM concept framework(BSI as PAS 182) that describe:</li> <li>Tools and systems (e.g. PATCH, Validator 903)</li> <li>Frameworks and inspections (e.g. Ofsted ILACS, JTAI)</li> <li>Relationships and service models</li> <li>Rules, plans, events and guidance</li> <li>Searchable resource: The search page enables you to explore the structured data model directly.</li> <li>This is separate from the standard MkDocs search (top-right), which only covers page text within this site.</li> <li>The CSC knowledge search indexes structured YAML content as well as <code>.md</code>, <code>.pdf</code>, <code>.py</code>, <code>.js</code> and <code>.html</code> files, and supports keyword relevance, match scoring, and metadata extraction.</li> <li>(in dev)The search index|scope currently takes a data sample direct from local authority web sites. At the moment this is throttled to ~10, but with the potential to extract simplistic reference resource(s) directly from all ~153</li> <li>(in dev)The search index|scope aims to scrape from relevant CSC public data sources in order to increase the tool's search scope. This could schedule indexing of relevant documents or data sources from defined .gov or .edu sites.   </li> <li>Documentation hub: Local documentation from D2I projects(Git repos) is also live-indexed to provide technical context</li> </ul>"},{"location":"index_PREV/#how-is-this-structured","title":"How is this structured?","text":"<p>The aim was to align records in this tool with the Smart City Concept Model (SCCM), an open framework for describing public service ecosystems. Every entity towards the documented network(diagram) is represented as a YAML file, defined at the top level via SCCM concept types. Note: This does mean that for LA colleagues some language/grammer use might appear disconnected, incl. minor such as ORGANZATION vs ORGANISATION :</p> <p>e.g. - <code>@type: AGENT</code> \u2013 people, teams, or organisations - <code>@type: SERVICE</code> \u2013 a system, service or tool - <code>@type: EVENT</code> \u2013 events such as inspections, launches, reviews - <code>@type: RULE</code>, <code>@type: PLAN</code>, <code>@type: COLLECTION</code> \u2013 policy elements, datasets or strategies - <code>@type: RELATIONSHIP</code> \u2013 links between entities (e.g. oversight, supply, influence)</p> <p>YAML files are validated, searchable, and designed to be easier to contribute to as they're more human readable than other structured data forms (e.g. JSON, CSV... )</p> <p>Note: Further SCCM allignment examples under Possible SCCM Mapping to CSC Eco-System</p>"},{"location":"index_PREV/#how-can-i-get-involved","title":"How can I get involved?","text":"<p>This project is being developed with and for the sector. We welcome:</p> <ul> <li>Feedback and suggestions on what\u2019s useful or missing(or broken)</li> <li>Contributions of local projects or documentation</li> <li>Ideas for how the tool could better support the sector</li> </ul> <p>To contribute or get involved, please contact the Data to Insight team or fork from/visit the GitHub repo.</p>"},{"location":"index_PREV/#foundations-and-inspiration","title":"Foundations and Inspiration","text":"<p>This tool builds on the thinking behind platforms like the Children\u2019s Services Network and grounded in open modelling approaches like the already mentioned Smart City Concept Model.</p> <p>It is designed to be lightweight, transparent, and openly extensible \u2014 enabling others to adopt or adapt it for their own contexts.</p>"},{"location":"index_PREV/#soft-systems-conceptual-mapping","title":"Soft Systems Conceptual Mapping","text":"<p>System of Interest </p> <p>Shared|public data platform and ecosystem used within Children\u2019s Social Care to connect people|LA colleagues, data, tools, and services</p> <p>Purpose </p> <p>To enable shared sector understanding, validation, discovery, and collaboration between local authorities, tool development, projects and other initiatives in CSC</p> <p>Worldview (Weltanschauung) </p> <p>Fragmented data landscapes transformed into a collaborative, open ecosystem using lightweight, transparent structures like SCCM, JSON, YAML + MkDocs</p> <p>Owner(s) </p> <p>Likely data platform stewards: D2I, local authority data teams, ecosystem developers</p> <p>Environment (External Constraints) </p> <p>GitHub Pages (no backend), data security and ethics, evolving standards, distributed maintenance, changing ecosystem, frameworks and statuatory guidance, browser-only deployments</p>"},{"location":"index_PREV/#whats-next","title":"What\u2019s next?","text":"<ul> <li>Ongoing expansion of linked tools, rules, and frameworks</li> <li>Live|scheduled scrapes from key web resources or published docs/framesworks</li> <li>Search and filter interface (in beta, but aiming to implement network diagram filters)</li> <li>Option for local teams to submit structured entries or link live repositories</li> <li>Export options for integration into other data tools</li> </ul> <p>Thanks for the interest in CSC Knowledge Base Network We hope it supports your work, and welcome your feedback as we continue to improve and expand it.</p>"},{"location":"index_PREV/#possible-sccm-mapping-to-csc-eco-system","title":"Possible SCCM Mapping to CSC Eco-System","text":"SCCM Concept (Category) Suggested example(s) (in progress) Community South East fostering cluster Documentation CSC Independent Review Events Children\u2019s Social Care Review, ILACS Inspections, Public Inquiries Organization Data to Insight, LIIA Persons Organisational/sector tools linked where consent given or public record Plans Kinship Care Strategy, Children\u2019s Social Care National Framework Relationships LA-1 \u2194 Supports \u2194 SSD Tests, DfE \u2194 Pilots \u2194 API Data Flows Rules Statutory Guidance, Keeping Children Safe in Education 2025 Sector Tools PATCH, ChAT Services"},{"location":"motw-reuse-guide-local-authorities/","title":"Reusing -Map of the World- corpus locally","text":"<p>A short guide for local authority colleagues</p> <p>This note explains how you can start from the shared corpus and add your own documents without rebuilding everything. It also lists the files the notebook creates and where they live.</p>"},{"location":"motw-reuse-guide-local-authorities/#what-you-can-do","title":"What you can do","text":"<ul> <li>Start from the shared corpus and add your own PDFs, Word documents, or text files.  </li> <li>Append only what is new. Unchanged documents are skipped automatically.  </li> <li>Keep storage small. We store compact Parquet tables and a FAISS index, not the original PDFs.  </li> <li>Reuse the corpus in your own analysis tools. Parquet and FAISS are standard, portable formats.</li> </ul>"},{"location":"motw-reuse-guide-local-authorities/#what-is-in-the-shared-pack","title":"What is in the shared pack","text":"<p>Place these in your project folder:</p> <pre><code>csc_artifacts/\n  state.json                 # records which documents were processed and with which settings\n  chunks/                    # per-document Parquet of chunked text + metadata\n  vectors/                   # per-document Parquet of vectors (uint8 by default)\n  motw_index.faiss           # FAISS index (HNSW) built over all vectors\n# optional\n  motw_chunks.parquet        # combined text+meta (only if materialised)\n  motw_vectors.parquet       # combined text+vectors (only if materialised)\n</code></pre> <p>You provide your own documents here:</p> <pre><code>data_published/              # put your PDFs, .docx, or .txt here\n</code></pre> <p>We avoid storing PDFs in Git. Keep them local or in object storage if you need an archive.</p>"},{"location":"motw-reuse-guide-local-authorities/#what-the-notebook-creates","title":"What the notebook creates","text":"<p>When you run the notebook, it will create or update:</p> <ul> <li><code>csc_artifacts/state.json</code> - a small manifest with hashes, sizes, chunk counts, and build settings.  </li> <li><code>csc_artifacts/chunks/&lt;doc_id&gt;.parquet</code> - chunked text and metadata for each document.  </li> <li><code>csc_artifacts/vectors/&lt;doc_id&gt;.parquet</code> - the matching vectors for each document. Stored as <code>embedding_q</code> (uint8) with a note on how to dequantise.  </li> <li><code>csc_artifacts/motw_index.faiss</code> - the FAISS search index.  </li> <li>Optionally, <code>csc_artifacts/motw_chunks.parquet</code> and <code>csc_artifacts/motw_vectors.parquet</code> if you turn on materialisation.</li> </ul> <p>We identify each document by the first 16 hex characters of its SHA-256 hash. This keeps the build incremental and avoids duplicates.</p>"},{"location":"motw-reuse-guide-local-authorities/#two-main-ways-to-run","title":"Two main ways to run","text":""},{"location":"motw-reuse-guide-local-authorities/#1fresh-build","title":"1.Fresh build","text":"<p>Use when you are starting from scratch or want a clean rebuild.</p> <ul> <li>The notebook scans <code>data_published/</code>, extracts text, chunks it, embeds it, writes per-document Parquet, and builds a fresh FAISS index.  </li> <li>It writes a new <code>state.json</code> so future runs are incremental.</li> </ul>"},{"location":"motw-reuse-guide-local-authorities/#2append-your-new-files","title":"2.Append your new files","text":"<p>Use when you are starting from the shared pack and want to add your own documents.</p> <ul> <li>Put your documents into <code>data_published/</code>.  </li> <li>The notebook compares hashes against <code>state.json</code>. Unchanged documents are skipped.  </li> <li>New documents are processed and appended to the existing FAISS index.  </li> <li>If any existing document has changed, the notebook rebuilds the index from the per-document Parquet to keep it correct.</li> </ul>"},{"location":"motw-reuse-guide-local-authorities/#other-useful-options","title":"Other useful options","text":"<ul> <li>Rebuild from Parquet only - if you have the per-document Parquet but no FAISS file, the notebook can rebuild the index without touching PDFs.  </li> <li>Materialise a single-file Parquet - if you prefer a single file, turn on materialisation to write <code>motw_chunks.parquet</code> and <code>motw_vectors.parquet</code>. This is optional because the per-document layout already supports incremental updates.  </li> <li>Upload to object storage - if you set the Cloudflare R2 variables, the notebook can upload updated artefacts after a run.</li> </ul>"},{"location":"motw-reuse-guide-local-authorities/#notes-on-changes-and-deletions","title":"Notes on changes and deletions","text":"<ul> <li>Changed documents - if the content of a document changes, its hash changes. The notebook reprocesses that document and rebuilds the FAISS index so your search stays correct.  </li> <li>Deleted documents - remove the matching files in <code>csc_artifacts/chunks/</code> and <code>csc_artifacts/vectors/</code> and delete the entry in <code>state.json</code>, then run the notebook to rebuild the FAISS index.</li> </ul>"},{"location":"motw-reuse-guide-local-authorities/#what-success-looks-like","title":"What success looks like","text":"<p>At the end of a run you will see a summary like:</p> <pre><code>Docs: 120  Read errors: 0  Empty: 0\nChars: 14,200,000  Chunks: 95,400\n[Embedding] 00:02:15s\nWrite/Update FAISS...\nrows (meta, vectors, index): 95,400, 95,400, 95,400\nDone.\n</code></pre> <ul> <li>The counts for meta, vectors, and the FAISS index should match.  </li> <li>If you only added new files, you should see a note about appending to the index rather than rebuilding.</li> </ul>"},{"location":"motw-reuse-guide-local-authorities/#why-this-is-shareable-and-future-proof","title":"Why this is shareable and future proof","text":"<ul> <li>Standard formats - Parquet can be read by Pandas, Polars, DuckDB, Spark and more. FAISS is a common vector index.  </li> <li>Local-first - you can work entirely on your machine. No need to upload PDFs.  </li> <li>Incremental by design - starting from the shared pack means you only build what is new, then carry on where we left off.</li> </ul>"},{"location":"network/","title":"CSC Network Graph (IN DEV)","text":"<p>This interactive graph is a work in progress mapping of key organisations, plans, and events and more that form the children\u2019s services data ecosystem. It aims to provide both the simple connections and interelations between related CSC data work, projects, guidance, people and sub layer of detail and meta-data for those connected elements. </p> <p>This is a large scale mapping with many interconnected (moving)parts; currently a work-in-progress MVP. Data &amp; relations are currently being added, graph layout and naming conventions for nodes in particular, is an ongoing discussion as we progress possible use-cases and standardise yml object structure.</p> Search: Keep neighbours (context) Filter by node type(s): Organizations Plans Events Services Reset View Quick tips for filtering <p>Free text matches the node\u2019s name, tags, and summary.</p> <ul> <li><code>tag:&lt;word&gt;</code> \u2014 match nodes with that tag (e.g. <code>tag:ilacs</code>)</li> <li><code>type:&lt;kind&gt;</code> \u2014 restrict by type (<code>type:org</code>, <code>type:plan</code>, <code>type:event</code>, <code>type:service</code>)</li> <li>Combine terms: <code>tag:ilacs type:org</code> (all terms must match)</li> <li>Context mode: keeps neighbours of matches visible for exploration</li> <li>Filters + search intersect (both must match)</li> <li>Share state: copy the URL (types and query persist in the hash)</li> </ul> <p>Examples</p> <ul> <li><code>ilacs</code> \u2014 any node mentioning \u201cilacs\u201d</li> <li><code>tag:children_services</code> \u2014 nodes tagged \u201cchildren_services\u201d</li> <li><code>type:org dfe</code> \u2014 organisation nodes mentioning \u201cdfe\u201d</li> <li><code>tag:data_tools type:service</code> \u2014 services tagged \u201cdata_tools\u201d</li> </ul> <p></p>"},{"location":"network_fullscreen/","title":"Network, full screen","text":""},{"location":"sccm_relation_types/","title":"CSC Network Relationships","text":"<p>This is a reference page detailing the scope of relationship types defined within the Smart City Concept Model (SCCM) as applied to this tools Children's Social Care CSC Network Diagram. These relationships are reproduced directly from the Smart City Concept Model definitions, but shown here for each relevant object type towards additional clarity on the derived network diagram (generated from the core YAML definitions within this tool).</p> Service type relationships SubjectRelationshipObject ServicecontainsService ServiceinfluencedByObjective ServiceprovidedByAgent ServiceresponsibilityOfAgent ServiceserviceImplementsMethodMethod ServiceusedByCommunity ServicesubjectOfAgreement ServicecontainedInService ServicecontainedInFunction ServiceraisesCase ServiceusesResourceResource ServicehasRuleRule Event type relationships SubjectRelationshipObject EventatPlacePlace EventhasOutcomeState EventcontainedInCase EventcontainedInAccount EventhasRoleFromItem EventhasOutcomeDecision EventeventPlannedInPlan Plan type relationships SubjectRelationshipObject PlancontainsPlan PlanhasTargetTarget PlaninfluencedByObjective PlanplanDerivedFromMethodMethod PlanplanForEventEvent PlanplanForCaseCase PlancontainedInPlan PlanplanOfAgent PlanusesResourceResource Community type relationships SubjectRelationshipObject CommunitycontainsCommunity CommunitycontainedInCommunity CommunityusesService Organization type relationships SubjectRelationshipObject OrganizationcontainsOrganization OrganizationcontainedInOrganization OrganizationhasMemberPerson Person type relationships SubjectRelationshipObject PersonmemberOfOrganization Rule type relationships SubjectRelationshipObject RuleruleForService Resource type relationships SubjectRelationshipObject ResourceresourceForService ResourceresourceForPlan ResourceresourceOfAgent Collection type relationships SubjectRelationshipObject CollectioncollectionContainsItem CollectioncollectionDefinedByAgent Agent type relationships SubjectRelationshipObject AgenthasObject AgenthasAbstract AgenthasAgreementAgreement AgenthasObjectiveObjective AgenthasPlanPlan AgenthasResourceResource AgenttakesDecisionDecision AgentusesItem AgentmakesAssumptionAssumption AgentdefinesCollectionCollection AgentownsAccount AgentprovidesService AgentresponsibleForService"},{"location":"search/","title":"CSC Search","text":"Search the CSC Network Knowledge Base <p> Reference detail of the search index scope and the search strategy applied here.</p> <p>JavaScript is required to use search function.</p>"},{"location":"search_pipeline/","title":"CSC Search Strategy","text":""},{"location":"search_pipeline/#full-text-search-strategy-optimised-for-static-hosting","title":"Full-Text Search Strategy (Optimised for Static Hosting)","text":"<p>To support search across the contents of hundreds of documents and varied sources (including PDFs) within our MkDocs-based site, we needed an approach that simulates full-text search while remaining scalable, performant, and static-host compatible (i.e. no backend/db).</p>"},{"location":"search_pipeline/#main-goals","title":"Main Goals","text":"<ul> <li>Allow users to search document content (not just titles)</li> <li>Ensure performance remains fast even as hundreds of documents are added</li> <li>Avoid full-text duplication or payload bloat in both the frontend and backend(to avoid hitting Git limits)</li> <li>Keep compatibility with static site hosting (e.g. GitHub Pages, <code>list.js</code>, <code>lunr.js</code>, Mkdocs)</li> </ul>"},{"location":"search_pipeline/#optimisation-strategy","title":"Optimisation Strategy","text":"<p>Rather than store the entire text of every PDF in the search index (which would be slow and bloated), we take the following approach:</p> <ol> <li>Extract Text from PDFs </li> <li>Using <code>pdfplumber</code>, each document\u2019s text is extracted and cleaned</li> <li> <p>Unicode characters like curly quotes, dashes, ellipses are normalised for consistency</p> </li> <li> <p>Generate Excerpts </p> </li> <li>A short <code>excerpt</code> is created from the first meaningful paragraph (ignoring TOCs and headings)</li> <li> <p>This is used in search results and graph visualisation tooltips</p> </li> <li> <p>Lemmatise and Tokenise Keywords </p> </li> <li>Words are lemmatised (e.g. running, ran, runs \u2192 run) using <code>nltk</code></li> <li>Common English stopwords are removed</li> <li> <p>This creates a compressed keyword representation of each document</p> </li> <li> <p>Apply Document Frequency Filtering </p> </li> <li>Using <code>CountVectorizer</code> from <code>scikit-learn</code>, we:<ul> <li>Remove overly common terms (appear in &gt;85% of docs)</li> <li>Remove rare noise terms (appear in &lt;2 docs)</li> </ul> </li> <li> <p>This results in signal-rich keywords per document</p> </li> <li> <p>Final Search Index Generation </p> </li> <li>Each optimised document (content) is represented in <code>docs/search_index.json</code> with:<ul> <li><code>doc_id</code>, <code>name</code>, <code>excerpt</code>, <code>url</code>, <code>tags</code>, and optimised <code>keywords</code></li> </ul> </li> <li>At runtime, search results display:<ul> <li>Matched titles</li> <li>Decoded excerpts</li> <li>A prioritised and randomly sampled subset of keywords (max 20) to avoid repetition or alphabetical bias</li> </ul> </li> </ol>"},{"location":"search_pipeline/#optional-archive-parquet","title":"Optional Archive (Parquet)","text":"<p>For potential archival or later ML processing is needed, a full <code>.parquet</code> file can also be saved  (<code>admin_scripts/docs_index.parquet</code>) with complete text and metadata (disabled by default to keep project lightweight, but a variable flag can bve set in the py index build script).</p>"},{"location":"search_pipeline/#key-libraries-used","title":"Key Libraries Used","text":"Purpose Tool PDF text extraction <code>pdfplumber</code> Text cleanup <code>re</code>, <code>unicodedata</code>, <code>DOMParser</code> (JS) Lemmatisation &amp; stopwords <code>nltk</code> Frequency filtering <code>scikit-learn</code> (<code>CountVectorizer</code>) Output formats <code>json</code>, <code>pandas</code>, <code>parquet</code> <p>This setup we think ensures search is (acceptably)fast, useful, and scalable \u2014 and that the project can grow without sacrificing performance or frontend simplicity. We're in the process of scaling this up for more complete/realistic testing alongside a cyclic review approach. </p>"},{"location":"sources/","title":"Data Sources","text":"<p>_To add transparency to the search in particular, below is the current scope of the input data/sources. Some of the labelling comes direct/dynamically off the sources themselves and may therefore be inconsistent. _</p>"},{"location":"sources/#sccm-aligned-yaml-metadata-2033-sources","title":"SCCM-aligned YAML Metadata (~2033 sources)","text":""},{"location":"sources/#events","title":"events","text":"Source File Type Word Count Last Refreshed childrens_social_care_in_england_2025 .yaml - 18/09/2025 childrens_social_care_review .yaml - 18/09/2025 childrens_wellbeing_schools_bill_2024_25 .yaml - 18/09/2025 nvest1 .yaml - 18/09/2025 nvest2 .yaml - 18/09/2025 nvest3 .yaml - 18/09/2025"},{"location":"sources/#organizations","title":"organizations","text":"Source File Type Word Count Last Refreshed adcs .yaml - 21/10/2025 coram .yaml - 21/10/2025 csdug .yaml - 21/10/2025 data_to_insight .yaml - 21/10/2025 department_for_education .yaml - 21/10/2025 essex_county_council .yaml - 21/10/2025 hertfordshire_county_council .yaml - 21/10/2025 knowsley_council .yaml - 21/10/2025 lancaster_university .yaml - 21/10/2025 lga .yaml - 21/10/2025 research_in_practice .yaml - 21/10/2025 social_finance .yaml - 02/12/2025 test_0001 .yaml - 21/10/2025 test_0002 .yaml - 21/10/2025 test_0003 .yaml - 21/10/2025 test_0004 .yaml - 21/10/2025 test_0005 .yaml - 21/10/2025 snipped scale-up - testing test_1992 .yaml - 21/10/2025 test_1993 .yaml - 21/10/2025 test_1994 .yaml - 21/10/2025 test_1995 .yaml - 21/10/2025 test_1996 .yaml - 21/10/2025 test_1997 .yaml - 21/10/2025 test_1998 .yaml - 21/10/2025 test_1999 .yaml - 21/10/2025 test_2000 .yaml - 21/10/2025"},{"location":"sources/#plans","title":"plans","text":"Source File Type Word Count Last Refreshed childrens_social_care_national_framework .yaml - 18/09/2025 ddsf .yaml - 18/09/2025 ffp_programme_guide_2025 .yaml - 18/09/2025 national_kinship_care_strategy .yaml - 18/09/2025 nvest .yaml - 18/09/2025"},{"location":"sources/#resources","title":"resources","text":"Source File Type Word Count Last Refreshed ddsf1a_standard_safeguarding_dataset .yaml - 18/09/2025 ddsf2a_social_workers_and_cms_constraints .yaml - 18/09/2025 standard_safeguarding_dataset .yaml - 18/09/2025"},{"location":"sources/#rules","title":"rules","text":"Source File Type Word Count Last Refreshed childrens_act_1989 .yaml - 18/09/2025 cin_census_data_requirements .yaml - 18/09/2025 keeping_children_safe_in_education_2025 .yaml - 18/09/2025"},{"location":"sources/#services","title":"services","text":"Source File Type Word Count Last Refreshed centre_of_excellence .yaml - 18/09/2025 d2i_apprenticeships .yaml - 18/09/2025 d2i_data_validators .yaml - 18/09/2025 d2i_excel_toolkit_maintenance .yaml - 18/09/2025"},{"location":"sources/#public-web-data-6-sources","title":"Public Web Data (~6 sources)","text":""},{"location":"sources/#data_web","title":"data_web","text":"Source File Type Word Count Last Refreshed adcsorguk-adcs-safeguarding-page .txt 843 18/09/2025 barnetgovuk-barnet-resource .txt 310 18/09/2025 bathnesgovuk-bath-and-north-east-somerset-resource .txt 720 18/09/2025 childrensservicesnetwork-childrens-services-network .txt 28 18/09/2025 govuk-department-for-education .txt 1510 18/09/2025 lbbdgovuk-barking-and-dagenham-resource .txt 317 18/09/2025"}]}