{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CSC Knowledge Base Network","text":"<p>A structured, extensible open-source data-eco-system map and 'knowledge base' for the Children\u2019s Social Care (CSC) sector. This project collates and creates an overview of key sector documentation, project relationships, data services, sector tools, rules, plans and events using a flexible YAML-based data model aligned with the Smart City Concept Model (SCCM) towards data interoperability.</p> <p>Alongside the (filtered)graph-based relations visualisation, it aims to support key-term search and YAML schema validation across the structured/human readable <code>.yml</code> structured records. Development is scaffolded/designed to be extensible, transparent, and Git-native.</p> <p>With a focus on ensuring an entirely open-source and Git-native tool, there are some interesting and potentially constraining problems to solve just within the available tech-stack(even before we address the arguably bigger issues of how to bring all the relvant data together, and what's a sustainable internal structure). We're particularly interested in the Git data limits vs CSC sector need on data storage towards such as optimised full text searches. The potential data-estate in this exploratory development, alongside how we might take or manage direct sector input into the YML structure(s) also ongoing factors. </p>"},{"location":"#current-dev-phase-discovery-alpha","title":"Current Dev Phase: Discovery-Alpha","text":""},{"location":"#what-is-this-for","title":"What is this for?","text":"<p>This proof of concept(PoC) project explores how a structured, searchable map of Children\u2019s Social Care (CSC) data, tools, data projects, key frameworks, statuatoary guidance and activity could enable collaboration and support or optimise efforts across the sector. While exact use-cases are still emerging, our goal is to create a shared resource that brings together:</p> <ul> <li>published reports  </li> <li>pre-defined data objects  </li> <li>web data (e.g. from the DfE, local authorities, third parties)  </li> <li>sector-developed tools and frameworks</li> <li>connected people (organisational/sector tools linked where consent given or public record)  </li> </ul> <p>All of this would be made accessible through a visual or navigable interface, allowing users to explore connections between people, projects, standards, and services.</p> <p>We think this could help:</p> <ul> <li>Make relationships clearer \u2014 between local and national CSC initiatives, policies, systems, and data sources  </li> <li>Show who\u2019s doing what \u2014 helping users track new tools, updates to frameworks, or structural changes in services  </li> <li>Bring siloed or under-the-radar work into view \u2014 so efforts can align, build on each other, or avoid duplication  </li> <li>Support local teams \u2014 by contributing to a more joined-up picture of activity across the sector  </li> </ul> <p>We envisage use-cases from:</p> <ul> <li>Local authority data and performance teams</li> <li>Children\u2019s social care service managers and strategic leads</li> <li>Academic researchers and national analysts</li> <li>Project leads, developers and architects working in CSC data or digital delivery</li> </ul> <p>We see this as a collaborative mapping tool, developed potentially with input from local authority teams, analysts, service leads, academic partners, and national bodies.</p>"},{"location":"#plan","title":"Plan","text":"<ul> <li>Interactive network map: Navigate to Network to view entities, relationships, and systems as a live graph</li> <li>Structured data records: Underpinning the map is a growing library of structured YAML records, aligned to a SCCM concept framework(BSI as PAS 182) that describe:</li> <li>Tools and systems (e.g. PATCH, Validator 903)</li> <li>Frameworks and inspections (e.g. Ofsted ILACS, JTAI)</li> <li>Relationships and service models</li> <li>Rules, plans, events and guidance</li> <li>Searchable resource: The search page enables you to explore the structured data model directly.</li> <li>This is separate from the standard MkDocs search (top-right), which only covers page text within this site.</li> <li>The CSC knowledge search indexes structured YAML content as well as <code>.md</code>, <code>.pdf</code>, <code>.py</code>, <code>.js</code> and <code>.html</code> files, and supports keyword relevance, match scoring, and metadata extraction.</li> <li>(in dev)The search index|scope currently takes a data sample direct from local authority web sites. At the moment this is throttled to ~10, but with the potential to extract simplistic reference resource(s) directly from all ~153</li> <li>(in dev)The search index|scope aims to scrape from relevant CSC public data sources in order to increase the tool's search scope. This could schedule indexing of relevant documents or data sources from defined .gov or .edu sites.   </li> <li>Documentation hub: Local documentation from D2I projects(Git repos) is also live-indexed to provide technical context</li> </ul>"},{"location":"#how-is-this-structured","title":"How is this structured?","text":"<p>Records in this tool are aligned with the Smart City Concept Model (SCCM), an open framework for describing public service ecosystems. Every entity towards the documented network(diagram) is represented as a YAML file, defined at the top level via SCCM concept types :</p> <p>e.g. - <code>@type: AGENT</code> \u2013 people, teams, or organisations - <code>@type: SERVICE</code> \u2013 a system, service or tool - <code>@type: EVENT</code> \u2013 events such as inspections, launches, reviews - <code>@type: RULE</code>, <code>@type: PLAN</code>, <code>@type: COLLECTION</code> \u2013 policy elements, datasets or strategies - <code>@type: RELATIONSHIP</code> \u2013 links between entities (e.g. oversight, supply, influence)</p> <p>YAML files are validated, searchable, and designed to be easier to contribute to as they're more human readable than other structured data forms (e.g. JSON, CSV... )</p> <p>Note: Further SCCM allignment examples under Possible SCCM Mapping to CSC Eco-System</p>"},{"location":"#how-can-i-get-involved","title":"How can I get involved?","text":"<p>This project is being developed with and for the sector. We welcome:</p> <ul> <li>Feedback and suggestions on what\u2019s useful or missing(or broken)</li> <li>Contributions of local projects or documentation</li> <li>Ideas for how the tool could better support the sector</li> </ul> <p>To contribute or get involved, please contact the Data to Insight team or fork from/visit the GitHub repo.</p>"},{"location":"#foundations-and-inspiration","title":"Foundations and Inspiration","text":"<p>This tool builds on the thinking behind platforms like the Children\u2019s Services Network and grounded in open modelling approaches like the already mentioned Smart City Concept Model.</p> <p>It is designed to be lightweight, transparent, and openly extensible \u2014 enabling others to adopt or adapt it for their own contexts.</p>"},{"location":"#soft-systems-conceptual-mapping","title":"Soft Systems Conceptual Mapping","text":"<p>System of Interest </p> <p>Shared|public data platform and ecosystem used within Children\u2019s Social Care to connect people|LA colleagues, data, tools, and services</p> <p>Purpose </p> <p>To enable shared sector understanding, validation, discovery, and collaboration between local authorities, tool development, projects and other initiatives in CSC</p> <p>Worldview (Weltanschauung) </p> <p>Fragmented data landscapes transformed into a collaborative, open ecosystem using lightweight, transparent structures like SCCM, JSON, YAML + MkDocs</p> <p>Owner(s) </p> <p>Likely data platform stewards: D2I, local authority data teams, ecosystem developers</p> <p>Environment (External Constraints) </p> <p>GitHub Pages (no backend), data security and ethics, evolving standards, distributed maintenance, changing ecosystem, frameworks and statuatory guidance, browser-only deployments</p>"},{"location":"#whats-next","title":"What\u2019s next?","text":"<ul> <li>Ongoing expansion of linked tools, rules, and frameworks</li> <li>Live|scheduled scrapes from key web resources or published docs/framesworks</li> <li>Search and filter interface (in beta, but aiming to implement network diagram filters)</li> <li>Option for local teams to submit structured entries or link live repositories</li> <li>Export options for integration into other data tools</li> </ul> <p>Thanks for the interest in CSC Knowledge Base Network We hope it supports your work, and welcome your feedback as we continue to improve and expand it.</p>"},{"location":"#possible-sccm-mapping-to-csc-eco-system","title":"Possible SCCM Mapping to CSC Eco-System","text":"SCCM Concept (Category) Suggested example(s) (in progress) Community South East fostering cluster Documentation CSC Independent Review Events Children\u2019s Social Care Review, ILACS Inspections, Public Inquiries Organization Data to Insight, LIIA Persons Organisational/sector tools linked where consent given or public record Plans Kinship Care Strategy, Children\u2019s Social Care National Framework Relationships LA-1 \u2194 Supports \u2194 SSD Tests, DfE \u2194 Pilots \u2194 API Data Flows Rules Statutory Guidance, Keeping Children Safe in Education 2025 Sector Tools PATCH, ChAT Services"},{"location":"data_source_optimisation/","title":"Optimising Search for \u201cMap of the World\u201d","text":"<p>From PDFs to compact, fast, portable indexes</p>"},{"location":"data_source_optimisation/#why-this-matters-for-childrens-social-care-data","title":"Why this matters for children\u2019s social care data","text":"<p>Children\u2019s social care information is extensive and often spread very wide across government and partner websites, (long)PDFs, and research portals. Our approach (in-part)focuses on making that material accessible and searchable without moving heavy files around or duplicating storage.</p> <ul> <li>Fast, lightweight search: we extract text once and publish a very small index, so people can jump straight to relevant paragraphs instead of paging through 120-page PDFs.</li> <li>Clarity and provenance: chunk-level search surfaces the exact clauses (for example thresholds, Section 47, kinship care) with links back to the authoritative source and licence.</li> <li>Low overhead, high resilience: compact Parquet/FAISS artefacts are inexpensive to store (for example in R2), simple to mirror, and easy to reuse locally by partners.</li> <li>Keeps pace with change: incremental builds detect updates and only reprocess what changed, so the map stays current as guidance evolves.</li> <li>Privacy by design: data minimisation (no PDFs in the site), licensing metadata, and explicit source tracking align with safeguarding and governance expectations.</li> </ul>"},{"location":"data_source_optimisation/#who-this-is-for-and-what-youll-get","title":"Who this is for (and what you\u2019ll get)","text":"<ul> <li>Product / content folks: how we take lots of PDFs and turn them into a tiny, fast search index that still finds the right things.</li> <li>Engineers / data people: the design choices, math, and scaling numbers so you can reuse this pipeline on your own document sets.</li> </ul> <p>One\u2011line summary: we extract text -&gt; split into semantically sensible chunks -&gt; embed each chunk as a vector -&gt; store compactly in Parquet using uint8 quantisation -&gt; build a FAISS similarity index (HNSW by default). The public search can then use a small JSON keyword index for near-instant client\u2011side search; deeper semantic search uses the vector index as needed.</p>"},{"location":"data_source_optimisation/#why-this-approach-works","title":"Why this approach works","text":"<ol> <li>We do heavy lifting once, locally. PDFs are slow to parse and (comparitively)large to store especially at scale; we extract the text and never ship the raw files within this site.</li> <li>We chunk long documents. Searching against topically coherent chunks beats whole\u2011document matching for retrieval quality.</li> <li>We use sentence embeddings. Similarity in embedding space approximates \u201cis this about the same thing?\u201d</li> <li>We store vectors compactly. Quantising embeddings to uint8 shrinks storage ~4\u00d7 with negligible impact on retrieval quality for our use case.</li> <li>We separate UX search from deep search. The site ships a tiny keyword index for instant results; power workflows can use the vector index offline or via services.</li> </ol>"},{"location":"data_source_optimisation/#what-tiny-recall-loss-means-plain-english-first","title":"What \u201ctiny recall loss\u201d means (plain English first)","text":"<p>When we save embeddings as uint8 instead of 32\u2011bit floats, we\u2019re using a lossy compression scheme. That slightly perturbs the numbers. If someone later rebuilds a vector index from those saved numbers, the nearest\u2011neighbour results may differ a little from a float32 baseline.</p> <p>In practice with normalised sentence embeddings:</p> <ul> <li>Cosine similarity between original and dequantised vectors is usually \u2265 0.995 (often ~0.999).</li> <li>End\u2011to\u2011end recall@10 typically changes by 0-2% on common text corpora.</li> <li>Our live FAISS index is built directly from float32 in memory, so website retrieval quality is unaffected by how we store vectors in Parquet.</li> </ul>"},{"location":"data_source_optimisation/#the-quick-math","title":"The quick math","text":"<p>We L2\u2011normalise each embedding vector <code>x</code> (so values lie roughly in [-1, 1]) and map each component to 8 bits:</p> <ul> <li>Quantise: <code>q = round((x + 1) * 127.5)</code> -&gt; <code>q \u2208 {0,\u2026,255}</code> </li> <li>Dequantise: <code>x' = (q / 127.5) - 1.0</code> </li> <li>Re\u2011normalise: <code>x'' = x' / ||x'||</code> (keeps cosine similarity meaningful)</li> </ul> <p>Because most information in sentence embeddings is in direction (not exact magnitudes), cosine similarity is very stable after this process.</p>"},{"location":"data_source_optimisation/#the-pipeline-what-we-do-and-why","title":"The pipeline (what we do and why)","text":"<ol> <li>Text extraction (PyMuPDF fast path; pdfminer fallback)  </li> <li>Why: PyMuPDF is 2-5\u00d7 faster on many PDFs; we skip OCR for now.</li> <li>Chunking (default ~1,800 chars with ~150 overlap)  </li> <li>Why: balances context and index size; fewer, richer chunks -&gt; faster builds and smaller artefacts.</li> <li>Embedding (<code>all-MiniLM-L6-v2</code>, 384\u2011dim, normalised)  </li> <li>Why: Small, fast, good quality; normalisation makes cosine = inner product and improves quantisation robustness.</li> <li>Storage </li> <li>Per\u2011doc Parquet (text + metadata) -&gt; easy incremental updates.  </li> <li>Vectors in Parquet as <code>uint8</code> -&gt; ~4\u00d7 smaller than float32, plus schema metadata describing de/quantisation.  </li> <li>Combined Parquet (optional) for simple downstream consumption.</li> <li>Indexing (FAISS)  </li> <li>HNSW default: excellent recall, simple to tune.  </li> <li>IVF\u2011PQ optional: order\u2011of\u2011magnitude smaller index for very large corpora; small recall trade\u2011off, tunable with <code>nprobe</code>.</li> <li>Incremental builds </li> <li><code>state.json</code> records per\u2011document SHA\u2011256; unchanged docs are skipped. Rebuilds are proportional to change, not corpus size.</li> <li>Website search index (keyword JSON) </li> <li>From Parquet, not PDFs. We clean text, remove stop\u2011words and over\u2011common terms, derive per\u2011doc keywords -&gt; a very small JSON file for client\u2011side search.</li> </ol>"},{"location":"data_source_optimisation/#realworld-impact-for-users","title":"Real\u2011world impact for users","text":"<ul> <li>Fast search: small on\u2011site JSON makes type\u2011ahead and filters instant.  </li> <li>Better hits: chunk\u2011level indexing surfaces relevant passages from long PDFs.  </li> <li>Stable URLs: search results refer back to authoritative documents and locations.  </li> <li>Scales with you: adding thousands of documents won\u2019t slow the site or bloat the repo.</li> </ul>"},{"location":"data_source_optimisation/#realworld-impact-for-engineers","title":"Real\u2011world impact for engineers","text":"<ul> <li>Artefacts, not assets: Git &amp; Pages store compact Parquet/JSON, not PDFs.  </li> <li>Cheap storage: vectors as <code>uint8</code> and HNSW/IVF\u2011PQ keep R2 costs low.  </li> <li>Reproducible builds: manifest + hashes = deterministic rebuilds.  </li> <li>Interoperability: Parquet + FAISS are standard; others can reuse the corpus locally.</li> </ul>"},{"location":"data_source_optimisation/#sizing-scaling-rules-of-thumb","title":"Sizing &amp; scaling (rules of thumb)","text":"<p>Let C = number of chunks. With 384\u2011dim embeddings:</p> <ul> <li>Float32 vector size (raw): <code>C \u00d7 384 \u00d7 4</code> bytes  </li> <li>UInt8 vector size (raw): <code>C \u00d7 384 \u00d7 1</code> bytes (~4\u00d7 smaller)  </li> <li>HNSW index size: typically ~1-2 KB per chunk (depends on <code>M</code>, dataset)  </li> <li>Parquet overhead: +10-30% over raw, but ZSTD helps especially for text.</li> </ul>"},{"location":"data_source_optimisation/#example-projections","title":"Example projections","text":"<p>Assuming current chunking yields ~74 chunks/doc (observed ~1,928 chunks for 26 docs) -&gt; 3,000 docs ~ 222k chunks.</p> Component Formula 222k chunks (est.) Parquet vectors (float32 raw) <code>C \u00d7 384 \u00d7 4</code> ~341 MB (+overhead) Parquet vectors (uint8 raw) <code>C \u00d7 384 \u00d7 1</code> ~85 MB (+overhead) HNSW index ~1-2 KB \u00d7 C ~220-440 MB Parquet text+meta compressed tens of MB (depends on text volume) <p>If the HNSW index starts feeling large to juggle, flip to IVF\u2011PQ: you can expect tens of MB with recall@10 often 95-99%, tunable via <code>nprobe</code>.</p>"},{"location":"data_source_optimisation/#keyword-index-whats-in-the-tiny-json","title":"Keyword index (what\u2019s in the tiny JSON)","text":"<p>We derive a compact per\u2011document keyword list from the Parquet text:</p> <ul> <li>Normalise punctuation/quotes and remove boilerplate (e.g., page headers).  </li> <li>Lowercase, keep words of 4+ letters.  </li> <li>Remove stop\u2011words (scikit\u2011learn English) and domain\u2011common words (CSC custom list of high freq/low importance words).  </li> <li>Filter too\u2011common terms with <code>max_df</code> (e.g., drop tokens appearing in &gt;85% of docs) and too\u2011rare with <code>min_df</code> (e.g., keep tokens in \u22652 docs).</li> </ul> <p>This makes <code>docs/search_index.json</code> tiny and fast(er) for lookup/searches.</p>"},{"location":"data_source_optimisation/#why-hnsw-now-ivfpq-later","title":"Why HNSW now, IVF\u2011PQ later?","text":"<ul> <li>HNSW gives near\u2011exact recall with simple knobs (<code>M</code>, <code>efConstruction</code>, <code>efSearch</code>), great for up to a few hundred thousand vectors on a single machine.</li> <li>IVF\u2011PQ compresses vectors and partitions the space. It is useful when you need smaller indexes and faster queries at scale. We trade some/a little recall, but can dial it back with <code>nprobe</code> and codebook size.</li> </ul> <p>Switching rule of thumb: if chunks exceed ~200-250k or HNSW file becomes problematic (&gt;~300 MB), toggle IVF\u2011PQ.</p>"},{"location":"data_source_optimisation/#interoperability-reuse","title":"Interoperability &amp; reuse","text":"<ul> <li>Parquet is easy to read from Python, R, Spark, DuckDB, Polars, etc.</li> <li>FAISS is a standard ANN engine; the index can be loaded locally or by a lightweight service.</li> <li>The corpus is self\u2011describing: Parquet schema metadata indicates whether embeddings are <code>float32</code> or quantised (<code>uint8</code> with dequant rule).</li> </ul>"},{"location":"data_source_optimisation/#frequently-asked-but-will-it","title":"Frequently asked \u201cbut will it\u2026?\u201d","text":"<ul> <li> <p>\u2026find the right stuff if we've compressed vectors?   Yes-for docs and model, differences are tiny. We've normalised vectors, which preserves cosine structure.</p> </li> <li> <p>\u2026scale to thousands of PDFs?   Yes. We already decouple build time from corpus size via incremental hashing, and so work with only compact artefacts.</p> </li> <li> <p>\u2026lock me into a specific, or needed paid software?   No. We use open formats (Parquet, FAISS) + standard Python libs.</p> </li> <li> <p>\u2026support OCR/scanned PDFs?   Not yet; we can add Tesseract or cloud OCR later if needed but we've not come accross any relevant pdfs that weren't text content based. We'll fix this later if any colleagues are aware of such docs. </p> </li> </ul>"},{"location":"data_source_optimisation/#key-libraries-and-why-we-chose-them","title":"Key libraries and why we chose them","text":"<ul> <li>PyMuPDF (<code>pymupdf</code>): fast, robust PDF text extraction.  </li> <li>Sentence\u2011Transformers: high\u2011quality sentence embeddings + easy APIs.  </li> <li>FAISS: full vector search (HNSW, IVF\u2011PQ) with scalable performance.  </li> <li>PyArrow/Parquet: portable column based storage; perfect for text + vectors + metadata.  </li> <li>scikit\u2011learn: light\u2011weight keyword vocab building with stop\u2011word &amp; frequency filters.</li> </ul>"},{"location":"data_source_optimisation/#takeaways","title":"Takeaways","text":"<ul> <li>By separating heavy preprocessing from what's hosted in the main/mkdocs site here, plus by quantising vectors, we massively cut the needed storage and bandwidth without compromising colleagues' experience in the search etc</li> <li>The site\u2019s search is able to be near-instant with smaller footprint, and an underlying corpus remains portable and reusable for future or onward data projects.</li> </ul>"},{"location":"data_source_optimisation/#faiss-and-parquet-artefacts-structure-and-reference","title":"FAISS and Parquet artefacts: structure and reference","text":"<p>A bit about how derived files are organised within the project, what's inside Parquet tables, and what the FAISS index stores. </p>"},{"location":"data_source_optimisation/#repo-layout-derived-artefacts","title":"Repo layout (derived artefacts)","text":"<pre><code>artifacts/\n  state.json                        # per-document hashes, build config, last_built\n  chunks/                           # per-document Parquet (text + metadata)\n    1a2b3c4d5e6f7a8b.parquet\n    9c0d1e2f3a4b5c6d.parquet\n    ...\n  vectors/                          # per-document Parquet (text + vectors)\n    1a2b3c4d5e6f7a8b.parquet\n    9c0d1e2f3a4b5c6d.parquet\n    ...\n  motw_chunks.parquet               # combined (optional)\n  motw_vectors.parquet              # combined (optional)\n  motw_index.faiss                  # FAISS HNSW index (default)\n  motw_index_ivfpq.faiss            # FAISS IVF-PQ index (optional)\n</code></pre> <ul> <li><code>doc_id</code> is the first 16 hex of the SHA-256 of the raw file content.</li> <li>Per-document Parquet files enable incremental builds and quick reloads.</li> <li>Combined Parquet files are optional and exist for convenience.</li> </ul>"},{"location":"data_source_optimisation/#parquet-schemas","title":"Parquet schemas","text":""},{"location":"data_source_optimisation/#per-document-chunks-parquet-artifactschunksdoc_idparquet","title":"Per-document chunks Parquet (<code>artifacts/chunks/&lt;doc_id&gt;.parquet</code>)","text":"<p>Schema:</p> <pre><code>doc_id:      string          # 16-char hex id (content address)\nchunk_id:    int32           # 0..N-1 per document\nsource_path: string          # original path on disk\nsource_name: string          # original filename (for example My_File.pdf)\ntext:        string          # chunked, cleaned text\n</code></pre> <p>Example row:</p> doc_id chunk_id source_name text (truncated) 1a2b3c4d5e6f7a8b 0 ADCS_Safeguarding_...pdf Local authorities should..."},{"location":"data_source_optimisation/#per-document-vectors-parquet-artifactsvectorsdoc_idparquet","title":"Per-document vectors Parquet (<code>artifacts/vectors/&lt;doc_id&gt;.parquet</code>)","text":"<p>Same columns as chunks, plus one embedding column:</p> <ul> <li>If stored as float32:   <code>embedding: list&lt;float32&gt;    # length 384</code></li> <li>If stored as uint8 (space-saving default):   <code>embedding_q: list&lt;uint8&gt;    # length 384, quantised</code></li> </ul> <p>Parquet schema metadata (present on the vectors file) indicates storage:</p> <pre><code>motw.embedding.storage = \"float32\"      # or \"uint8_sym\"\nmotw.embedding.dequant  = \"x = (q / 127.5) - 1.0\"\n</code></pre> <ul> <li>For <code>embedding_q</code>, clients can reconstruct an approximate float vector as:</li> <li><code>x = (q / 127.5) - 1.0</code>, then L2-normalise <code>x</code> before cosine similarity.</li> <li>Vectors were normalised during embedding, which keeps cosine behaviour stable after dequantisation.</li> </ul>"},{"location":"data_source_optimisation/#combined-parquet-files-optional","title":"Combined Parquet files (optional)","text":"<p><code>artifacts/motw_chunks.parquet</code> and <code>artifacts/motw_vectors.parquet</code> have the same schemas as above, but hold all docs together. They are convenient for one-shot analysis and for users/colleagues that do not want to iterate per-doc files.</p>"},{"location":"data_source_optimisation/#faiss-index-files","title":"FAISS index files","text":"<p>The pipeline can build either:</p>"},{"location":"data_source_optimisation/#hnsw-index-default-artifactsmotw_indexfaiss","title":"HNSW index (default) - <code>artifacts/motw_index.faiss</code>","text":"<p>Stored properties inside the binary:</p> <ul> <li><code>d</code> -&gt; 384 (embedding dimension)</li> <li><code>metric</code> -&gt; inner product (cosine when vectors are normalised)</li> <li><code>ntotal</code> -&gt; number of vectors (equals total chunks)</li> <li>Graph parameters saved with the index:</li> <li><code>M</code> -&gt; graph degree used at build time (for example 32)</li> <li><code>efConstruction</code> -&gt; build breadth (for example 80)</li> <li>Query-time parameter you set after loading:</li> <li><code>efSearch</code> -&gt; search breadth (for example 64 or 128). Higher -&gt; better recall, slower.</li> </ul> <p>Notes: - Excellent recall and speed up to a few hundred thousand vectors on a single machine. - File size typically ~1-2 KB per vector, depending on <code>M</code> and data.</p>"},{"location":"data_source_optimisation/#ivf-pq-index-optional-artifactsmotw_index_ivfpqfaiss","title":"IVF-PQ index (optional) - <code>artifacts/motw_index_ivfpq.faiss</code>","text":"<p>Stored properties:</p> <ul> <li><code>d</code> -&gt; 384</li> <li><code>metric</code> -&gt; inner product</li> <li><code>ntotal</code> -&gt; number of vectors</li> <li>Coarse quantiser and codebooks:</li> <li><code>nlist</code> -&gt; number of coarse centroids (for example 1024)</li> <li><code>m</code> -&gt; number of subquantisers (for example 16)</li> <li><code>nbits</code> -&gt; bits per subvector (for example 8)</li> <li>Trained centroids and PQ codebooks included in the file</li> <li>Query-time parameter (set after loading):</li> <li><code>nprobe</code> -&gt; how many coarse lists to search (for example 16, 32, 64). Higher -&gt; better recall, slower.</li> </ul> <p>Notes: - Much smaller on disk than HNSW (often tens of MB), with a small recall trade-off that you can tune via <code>nprobe</code>.</p>"},{"location":"data_source_optimisation/#minimal-load-examples-for-ref","title":"Minimal load examples (for ref)","text":"<pre><code># Load FAISS index\nimport faiss\nindex = faiss.read_index(\"artifacts/motw_index.faiss\")  # or motw_index_ivfpq.faiss\n# For HNSW, can increase recall at query time:\ntry:\n    index.hnsw.efSearch = 64\nexcept AttributeError:\n    pass\n# For IVF-PQ:\ntry:\n    index.nprobe = 32\nexcept AttributeError:\n    pass\n</code></pre> <pre><code># Load vectors from Parquet (handles float32 or uint8 storage)\nimport pyarrow.parquet as pq\nimport numpy as np\n\nt = pq.read_table(\"artifacts/motw_vectors.parquet\")  # or per-document under artifacts/vectors/\ncols = set(t.schema.names)\n\nif \"embedding\" in cols:  # float32\n    embs = np.array([np.array(x) for x in t.column(\"embedding\").to_pylist()], dtype=\"float32\")\nelif \"embedding_q\" in cols:  # uint8\n    q = np.array([np.array(x, dtype=\"uint8\") for x in t.column(\"embedding_q\").to_pylist()], dtype=\"uint8\")\n    x = (q.astype(\"float32\") / 127.5) - 1.0\n    # re-normalise to unit length\n    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-6\n    embs = x / n\nelse:\n    raise ValueError(\"No embedding column found\")\n</code></pre>"},{"location":"data_source_optimisation/#how-these-pieces-fit-the-platform","title":"How these pieces fit the platform","text":"<ul> <li>Parquet carries all chunk text and metadata for analytics, auditing, and lightweight keyword indexing.</li> <li>FAISS provides high-quality semantic retrieval over the same chunks.</li> <li>Storing Parquet embeddings as uint8 keeps long-term storage small while preserving near-identical search behaviour when vectors are dequantised and normalised.</li> <li>HNSW is the default for accuracy and simplicity. IVF-PQ is available when a much smaller index is needed, with tunable recall.</li> </ul>"},{"location":"data_source_optimisation/#glossary","title":"Glossary","text":"<ul> <li>Parquet: a columnar file format for tables. Efficient to store and quick to scan. Works well with Pandas, DuckDB, Spark and friends.</li> <li>FAISS: Facebook AI Similarity Search - a library for fast similarity search over vectors (numerical representations of text). Lets you find \"things like this\" quickly. </li> <li>HNSW: Hierarchical Navigable Small World graph. A very fast index structure used by FAISS for high\u2011quality approximate nearest\u2011neighbour search.</li> <li>Artefacts: the files we produce from processing (e.g. Parquet tables and FAISS indexes) that you can store, publish, or reuse.</li> <li>L2\u2011normalise: scale a vector so its length is 1. This keeps cosine similarity meaningful and makes quantisation behave well.</li> </ul>"},{"location":"dev_log/","title":"Development Log \u2014 MapOfTheWorld","text":""},{"location":"dev_log/#2025-08-06-review-the-savvi-model-concepts-agin","title":"2025-08-06 - Review the SAVVI model concepts agin","text":"<ul> <li>Landed back on the SAVVI model, wondering if that was perhaps a better fit for htis project. Decided against. </li> </ul>"},{"location":"dev_log/#2025-08-05-input-form-logic-with-yaml-export","title":"2025-08-05 \u2014 Input form logic with YAML export","text":"<ul> <li>Finalised static HTML form for submitting new entries to the Map of the World</li> <li>Form supports(relevant) <code>@type</code> values defined in the SCCM-aligned data model: ORGANIZATION, SERVICE, EVENT, PLAN, COLLECTION, PERSON, RESOURCE, RELATIONSHIP</li> <li>Each type has type specific field section, dynamically shown/hidden on selection</li> <li>RELATIONSHIP includes source type dropdown to filter valid <code>relationship_type</code> options</li> <li>To help users, placeholder examples added (in grey text) for multi-value fields like Collaborators, Related Entities, Interests, etc</li> <li>Placeholders are auto-stripped from YAML output if unchanged(to make sure examples dont go into yml output)</li> <li>When <code>@type</code> changes, previously generated YAML box cleared to avoid confusion between entries</li> </ul>"},{"location":"dev_log/#implementation-rationale","title":"Implementation rationale","text":"<p>The form built using a lightweight HTML/CSS/JS stack - no frameworks or backend, relying only on js-yaml to handle YAML serialisation in browser. This driven by the following initial thinking:</p>"},{"location":"dev_log/#full-compatibility-with-github-pages","title":"Full compatibility with GitHub Pages","text":"<p>Form intended to be served directly from the repo \u2014 no server, no build process. Fully static ensures GitHub Pages compatibility, i.e. support for only static HTML/CSS/JS. This ensures all aspects of the form is human-readable, no reliance on 3rd party form, JS frameworks, or backend APIs that could disappear or break over time. </p>"},{"location":"dev_log/#git-native-and-open-source-aligned","title":"Git-native and open source aligned","text":"<p>The YAML output integrates directly with the project\u2019s Git-based workflow \u2014 contributors can download the file or submit via PR(not yet implemented), and the data remains version-controlled. This supports the vision of open, and collaborative mapping of the CSC ecosystem.</p>"},{"location":"dev_log/#problems-encountered","title":"Problems encountered","text":"<ul> <li>Type-specific sections not appearing: logic flaw, only <code>ORGANIZATION</code> and <code>RELATIONSHIP</code> type fields were initially showing. fixed by ensuring <code>toggleTypeSections()</code> logic applied on both load and change.</li> <li>YAML not generating or clearing the form unexpectedly: \"Generate YAML\" button reset the form and wipe data. caused by incorrect form handling, resolved by preventing the default submit behaviour.</li> <li>Type-specific fields missing from YAML output: type fields not always included in the final output. fixed by reintroducing and checking all relevant fields per type</li> <li>Placeholder values being treated as real input: <code>example_value</code> showing in YAML. We introduced <code>cleanList()</code> and <code>cleanTextList()</code> utilities to strip placeholder content automatically.</li> </ul>"},{"location":"dev_log/#2025-08-01-mailto-links-devlog-structure-and-public-facing-dev-log","title":"2025-08-01 \u2014 Mailto links, devlog structure, and public-facing dev log","text":"<ul> <li>Revised 'suggest improvement|fix' mailto to make it easier for direct contributions</li> <li>define a structure for this devlog. Using as a hybrid devlog/changelog</li> <li>Skipped tags and automation for now \u2014 Markdown works well and is fast to edit manually</li> </ul> <p>Notes: </p>"},{"location":"dev_log/#2025-07-31-cytoscape-class-filters-bug-stlite-graph-fixes","title":"2025-07-31 \u2014 Cytoscape class filters bug, stlite graph fixes","text":"<ul> <li>Found a stubborn bug where class-based filters in Cytoscape weren\u2019t applying on page load</li> <li>After investigating, I discovered that class assignment lagged layout rendering. A simple <code>setTimeout</code> workaround after graph layout stabilisation solved it</li> <li>Also fixed a mismatch in hardcoded colours and class names in the static legend \u2014 \u2018ORGANIZATION\u2019 nodes were showing grey due to casing</li> </ul>"},{"location":"dev_log/#2025-07-30-static-legend-switch-for-graph-ui","title":"2025-07-30 \u2014 Static legend switch for graph UI","text":"<ul> <li>The dynamic legend system was fragile, especially when class names didn\u2019t match types consistently</li> <li>I rebuilt the legend as a static block \u2014 hardcoding known entity types like <code>ORGANIZATION</code>, <code>SERVICE</code>, <code>PERSON</code> etc. with consistent colours</li> <li>Filter defaults now apply correctly on page load. The legend also starts collapsed, which feels cleaner</li> </ul>"},{"location":"dev_log/#2025-07-29-yaml-errors-and-edge-failures-in-cytoscape-build","title":"2025-07-29 \u2014 YAML errors and edge failures in Cytoscape build","text":"<ul> <li>Ran into YAML errors during <code>admin-build_cytoscape_json.py</code> \u2014 one malformed file broke the whole build</li> <li>Also noticed skipped edges due to missing nodes, traced to naming mismatches (e.g. <code>data_to_insight -&gt; d2i_excel_toolkit_maintenance</code>)</li> <li>Rebuilt one broken relationship YAML from a working template, which fixed the issue \u2014 reinforcing the need for stricter YAML validation tooling</li> </ul>"},{"location":"dev_log/#2025-07-22-to-2025-07-26-external-data-inclusion-and-web-scraping","title":"2025-07-22 to 2025-07-26 \u2014 External data inclusion and web scraping","text":"<ul> <li>Decided to expand the knowledge base by integrating published web content (PDFs, guidance, reports)</li> <li>Designed a strategy to store scraped content under <code>data_web/</code> and generate YAML metadata for indexing</li> <li>Early blockers included inconsistent text extraction (e.g. malformed GOV.UK PDFs) and parsing issues for structured headers in some <code>.txt</code> and <code>.pdf</code> files</li> </ul>"},{"location":"dev_log/#2025-07-19-pivot-away-from-streamlitstlite-to-mkdocs-js","title":"2025-07-19 \u2014 Pivot away from Streamlit/stlite to MkDocs + JS","text":"<ul> <li>After several attempts to render interactive network graphs in <code>stlite</code>, I hit repeated blockers</li> <li>Despite simplifying the data and testing basic layouts, Cytoscape.js would either not load properly or failed silently due to lack of support for external JavaScript modules and delayed layout rendering</li> <li>These limitations made <code>stlite</code> too fragile for graph-based exploration \u2014 especially with filters, tooltips, and legend controls</li> <li>Decided to switch fully to MkDocs as the primary documentation and frontend base, embedding custom HTML + JavaScript components directly</li> <li>This offered a much more stable and extensible foundation for publishing network diagrams, filtered views, and layered data exploration</li> </ul> <p>Notes: This was a significant architectural shift, but it unlocked better search integration, reusable visual components, and static hosting via GitHub Pages without relying on Python runtime hacks</p>"},{"location":"dev_log/#2025-07-18-migration-to-stlite-for-frontend-hosting","title":"2025-07-18 \u2014 Migration to <code>stlite</code> for frontend hosting","text":"<ul> <li>Shifted from standard Streamlit to <code>stlite</code> to support deployment via GitHub Pages \u2014 keeping everything browser-based</li> <li>Had to strip out unsupported modules like <code>pyvis</code>, <code>pathlib.Path(__file__)</code>, and Parquet I/O</li> <li>Rebuilt the visualisation to use JSON and embedded Cytoscape.js directly in HTML</li> </ul>"},{"location":"dev_log/#2025-07-17-parquet-removal-and-frontend-browser-shift","title":"2025-07-17 \u2014 Parquet removal and frontend browser shift","text":"<ul> <li>Began adapting the Streamlit app to run fully in-browser using <code>stlite</code></li> <li>Removed Parquet-based data loading due to Pyodide/browser compatibility</li> <li>Replaced with pre-generated JSON stored in <code>data/index_data.json</code> and loaded via HTTP</li> <li>Shifted from Python visualisation tools to pure JS (Cytoscape.js) for performance and portability</li> </ul>"},{"location":"dev_log/#2025-07-16-sccm-node-type-update-for-yaml-compatibility","title":"2025-07-16 \u2014 SCCM node type update for YAML compatibility","text":"<ul> <li>Switched all entity type declarations from <code>@type: PERSON</code> to <code>@type: 'PERSON'</code> (with quotes) to avoid YAML parsing issues</li> <li>This change rippled through the data layer and required updates to validation and the graph builder</li> <li>Highlighted the need for stricter YAML schema validation down the line</li> </ul>"},{"location":"dev_log/#2025-07-14-fixing-schema-to-graph-disconnects","title":"2025-07-14 \u2014 Fixing schema-to-graph disconnects","text":"<ul> <li>Spotted broken links and dangling edges in the graph caused by mismatches between service/organisation IDs and their relationship definitions</li> <li>Rewrote edge-building logic to verify both subject and object exist before drawing the edge</li> <li>Output clearer error messages when skipping invalid relationships</li> </ul>"},{"location":"dev_log/#2025-07-09-visual-and-navigational-structure-rethink","title":"2025-07-09 \u2014 Visual and navigational structure rethink","text":"<ul> <li>Reorganised the <code>/docs/</code> folder to avoid the sprawl of earlier MkDocs projects</li> <li>Added grouped navigation for tools, scrapes, and thematic areas like Early Help, SEND, and Benchmarking</li> <li>Refined the script that auto-generates <code>mkdocs.yml</code> navigation from the folder structure</li> </ul>"},{"location":"dev_log/#2025-07-03-csv-and-contact-data-integration","title":"2025-07-03 \u2014 CSV and contact data integration","text":"<ul> <li>Added ability to merge contact lists from separate sources (e.g. Wix exports and curated CSVs)</li> <li>Extracted email domains and lowercased fields for consistency</li> <li>Laid early groundwork for people-entity mapping within the broader ecosystem graph</li> </ul>"},{"location":"motw-reuse-guide-local-authorities/","title":"Reusing the \u201cMap of the World\u201d corpus locally","text":"<p>A short guide for local authority colleagues</p> <p>This note explains how you can start from the shared corpus and add your own documents without rebuilding everything. It also lists the files the notebook creates and where they live.</p>"},{"location":"motw-reuse-guide-local-authorities/#what-you-can-do","title":"What you can do","text":"<ul> <li>Start from the shared corpus and add your own PDFs, Word documents, or text files.  </li> <li>Append only what is new. Unchanged documents are skipped automatically.  </li> <li>Keep storage small. We store compact Parquet tables and a FAISS index, not the original PDFs.  </li> <li>Reuse the corpus in your own analysis tools. Parquet and FAISS are standard, portable formats.</li> </ul>"},{"location":"motw-reuse-guide-local-authorities/#what-is-in-the-shared-pack","title":"What is in the shared pack","text":"<p>Place these in your project folder:</p> <pre><code>csc_artifacts/\n  state.json                 # records which documents were processed and with which settings\n  chunks/                    # per-document Parquet of chunked text + metadata\n  vectors/                   # per-document Parquet of vectors (uint8 by default)\n  motw_index.faiss           # FAISS index (HNSW) built over all vectors\n# optional\n  motw_chunks.parquet        # combined text+meta (only if materialised)\n  motw_vectors.parquet       # combined text+vectors (only if materialised)\n</code></pre> <p>You provide your own documents here:</p> <pre><code>data_published/              # put your PDFs, .docx, or .txt here\n</code></pre> <p>We avoid storing PDFs in Git. Keep them local or in object storage if you need an archive.</p>"},{"location":"motw-reuse-guide-local-authorities/#what-the-notebook-creates","title":"What the notebook creates","text":"<p>When you run the notebook, it will create or update:</p> <ul> <li><code>csc_artifacts/state.json</code> - a small manifest with hashes, sizes, chunk counts, and build settings.  </li> <li><code>csc_artifacts/chunks/&lt;doc_id&gt;.parquet</code> - chunked text and metadata for each document.  </li> <li><code>csc_artifacts/vectors/&lt;doc_id&gt;.parquet</code> - the matching vectors for each document. Stored as <code>embedding_q</code> (uint8) with a note on how to dequantise.  </li> <li><code>csc_artifacts/motw_index.faiss</code> - the FAISS search index.  </li> <li>Optionally, <code>csc_artifacts/motw_chunks.parquet</code> and <code>csc_artifacts/motw_vectors.parquet</code> if you turn on materialisation.</li> </ul> <p>We identify each document by the first 16 hex characters of its SHA-256 hash. This keeps the build incremental and avoids duplicates.</p>"},{"location":"motw-reuse-guide-local-authorities/#two-main-ways-to-run","title":"Two main ways to run","text":""},{"location":"motw-reuse-guide-local-authorities/#1-fresh-build","title":"1) Fresh build","text":"<p>Use when you are starting from scratch or want a clean rebuild.</p> <ul> <li>The notebook scans <code>data_published/</code>, extracts text, chunks it, embeds it, writes per-document Parquet, and builds a fresh FAISS index.  </li> <li>It writes a new <code>state.json</code> so future runs are incremental.</li> </ul>"},{"location":"motw-reuse-guide-local-authorities/#2-append-your-new-files","title":"2) Append your new files","text":"<p>Use when you are starting from the shared pack and want to add your own documents.</p> <ul> <li>Put your documents into <code>data_published/</code>.  </li> <li>The notebook compares hashes against <code>state.json</code>. Unchanged documents are skipped.  </li> <li>New documents are processed and appended to the existing FAISS index.  </li> <li>If any existing document has changed, the notebook rebuilds the index from the per-document Parquet to keep it correct.</li> </ul>"},{"location":"motw-reuse-guide-local-authorities/#other-useful-options","title":"Other useful options","text":"<ul> <li>Rebuild from Parquet only - if you have the per-document Parquet but no FAISS file, the notebook can rebuild the index without touching PDFs.  </li> <li>Materialise a single-file Parquet - if you prefer a single file, turn on materialisation to write <code>motw_chunks.parquet</code> and <code>motw_vectors.parquet</code>. This is optional because the per-document layout already supports incremental updates.  </li> <li>Upload to object storage - if you set the Cloudflare R2 variables, the notebook can upload updated artefacts after a run.</li> </ul>"},{"location":"motw-reuse-guide-local-authorities/#notes-on-changes-and-deletions","title":"Notes on changes and deletions","text":"<ul> <li>Changed documents - if the content of a document changes, its hash changes. The notebook reprocesses that document and rebuilds the FAISS index so your search stays correct.  </li> <li>Deleted documents - remove the matching files in <code>csc_artifacts/chunks/</code> and <code>csc_artifacts/vectors/</code> and delete the entry in <code>state.json</code>, then run the notebook to rebuild the FAISS index.</li> </ul>"},{"location":"motw-reuse-guide-local-authorities/#what-success-looks-like","title":"What success looks like","text":"<p>At the end of a run you will see a summary like:</p> <pre><code>Docs: 120  Read errors: 0  Empty: 0\nChars: 14,200,000  Chunks: 95,400\n[Embedding] 00:02:15s\nWrite/Update FAISS...\nrows (meta, vectors, index): 95,400, 95,400, 95,400\nDone.\n</code></pre> <ul> <li>The counts for meta, vectors, and the FAISS index should match.  </li> <li>If you only added new files, you should see a note about appending to the index rather than rebuilding.</li> </ul>"},{"location":"motw-reuse-guide-local-authorities/#why-this-is-shareable-and-future-proof","title":"Why this is shareable and future proof","text":"<ul> <li>Standard formats - Parquet can be read by Pandas, Polars, DuckDB, Spark and more. FAISS is a common vector index.  </li> <li>Local-first - you can work entirely on your machine. No need to upload PDFs.  </li> <li>Incremental by design - starting from the shared pack means you only build what is new, then carry on where we left off.</li> </ul>"},{"location":"network/","title":"CSC Network Graph (in dev)","text":"<p>This interactive graph shows key organisations, plans, and events in the children\u2019s services data ecosystem. Use the dropdown to filter by type. You can select more than one item using Ctrl/Cmd.</p> <p>Dev-notes: Data &amp; relations currently being added, graph layout and naming conventions for nodes in particular is a work in progress as we progress possible use-cases and standardise yml object structure.</p> Search: Filter by node type(s): Organizations Plans Events Services Reset View <p>Submit suggested map corrections or data additions</p>"},{"location":"sccm_relation_types/","title":"CSC Network Relationships","text":"<p>This is a reference page detailing the scope of relationship types defined within the Smart City Concept Model (SCCM) as applied to this tools Children's Social Care CSC Network Diagram. These relationships are reproduced directly from the Smart City Concept Model definitions, but shown here for each relevant object type towards additional clarity on the derived network diagram (generated from the core YAML definitions within this tool).</p> Service type relationships SubjectRelationshipObject ServicecontainsService ServiceinfluencedByObjective ServiceprovidedByAgent ServiceresponsibilityOfAgent ServiceserviceImplementsMethodMethod ServiceusedByCommunity ServicesubjectOfAgreement ServicecontainedInService ServicecontainedInFunction ServiceraisesCase ServiceusesResourceResource ServicehasRuleRule Event type relationships SubjectRelationshipObject EventatPlacePlace EventhasOutcomeState EventcontainedInCase EventcontainedInAccount EventhasRoleFromItem EventhasOutcomeDecision EventeventPlannedInPlan Plan type relationships SubjectRelationshipObject PlancontainsPlan PlanhasTargetTarget PlaninfluencedByObjective PlanplanDerivedFromMethodMethod PlanplanForEventEvent PlanplanForCaseCase PlancontainedInPlan PlanplanOfAgent PlanusesResourceResource Community type relationships SubjectRelationshipObject CommunitycontainsCommunity CommunitycontainedInCommunity CommunityusesService Organization type relationships SubjectRelationshipObject OrganizationcontainsOrganization OrganizationcontainedInOrganization OrganizationhasMemberPerson Person type relationships SubjectRelationshipObject PersonmemberOfOrganization Rule type relationships SubjectRelationshipObject RuleruleForService Resource type relationships SubjectRelationshipObject ResourceresourceForService ResourceresourceForPlan ResourceresourceOfAgent Collection type relationships SubjectRelationshipObject CollectioncollectionContainsItem CollectioncollectionDefinedByAgent Agent type relationships SubjectRelationshipObject AgenthasObject AgenthasAbstract AgenthasAgreementAgreement AgenthasObjectiveObjective AgenthasPlanPlan AgenthasResourceResource AgenttakesDecisionDecision AgentusesItem AgentmakesAssumptionAssumption AgentdefinesCollectionCollection AgentownsAccount AgentprovidesService AgentresponsibleForService"},{"location":"search/","title":"CSC Search","text":"Search the CSC Network Knowledge Base <p> Reference detail of the search index scope and the search strategy applied here.</p> <p>JavaScript is required to use search function.</p>"},{"location":"search_pipeline/","title":"CSC Search Strategy","text":""},{"location":"search_pipeline/#full-text-search-strategy-optimised-for-static-hosting","title":"Full-Text Search Strategy (Optimised for Static Hosting)","text":"<p>To support search across the contents of hundreds of documents and varied sources (including PDFs) within our MkDocs-based site, we needed an approach that simulates full-text search while remaining scalable, performant, and static-host compatible (i.e. no backend/db).</p>"},{"location":"search_pipeline/#main-goals","title":"Main Goals","text":"<ul> <li>Allow users to search document content (not just titles)</li> <li>Ensure performance remains fast even as hundreds of documents are added</li> <li>Avoid full-text duplication or payload bloat in both the frontend and backend(to avoid hitting Git limits)</li> <li>Keep compatibility with static site hosting (e.g. GitHub Pages, <code>list.js</code>, <code>lunr.js</code>, Mkdocs)</li> </ul>"},{"location":"search_pipeline/#optimisation-strategy","title":"Optimisation Strategy","text":"<p>Rather than store the entire text of every PDF in the search index (which would be slow and bloated), we take the following approach:</p> <ol> <li>Extract Text from PDFs </li> <li>Using <code>pdfplumber</code>, each document\u2019s text is extracted and cleaned</li> <li> <p>Unicode characters like curly quotes, dashes, ellipses are normalised for consistency</p> </li> <li> <p>Generate Excerpts </p> </li> <li>A short <code>excerpt</code> is created from the first meaningful paragraph (ignoring TOCs and headings)</li> <li> <p>This is used in search results and graph visualisation tooltips</p> </li> <li> <p>Lemmatise and Tokenise Keywords </p> </li> <li>Words are lemmatised (e.g. running, ran, runs \u2192 run) using <code>nltk</code></li> <li>Common English stopwords are removed</li> <li> <p>This creates a compressed keyword representation of each document</p> </li> <li> <p>Apply Document Frequency Filtering </p> </li> <li>Using <code>CountVectorizer</code> from <code>scikit-learn</code>, we:<ul> <li>Remove overly common terms (appear in &gt;85% of docs)</li> <li>Remove rare noise terms (appear in &lt;2 docs)</li> </ul> </li> <li> <p>This results in signal-rich keywords per document</p> </li> <li> <p>Final Search Index Generation </p> </li> <li>Each optimised document (content) is represented in <code>docs/search_index.json</code> with:<ul> <li><code>doc_id</code>, <code>name</code>, <code>excerpt</code>, <code>url</code>, <code>tags</code>, and optimised <code>keywords</code></li> </ul> </li> <li>At runtime, search results display:<ul> <li>Matched titles</li> <li>Decoded excerpts</li> <li>A prioritised and randomly sampled subset of keywords (max 20) to avoid repetition or alphabetical bias</li> </ul> </li> </ol>"},{"location":"search_pipeline/#optional-archive-parquet","title":"Optional Archive (Parquet)","text":"<p>For potential archival or later ML processing is needed, a full <code>.parquet</code> file can also be saved  (<code>admin_scripts/docs_index.parquet</code>) with complete text and metadata (disabled by default to keep project lightweight, but a variable flag can bve set in the py index build script).</p>"},{"location":"search_pipeline/#key-libraries-used","title":"Key Libraries Used","text":"Purpose Tool PDF text extraction <code>pdfplumber</code> Text cleanup <code>re</code>, <code>unicodedata</code>, <code>DOMParser</code> (JS) Lemmatisation &amp; stopwords <code>nltk</code> Frequency filtering <code>scikit-learn</code> (<code>CountVectorizer</code>) Output formats <code>json</code>, <code>pandas</code>, <code>parquet</code> <p>This setup we think ensures search is (acceptably)fast, useful, and scalable \u2014 and that the project can grow without sacrificing performance or frontend simplicity. We're in the process of scaling this up for more complete/realistic testing alongside a cyclic review approach. </p>"},{"location":"sources/","title":"Data Sources","text":"<p>To add transparency to the search in particular, below is the current scope of the input data/sources. Some of the labelling comes direct/dynamically off the sources themselves and may therefore be inconsistent.</p>"},{"location":"sources/#sccm-aligned-yaml-metadata-25-sources","title":"SCCM-aligned YAML Metadata (~25 sources)","text":""},{"location":"sources/#events","title":"events","text":"Source File Type Word Count Last Refreshed childrens_social_care_in_england_2025 .yaml - 29/07/2025 childrens_social_care_review .yaml - 29/07/2025 childrens_wellbeing_schools_bill_2024_25 .yaml - 29/07/2025"},{"location":"sources/#organizations","title":"organizations","text":"Source File Type Word Count Last Refreshed adcs .yaml - 29/07/2025 data_to_insight .yaml - 29/07/2025 department_for_education .yaml - 29/07/2025 essex_county_council .yaml - 29/07/2025 hertfordshire_county_council .yaml - 29/07/2025 knowsley_council .yaml - 29/07/2025 lancaster_university .yaml - 29/07/2025 rcc .yaml - 29/07/2025"},{"location":"sources/#plans","title":"plans","text":"Source File Type Word Count Last Refreshed childrens_social_care_national_framework .yaml - 30/07/2025 ddsf .yaml - 30/07/2025 ffp_programme_guide_2025 .yaml - 30/07/2025 national_kinship_care_strategy .yaml - 30/07/2025 nvest .yaml - 30/07/2025"},{"location":"sources/#resources","title":"resources","text":"Source File Type Word Count Last Refreshed ddsf1a_standard_safeguarding_dataset .yaml - 30/07/2025 ddsf2a_social_workers_and_cms_constraints .yaml - 30/07/2025 standard_safeguarding_dataset .yaml - 30/07/2025"},{"location":"sources/#rules","title":"rules","text":"Source File Type Word Count Last Refreshed childrens_act_1989 .yaml - 30/07/2025 keeping_children_safe_in_education_2025 .yaml - 29/07/2025"},{"location":"sources/#services","title":"services","text":"Source File Type Word Count Last Refreshed centre_of_excellence .yaml - 30/07/2025 d2i_apprenticeships .yaml - 30/07/2025 d2i_data_validators .yaml - 30/07/2025 d2i_excel_toolkit .yaml - 30/07/2025"},{"location":"sources/#published-reports-and-frameworks-16-sources","title":"Published Reports and Frameworks (~16 sources)","text":""},{"location":"sources/#data_published","title":"data_published","text":"Source File Type Word Count Last Refreshed ADCS_Safeguarding_Pressures_Phase9_FINAL .pdf 28275 29/07/2025 Adapting Case Management Systems for Families First_May 2025 .pdf 2495 29/07/2025 CSC for Data People v1.0 2024-05-21 .pdf 6658 29/07/2025 Children_in_need_census_2023_to_2024_guide_v1.0 .pdf 20749 29/07/2025 Childrens_Social_Care_National_Framework__Dec_2023 .pdf 21664 29/07/2025 Childrens_social_care_dashboard_supporting_information_Oct_2024 .pdf 4182 29/07/2025 Childrens_social_work_workforce_census_2024_to_2025_technical_specification_v1 .pdf 4124 29/07/2025 Evaluation of the outcomes and impact of the TFD and associated .pdf 43461 29/07/2025 Keeping_children_safe_in_education_2025 .pdf 67091 29/07/2025 NVEST Newsletter June 25 .pdf 704 29/07/2025 Reimagining Case Management in Children's Social Care Pilot - NYC Evaluation .pdf 21944 29/07/2025 SSD Benefits Summary .pdf 315 29/07/2025 SSD for Power BI and Tableau .pdf 585 29/07/2025 Standard Safeguarding Dataset - Final Report April 2024 v1.0 .pdf 8932 29/07/2025 Standard Safeguarding Dataset - User Research Synthesis June 2023 v1.0 .pdf 2985 29/07/2025 Understanding social worker recording (Essex) .pdf 34882 29/07/2025"},{"location":"sources/#cloned-documentation-repos-18-sources","title":"Cloned Documentation Repos (~18 sources)","text":"Source File Type Word Count Last Refreshed annex-a-sen-validator-be_README .md 12 30/07/2025 cs-demand-model_README .md 625 30/07/2025 csc-validator-be-903_README .md 1063 30/07/2025 csc-validator-be-cin_README .md 1099 30/07/2025 d2i-contacts_README .md 593 30/07/2025 d2i-linux-build_README .md 1247 30/07/2025 foi-csc-scrape-tool_README .md 486 30/07/2025 foi-csc-scrape-tool_sccm .yml - 30/07/2025 hmi-probation-youth-justice-scrape_README .md 249 30/07/2025 hmi-probation-youth-justice-scrape_sccm .yml - 30/07/2025 nvest_README .md 1109 30/07/2025 ofsted-ilacs-scrape-tool_README .md 1648 30/07/2025 ofsted-ilacs-scrape-tool_sccm .yml - 30/07/2025 ofsted-jtai-scrape-tool_README .md 1022 30/07/2025 ofsted-jtai-scrape-tool_sccm .yml - 30/07/2025 ofsted-send-scrape-tool_README .md 1026 30/07/2025 ofsted-send-scrape-tool_sccm .yml - 30/07/2025 patch_README .md 662 30/07/2025"},{"location":"sources/#public-web-data-6-sources","title":"Public Web Data (~6 sources)","text":""},{"location":"sources/#data_web","title":"data_web","text":"Source File Type Word Count Last Refreshed adcsorguk-adcs-safeguarding-page .txt 843 29/07/2025 barnetgovuk-barnet-resource .txt 310 29/07/2025 bathnesgovuk-bath-and-north-east-somerset-resource .txt 720 29/07/2025 childrensservicesnetwork-childrens-services-network .txt 28 29/07/2025 govuk-department-for-education .txt 1510 29/07/2025 lbbdgovuk-barking-and-dagenham-resource .txt 317 29/07/2025"}]}