{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#about-this-project","title":"About this project","text":"<p>Map of the World is an open-source knowledge base for Children\u2019s Social Care (CSC). It brings together key documents, tools, services, and guidance into one structured, searchable map.</p>"},{"location":"#what-youll-find","title":"What you\u2019ll find","text":"<ul> <li>A visual network showing how projects, organisations, and frameworks connect  </li> <li>A smart search tool for quick access to relevant information  </li> <li>Simple, human-readable data formats so anyone can contribute  </li> </ul> <p>The aim is to make the CSC landscape clearer and more connected. Built on open technology, this project is designed to be transparent, flexible, and collaborative, helping the sector work smarter, not harder.</p> <p>This proof of concept (PoC) forms part of the work toward a Centre of Excellence for CSC, building on thinking behind platforms like the Children\u2019s Services Network and using open modelling approaches such as the Smart City Concept Model. It is designed to be lightweight, transparent, and openly extensible so others can adopt or adapt it for their own contexts.</p>"},{"location":"#purpose","title":"Purpose","text":"<p>This PoC is the first step toward creating a clear, searchable map of everything happening in CSC. It will bring together reports, tools, frameworks, guidance, and projects into one place so people can easily find and understand what is out there. We aim to make it simple to:</p> <ul> <li>See how initiatives, policies, and systems connect  </li> <li>Find out who is working on what and track updates  </li> <li>Bring hidden or siloed work into view to align efforts and avoid duplication  </li> <li>Give local teams a clearer picture of sector-wide activity  </li> </ul>"},{"location":"#whats-included","title":"What\u2019s included","text":"<ul> <li>Published reports and official guidance  </li> <li>Data from government and local authority websites  </li> <li>Sector-developed tools and frameworks  </li> <li>Links to people and organisations (where consent or public information allows)  </li> </ul> <p>All accessible through an intuitive interface with a visual map and search tools.</p>"},{"location":"#who-will-use-it","title":"Who will use it","text":"<p>Local authority data teams, service managers, researchers, analysts, and project leads working on CSC data or digital delivery. Ultimately, this is a collaborative tool, built with input from local authorities, analysts, service leads, academic partners, and national bodies.</p>"},{"location":"#how-this-helps","title":"How this helps","text":"<p>This tool will make life easier across the CSC sector by:</p> <ul> <li>Showing the bigger picture, clarifying how local and national initiatives, policies, systems, and data sources fit together  </li> <li>Highlighting who is doing what, enabling you to track new tools, framework updates, and service changes  </li> <li>Bringing hidden work into view, helping teams align efforts, build on progress, and avoid duplication  </li> <li>Giving local authority data teams a more joined-up view of sector-wide activity  </li> </ul>"},{"location":"#the-plan","title":"The plan","text":"<ul> <li>Interactive network map, explore a live graph showing how entities, relationships, and systems connect across the CSC sector.  </li> <li>Structured data records, a growing library of YAML files (human-readable format) based on the Smart City Concept Model (SCCM, BSI PAS 182). These describe tools, frameworks, relationships, rules, plans, events, and guidance.  </li> <li>Documentation hub, technical documentation from Data to Insight (D2I) projects and Git repositories indexed for developers and analysts.  </li> <li>Searchable resource, a dedicated search page indexing YAML content and files (.md, .pdf, .py, .js, .html) with keyword relevance, match scoring, and metadata extraction.  </li> <li>In development, currently using a small sample of local authority data (around 10 sites), with plans to expand to all 153 authorities and scrape relevant CSC public sources (.gov, .edu).  </li> </ul>"},{"location":"#how-it-is-structured","title":"How it is structured","text":"<p>The tool is built around the Smart City Concept Model (SCCM), an open framework for describing public service ecosystems. Every item in the network diagram is stored as a YAML file, which is simple and easy to edit. Each file starts with a type, for example:</p> <ul> <li>AGENT, people, teams, or organisations  </li> <li>SERVICE, a system, service, or tool  </li> <li>EVENT, inspections, launches, or reviews  </li> <li>RULE / PLAN / COLLECTION, policies, datasets, or strategies  </li> <li>RELATIONSHIP, links between entities (for example oversight, supply, influence)  </li> </ul> <p>These YAML files are validated, searchable, and easier to edit than formats like JSON or CSV, making contributions simpler.</p>"},{"location":"#how-to-get-involved","title":"How to get involved","text":"<p>This project is being built with the sector, for the sector, and we would welcome your input. You can help by:</p> <ul> <li>Sharing feedback on what works, what is missing, or what needs fixing  </li> <li>Contributing local projects, tools, or documentation  </li> <li>Suggesting ideas for how this tool could better support the sector  </li> </ul> <p>To get involved, contact the Data to Insight team or visit our GitHub repository to explore, fork, or contribute.</p> <p>If you think that this tool might be useful within the sector and want to pass this on to colleagues, feel free to make use of the below infographic that we thought might offer the key headline insights more succintly.</p> <p></p> <p></p> <p>  This PoC forms part of the work towards a Centre of Excellence, building on the thinking behind platforms like the Children\u2019s Services Network and grounded in open modelling approaches like the already mentioned Smart City Concept Model.</p> <p> </p> <p> </p>"},{"location":"#soft-systems-conceptual-mapping","title":"Soft Systems Conceptual Mapping","text":"<p>System of Interest </p> <p>Shared|public data platform and ecosystem used within Children\u2019s Social Care to connect people|LA colleagues, data, tools, and services</p> <p>Purpose </p> <p>To enable shared sector understanding, validation, discovery, and collaboration between local authorities, tool development, projects and other initiatives in CSC</p> <p>Worldview (Weltanschauung) </p> <p>Fragmented data landscapes transformed into a collaborative, open ecosystem using lightweight, transparent structures like SCCM, JSON, YAML + MkDocs</p> <p>Owner(s) </p> <p>Likely data platform stewards: D2I, local authority data teams, ecosystem developers</p> <p>Environment (External Constraints) </p> <p>GitHub Pages (no backend), data security and ethics, evolving standards, distributed maintenance, changing ecosystem, frameworks and statuatory guidance, browser-only deployments</p>"},{"location":"#whats-next","title":"What\u2019s next?","text":"<ul> <li>Ongoing expansion of linked tools, rules, and frameworks</li> <li>Live|scheduled scrapes from key web resources or published docs/framesworks</li> <li>Search and filter interface (in beta, but aiming to implement network diagram filters)</li> <li>Option for local teams to submit structured entries or link live repositories</li> <li>Export options for integration into other data tools</li> </ul> <p>Thanks for the interest in CSC Knowledge Base Network We hope it supports your work, and welcome your feedback as we continue to improve and expand it.</p>"},{"location":"#possible-sccm-mapping-to-csc-eco-system","title":"Possible SCCM Mapping to CSC Eco-System","text":"SCCM Concept (Category) Suggested example(s) (in progress) Community South East fostering cluster Documentation CSC Independent Review Events Children\u2019s Social Care Review, ILACS Inspections, Public Inquiries Organization Data to Insight, LIIA Persons Organisational/sector tools linked where consent given or public record Plans Kinship Care Strategy, Children\u2019s Social Care National Framework Relationships LA-1 \u2194 Supports \u2194 SSD Tests, DfE \u2194 Pilots \u2194 API Data Flows Rules Statutory Guidance, Keeping Children Safe in Education 2025 Sector Tools PATCH, ChAT Services"},{"location":"#data-and-formats","title":"Data and formats","text":"<p>The network visualisations, and meta data about those objects, within the site are built from structured YAML records (the source of truth), processed and published as static JSON files that the browser app loads at runtime. Parquet and FAISS artifacts built from an increasing corpora of publically available CSC sector reference data used for the search/back-end are made available for offline or downstream semantic search workflows (e.g. for LAs/data teams/stakeholders). At the moment these artifacts are built exclusively from externally sourced documents, official reports, DfE/Gov guidance, Official validation rules, social care frameworks and much, much more that defines what is currently happening within the CSC and directly related sectors' data ecosystems; but part of the planned development is to additionally include all data from the defined yaml objects shown in the network graph(s). </p>"},{"location":"#reuse-and-downloads","title":"Reuse and downloads","text":"<p>The published set of downloadable artifacts for reuse (Parquet, FAISS), plus lightweight JSON outputs used by the website.</p> Downloadable artifacts (Parquet and FAISS) Artifact Format Link on the site Path in repo What it is for Vector chunks Parquet motw_chunks.parquet <code>docs/data/csc_artifacts/motw_chunks.parquet</code> Text chunks table for semantic retrieval Vector embeddings Parquet motw_vectors.parquet <code>docs/data/csc_artifacts/motw_vectors.parquet</code> Embeddings aligned to chunk ids Vector index FAISS motw_index.faiss <code>docs/data/csc_artifacts/motw_index.faiss</code> Nearest neighbour index over embeddings Site outputs (JSON used by the browser) Artifact Format Link on the site Path in repo What it is for Graph data JSON graph_data.json <code>docs/data/graph_data.json</code> Nodes and edges for the Cytoscape network Search indexes JSON search_index.json, graph_search_index.json <code>docs/data/search_index.json</code>, <code>docs/data/graph_search_index.json</code> Fast static search for the site"},{"location":"dev-data_source_optimisation/","title":"Optimising Search for 'Map of the World'","text":"<p>From PDFs to compact, fast, portable indexes</p>"},{"location":"dev-data_source_optimisation/#why-this-matters-for-childrens-social-care-data","title":"Why this matters for children\u2019s social care data","text":"<p>Children\u2019s social care information is extensive and often spread very wide across government and partner websites, (long)PDFs, and research portals. Our approach focuses(in-part) on making that material accessible and searchable without moving heavy files around or duplicating storage.</p> <ul> <li>Fast, lightweight search: we extract text once and create a minimised search index (no PDFs, just raw optimised data)</li> <li>Clarity and provenance: chunk-level search surfaces the exact clauses (e.g. thresholds, Section 47, kinship care) with links back to the authoritative source and licence</li> <li>Low overhead, high resilience: compact Parquet/FAISS artefacts are inexpensive to store (e.g. in R2), simple to mirror, and easy to reuse locally by LAs/sector stakeholders</li> <li>Keeps pace with change: incremental builds pick up updates. Reprocessing just what changed, so the map stays current as guidance, published reports or sector research evolves</li> <li>Defined scope boundaries: Defined licensing metadata, and explicit source boundaries ensure ring-fenced sector scope</li> </ul>"},{"location":"dev-data_source_optimisation/#who-this-is-for-and-what-youll-get","title":"Who this is for (and what you\u2019ll get)","text":"<ul> <li>Product / content folks: how we take lots of PDFs and turn them into a tiny, fast search index that still finds the right things.</li> <li>Engineers / data people: the design choices, math, and scaling numbers so you can reuse this pipeline on your own document sets.</li> </ul> <p>One\u2011line summary: we extract text -&gt; split into semantically sensible chunks -&gt; embed each chunk as a vector -&gt; store compactly in Parquet using uint8 quantisation -&gt; build a FAISS similarity index (HNSW by default). The public search can then use a small JSON keyword index for near-instant client\u2011side search; deeper semantic search uses the vector index as needed.</p>"},{"location":"dev-data_source_optimisation/#why-this-approach-works","title":"Why this approach works","text":"<ol> <li>We do heavy lifting once, locally. PDFs are slow to parse and (comparitively)large to store especially at scale; we extract the text and avoid shipping raw files within the platform. </li> <li>We chunk long documents. Searching against topically coherent chunks(minimised to the most relevant) beats whole\u2011document matching for retrieval quality. </li> <li>We use sentence embeddings. Similarity in embedding space approximates \u201cis this about the same thing?\u201d</li> <li>We store vectors compactly. Quantising embeddings to uint8 shrinks storage ~4\u00d7 with negligible impact on retrieval quality for our use case(s). - Although we are still defining/asking about the sector's possible use-cases. </li> <li>We separate UX search from deep search. The site ships a tiny keyword index for instant(or at least faster) results; we're also aiming for external LA workflows being able to use the vector index offline.</li> </ol>"},{"location":"dev-data_source_optimisation/#what-tiny-recall-loss-means-plain-english","title":"What \u201ctiny recall loss\u201d means (plain English)","text":"<p>When we save embeddings as uint8 instead of 32\u2011bit floats, we\u2019re using a lossy compression scheme. That slightly perturbs numbers. If someone later rebuilds a vector index from those saved numbers, the nearest\u2011neighbour results may differ a bit from a float32 baseline.</p> <p>In practice with normalised sentence embeddings:</p> <ul> <li>Cosine similarity between original and dequantised vectors is usually \u2265 0.995 (to around ~0.999)</li> <li>End\u2011to\u2011end recall@10 typically changes by 0-2% on common text corpora</li> <li>Our live FAISS index is built directly from float32 in memory, so website retrieval quality is unaffected by how we store vectors in Parquet</li> </ul>"},{"location":"dev-data_source_optimisation/#the-quick-math","title":"The quick math","text":"<p>L2\u2011normalise each embedding vector <code>x</code> (so values lie roughly in [-1, 1]) and map each component to 8 bits:</p> <ul> <li>Quantise: <code>q = round((x + 1) * 127.5)</code> -&gt; <code>q \u2208 {0,\u2026,255}</code> </li> <li>Dequantise: <code>x' = (q / 127.5) - 1.0</code> </li> <li>Re\u2011normalise: <code>x'' = x' / ||x'||</code> (keeps cosine similarity meaningful)</li> </ul> <p>Because most information in sentence embeddings is in direction (not exact magnitudes), cosine similarity is very stable after this process.</p>"},{"location":"dev-data_source_optimisation/#the-pipeline-what-and-why","title":"The pipeline (what and why)","text":"<p>This a reference for devs and those managing the platform/repo back-end stuff, probably not relevant to most stakeholders. </p> <ol> <li>Text extraction (PyMuPDF; pdfminer fallback)  </li> <li>Why: PyMuPDF is 2-5\u00d7 faster on many PDFs  </li> <li>Chunking (default ~1,800 chars with ~150 overlap)  </li> <li>Why: balances context and index size; fewer, richer chunks -&gt; faster builds and smaller artefacts  </li> <li>Embedding (<code>all-MiniLM-L6-v2</code>, 384\u2011dim, normalised)  </li> <li>Why: Small, fast, good quality; normalisation makes cosine = inner product and improves quantisation robustness  </li> <li>Storage </li> <li>Per\u2011doc Parquet (text + metadata) -&gt; easy incremental updates  </li> <li>Vectors in Parquet as <code>uint8</code> -&gt; ~4\u00d7 smaller than float32, plus schema metadata describing de/quantisation  </li> <li>Combined Parquet (optional) for simple downstream use if LAs have a use for  </li> <li>Indexing (FAISS)  </li> <li>HNSW default: excellent recall, easier to tune  </li> <li>IVF\u2011PQ optional: order\u2011of\u2011magnitude smaller index for very large corpora; small recall trade\u2011off, tunable with <code>nprobe</code></li> <li>Incremental builds </li> <li><code>state.json</code> records per\u2011document SHA\u2011256; unchanged docs are skipped. Rebuilds are proportional to change, not corpus size</li> <li>Website search index (keyword JSON) </li> <li>From Parquet, not PDFs. Cleaned text, remove stop\u2011words and over\u2011common terms, derive per\u2011doc keywords -&gt; a very small JSON file for client\u2011side search</li> </ol>"},{"location":"dev-data_source_optimisation/#realworld-impact-for-users","title":"Real\u2011world impact for users","text":"<ul> <li>Fast search: small on\u2011site JSON makes type\u2011ahead and filters (appear) near-instant  </li> <li>Better hits: chunk\u2011level indexing can return relevant text blocks from extensive PDFs  </li> <li>Stable URLs: Not yet in use. But would be backlinks within the platform to object specific detail(s)  </li> <li>Scales: adding thousands of documents has less of impact on site response and repo bloat  </li> </ul>"},{"location":"dev-data_source_optimisation/#realworld-impact-for-devsengineersplatform","title":"Real\u2011world impact for devs/engineers/platform","text":"<p>This a reference for devs and those managing the platform/repo back-end stuff, probably not relevant to most stakeholders. </p> <ul> <li>Artefacts, not assets: Git &amp; Pages store compact Parquet/JSON, not PDFs  </li> <li>Cheap(er) storage: vectors as <code>uint8</code> and HNSW/IVF\u2011PQ keep Git/R2 costs low(er)  </li> <li>Reproducible builds: manifest + hashes = deterministic rebuilds  </li> <li>Interoperability: Parquet + FAISS are standard; others can reuse the corpus locally  </li> </ul>"},{"location":"dev-data_source_optimisation/#sizing-scaling-rules-of-thumb","title":"Sizing &amp; scaling (rules of thumb)","text":"<p>This a reference for devs and those managing the platform/repo back-end stuff, probably not relevant to most stakeholders. </p> <p>Let C = number of chunks. With 384\u2011dim embeddings:</p> <ul> <li>Float32 vector size (raw): <code>C \u00d7 384 \u00d7 4</code> bytes  </li> <li>UInt8 vector size (raw): <code>C \u00d7 384 \u00d7 1</code> bytes (~4\u00d7 smaller)  </li> <li>HNSW index size: typically ~1-2 KB per chunk (depends on <code>M</code>, dataset)  </li> <li>Parquet overhead: +10-30% over raw, but ZSTD helps especially for text  </li> </ul>"},{"location":"dev-data_source_optimisation/#example-projections","title":"Example projections","text":"<p>Assuming current chunking yields ~74 chunks/doc (observed ~1,928 chunks for 26 docs) -&gt; 3,000 docs ~ 222k chunks </p> Component Formula 222k chunks (est.) Parquet vectors (float32 raw) <code>C \u00d7 384 \u00d7 4</code> ~341 MB (+overhead) Parquet vectors (uint8 raw) <code>C \u00d7 384 \u00d7 1</code> ~85 MB (+overhead) HNSW index ~1-2 KB \u00d7 C ~220-440 MB Parquet text+meta compressed tens of MB (depends on text volume) <p>If HNSW index starts feeling large to juggle, flip to IVF\u2011PQ: you can expect tens of MB with recall@10 often 95-99%, tunable via <code>nprobe</code>.</p>"},{"location":"dev-data_source_optimisation/#keyword-index-whats-in-tiny-json","title":"Keyword index (what\u2019s in tiny JSON)","text":"<p>A compact, derived per\u2011document keyword list from the Parquet text:</p> <ul> <li>Normalise punctuation/quotes and remove boilerplate (e.g., page headers)  </li> <li>Lowercase, keep words of 4+ letters  </li> <li>Remove stop\u2011words (scikit\u2011learn English) and domain\u2011common words (CSC custom list of high freq/low importance words)  </li> <li>Filter too\u2011common terms with <code>max_df</code> (e.g., drop tokens appearing in &gt;85% of docs) and too\u2011rare with <code>min_df</code> (e.g., keep tokens in \u22652 docs)</li> </ul> <p>This makes <code>docs/search_index.json</code> tiny and fast(er) for lookup/searches.</p>"},{"location":"dev-data_source_optimisation/#why-hnsw-now-ivfpq-later","title":"Why HNSW now, IVF\u2011PQ later?","text":"<ul> <li>HNSW gives near\u2011exact recall with simple knobs (<code>M</code>, <code>efConstruction</code>, <code>efSearch</code>), great for up to a few hundred thousand vectors on a single machine.</li> <li>IVF\u2011PQ compresses vectors and partitions the space. It is useful when you need smaller indexes and faster queries at scale. We trade some/a little recall, but can dial it back with <code>nprobe</code> and codebook size.</li> </ul> <p>Switching rule of thumb: if chunks exceed ~200-250k or HNSW file becomes problematic (&gt;~300 MB), toggle IVF\u2011PQ.</p>"},{"location":"dev-data_source_optimisation/#interoperability-reuse","title":"Interoperability &amp; reuse","text":"<ul> <li>Parquet is easy to read from Python, R, Spark, DuckDB, Polars, etc.</li> <li>FAISS is a standard ANN engine; the index can be loaded locally or by a lightweight service.</li> <li>The corpus is self\u2011describing: Parquet schema metadata indicates whether embeddings are <code>float32</code> or quantised (<code>uint8</code> with dequant rule).</li> </ul>"},{"location":"dev-data_source_optimisation/#frequently-asked-but-will-it","title":"Frequently asked \"but will it\u2026?\"","text":"<ul> <li> <p>\u2026find the right stuff in compressed vectors?   Yes-for docs and model, differences are tiny. We've normalised vectors, which preserves cosine structure.</p> </li> <li> <p>\u2026scale to thousands of PDFs?   Yes. The heavier corpus build, is seperate and done externally, size via incremental hashing, hence only compact artefacts. </p> </li> <li> <p>\u2026lock me into specific, or paid 3rd-party|software?   No. We've aimed for an entirely open/Git-centric approach on this + open formats (Parquet, FAISS) + standard Python libs</p> </li> <li> <p>\u2026support OCR/scanned PDFs?   No. But it could. WE just dont see the immeadiate need for this within the sector. Could later add Tesseract or cloud OCR  if needed. Can add later if any colleagues are aware of such docs. </p> </li> </ul>"},{"location":"dev-data_source_optimisation/#key-libraries-and-why-we-chose-them","title":"Key libraries and why we chose them","text":"<ul> <li>PyMuPDF (<code>pymupdf</code>): fast, robust PDF text extraction  </li> <li>Sentence\u2011Transformers: high\u2011quality sentence embeddings + easy APIs  </li> <li>FAISS: full vector search (HNSW, IVF\u2011PQ) with scalable performance  </li> <li>PyArrow/Parquet: portable column based storage; perfect for text + vectors + metadata  </li> <li>scikit\u2011learn: light\u2011weight keyword vocab building with stop\u2011word &amp; frequency filters</li> </ul>"},{"location":"dev-data_source_optimisation/#takeaways","title":"Takeaways","text":"<ul> <li>By separating heavy preprocessing from what's hosted in the main/mkdocs site here, plus by quantising vectors, we massively cut the needed storage and bandwidth without compromising colleagues' experience in the search etc</li> <li>The site\u2019s search is able to be near-instant with smaller footprint, and an underlying corpus remains portable and reusable for future or onward data projects - although we're not yet sure of the use-cases on this.</li> </ul>"},{"location":"dev-data_source_optimisation/#faiss-and-parquet-artefacts-structure-and-reference","title":"FAISS and Parquet artefacts: structure and reference","text":"<p>A bit about how derived files are organised within the project, what's inside Parquet tables, and what the FAISS index stores. </p>"},{"location":"dev-data_source_optimisation/#repo-layout-derived-artefacts","title":"Repo layout (derived artefacts)","text":"<pre><code>artifacts/\n  state.json                        # per-document hashes, build config, last_built\n  chunks/                           # per-document Parquet (text + metadata)\n    1a2b3c4d5e6f7a8b.parquet\n    9c0d1e2f3a4b5c6d.parquet\n    ...\n  vectors/                          # per-document Parquet (text + vectors)\n    1a2b3c4d5e6f7a8b.parquet\n    9c0d1e2f3a4b5c6d.parquet\n    ...\n  motw_chunks.parquet               # combined (optional)\n  motw_vectors.parquet              # combined (optional)\n  motw_index.faiss                  # FAISS HNSW index (default)\n  motw_index_ivfpq.faiss            # FAISS IVF-PQ index (optional)\n</code></pre> <ul> <li><code>doc_id</code> is the first 16 hex of the SHA-256 of the raw file content.</li> <li>Per-document Parquet files enable incremental builds and quick reloads.</li> <li>Combined Parquet files are optional and exist for convenience.</li> </ul>"},{"location":"dev-data_source_optimisation/#parquet-schemas","title":"Parquet schemas","text":""},{"location":"dev-data_source_optimisation/#per-document-chunks-parquet-artifactschunksdoc_idparquet","title":"Per-document chunks Parquet (<code>artifacts/chunks/&lt;doc_id&gt;.parquet</code>)","text":"<p>Schema:</p> <pre><code>doc_id:      string          # 16-char hex id (content address)\nchunk_id:    int32           # 0..N-1 per document\nsource_path: string          # original path on disk\nsource_name: string          # original filename (for example My_File.pdf)\ntext:        string          # chunked, cleaned text\n</code></pre> <p>Example row:</p> doc_id chunk_id source_name text (truncated) 1a2b3c4d5e6f7a8b 0 ADCS_Safeguarding_...pdf Local authorities should..."},{"location":"dev-data_source_optimisation/#per-document-vectors-parquet-artifactsvectorsdoc_idparquet","title":"Per-document vectors Parquet (<code>artifacts/vectors/&lt;doc_id&gt;.parquet</code>)","text":"<p>Same columns as chunks, plus one embedding column:</p> <ul> <li>If stored as float32:   <code>embedding: list&lt;float32&gt;    # length 384</code></li> <li>If stored as uint8 (space-saving default):   <code>embedding_q: list&lt;uint8&gt;    # length 384, quantised</code></li> </ul> <p>Parquet schema metadata (present on the vectors file) indicates storage:</p> <pre><code>motw.embedding.storage = \"float32\"      # or \"uint8_sym\"\nmotw.embedding.dequant  = \"x = (q / 127.5) - 1.0\"\n</code></pre> <ul> <li>For <code>embedding_q</code>, clients can reconstruct an approximate float vector as:</li> <li><code>x = (q / 127.5) - 1.0</code>, then L2-normalise <code>x</code> before cosine similarity.</li> <li>Vectors were normalised during embedding, which keeps cosine behaviour stable after dequantisation.</li> </ul>"},{"location":"dev-data_source_optimisation/#combined-parquet-files-optional","title":"Combined Parquet files (optional)","text":"<p><code>artifacts/motw_chunks.parquet</code> and <code>artifacts/motw_vectors.parquet</code> have the same schemas as above, but hold all docs together. They are convenient for one-shot analysis and for users/LA colleagues that do not want to iterate per-doc files.</p>"},{"location":"dev-data_source_optimisation/#faiss-index-files","title":"FAISS index files","text":"<p>The pipeline can build either:</p>"},{"location":"dev-data_source_optimisation/#hnsw-index-default-artifactsmotw_indexfaiss","title":"HNSW index (default) - <code>artifacts/motw_index.faiss</code>","text":"<p>Stored properties inside the binary:</p> <ul> <li><code>d</code> -&gt; 384 (embedding dimension)</li> <li><code>metric</code> -&gt; inner product (cosine when vectors are normalised)</li> <li><code>ntotal</code> -&gt; number of vectors (equals total chunks)</li> <li>Graph parameters saved with the index:</li> <li><code>M</code> -&gt; graph degree used at build time (for example 32)</li> <li><code>efConstruction</code> -&gt; build breadth (for example 80)</li> <li>Query-time parameter you set after loading:</li> <li><code>efSearch</code> -&gt; search breadth (for example 64 or 128). Higher -&gt; better recall, slower</li> </ul> <p>Notes: - Excellent recall and speed up to a few hundred thousand vectors - File size typically ~1-2 KB per vector, depending on <code>M</code> and data</p>"},{"location":"dev-data_source_optimisation/#ivf-pq-index-optional-artifactsmotw_index_ivfpqfaiss","title":"IVF-PQ index (optional) - <code>artifacts/motw_index_ivfpq.faiss</code>","text":"<p>Stored properties:</p> <ul> <li><code>d</code> -&gt; 384</li> <li><code>metric</code> -&gt; inner product</li> <li><code>ntotal</code> -&gt; number of vectors</li> <li>Coarse quantiser and codebooks:</li> <li><code>nlist</code> -&gt; number of coarse centroids (for example 1024)</li> <li><code>m</code> -&gt; number of subquantisers (for example 16)</li> <li><code>nbits</code> -&gt; bits per subvector (for example 8)</li> <li>Trained centroids and PQ codebooks included in the file</li> <li>Query-time parameter (set after loading):</li> <li><code>nprobe</code> -&gt; how many coarse lists to search (for example 16, 32, 64). Higher -&gt; better recall, slower.</li> </ul> <p>Notes: - Much smaller on disk than HNSW (often tens of MB), with a small recall trade-off - tune via <code>nprobe</code>.</p>"},{"location":"dev-data_source_optimisation/#minimal-load-examples-for-ref","title":"Minimal load examples (for ref)","text":"<pre><code># Load FAISS index\nimport faiss\nindex = faiss.read_index(\"artifacts/motw_index.faiss\")  # or motw_index_ivfpq.faiss\n# For HNSW, can increase recall at query time:\ntry:\n    index.hnsw.efSearch = 64\nexcept AttributeError:\n    pass\n# For IVF-PQ:\ntry:\n    index.nprobe = 32\nexcept AttributeError:\n    pass\n</code></pre> <pre><code># Load vectors from Parquet (handles float32 or uint8 storage)\nimport pyarrow.parquet as pq\nimport numpy as np\n\nt = pq.read_table(\"artifacts/motw_vectors.parquet\")  # or per-document under artifacts/vectors/\ncols = set(t.schema.names)\n\nif \"embedding\" in cols:  # float32\n    embs = np.array([np.array(x) for x in t.column(\"embedding\").to_pylist()], dtype=\"float32\")\nelif \"embedding_q\" in cols:  # uint8\n    q = np.array([np.array(x, dtype=\"uint8\") for x in t.column(\"embedding_q\").to_pylist()], dtype=\"uint8\")\n    x = (q.astype(\"float32\") / 127.5) - 1.0\n    # re-normalise to unit length\n    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-6\n    embs = x / n\nelse:\n    raise ValueError(\"No embedding column found\")\n</code></pre>"},{"location":"dev-data_source_optimisation/#how-these-pieces-fit-the-platform","title":"How these pieces fit the platform","text":"<ul> <li>Parquet carries all chunk text and metadata for analytics, auditing, and lightweight keyword indexing.</li> <li>FAISS provides high-quality semantic retrieval over the same chunks.</li> <li>Storing Parquet embeddings as uint8 keeps long-term storage small while preserving near-identical search behaviour when vectors are dequantised and normalised.</li> <li>HNSW is the default for accuracy and simplicity. IVF-PQ is available when a much smaller index is needed, with tunable recall.</li> </ul>"},{"location":"dev-data_source_optimisation/#glossary","title":"Glossary","text":"<ul> <li>Parquet: a columnar file format for tables. Efficient to store and quick to scan. Works well with Pandas, DuckDB, Spark and friends.</li> <li>FAISS: Facebook AI Similarity Search - a library for fast similarity search over vectors (numerical representations of text). Lets you find \"things like this\" quickly. </li> <li>HNSW: Hierarchical Navigable Small World graph. A very fast index structure used by FAISS for high\u2011quality approximate nearest\u2011neighbour search.</li> <li>Artefacts: the files we produce from processing (e.g. Parquet tables and FAISS indexes) that you can store, publish, or reuse.</li> <li>L2\u2011normalise: scale a vector so its length is 1. This keeps cosine similarity meaningful and makes quantisation behave well.</li> </ul>"},{"location":"dev-index_Pre-AlisonChgs/","title":"CSC Knowledge Base Network","text":"<p>A structured, extensible open-source data-eco-system map and 'knowledge base' for the Children\u2019s Social Care (CSC) sector. This project collates and creates an overview of key sector documentation, project relationships, data services, sector tools, rules, plans and events using a flexible YAML-based data model aiming to align with the Smart City Concept Model (SCCM) towards data interoperability.</p> <p>Alongside the (filtered)graph-based relations visualisation, it aims to support key-term search and YAML schema validation across the structured/human readable <code>.yml</code> structured records. Development is scaffolded/designed to be extensible, transparent, and Git-native.</p> <p>With a focus on ensuring an entirely open-source and Git-native tool, there are some interesting and potentially constraining problems to solve just within the available tech-stack(even before we address the arguably bigger issues of how to bring all the relvant data together, and what's a sustainable internal structure). We're particularly interested in the Git data limits vs CSC sector need on data storage towards such as optimised full text searches. The potential data-estate in this exploratory development, alongside how we might take or manage direct sector input into the YML structure(s) also ongoing factors. </p>"},{"location":"dev-index_Pre-AlisonChgs/#current-dev-phase-discovery-alpha","title":"Current Dev Phase: Discovery-Alpha","text":""},{"location":"dev-index_Pre-AlisonChgs/#what-is-this-for","title":"What is this for?","text":"<p>This proof of concept(PoC) project explores how a structured, searchable map of Children\u2019s Social Care (CSC) data, tools, data projects, key frameworks, statuatoary guidance and activity could enable collaboration and support or optimise efforts across the sector. While exact use-cases are still emerging, our goal is to create a shared resource that brings together:</p> <ul> <li>published reports  </li> <li>pre-defined data objects  </li> <li>web data (e.g. from the DfE, local authorities, third parties)  </li> <li>sector-developed tools and frameworks</li> <li>connected people (organisational/sector tools linked where consent given or public record)  </li> </ul> <p>All of this would be made accessible through a visual or navigable interface, allowing users to explore connections between people, projects, standards, and services.</p> <p>We think this could help:</p> <ul> <li>Make relationships clearer \u2014 between local and national CSC initiatives, policies, systems, and data sources  </li> <li>Show who\u2019s doing what \u2014 helping users track new tools, updates to frameworks, or structural changes in services  </li> <li>Bring siloed or under-the-radar work into view \u2014 so efforts can align, build on each other, or avoid duplication  </li> <li>Support local teams \u2014 by contributing to a more joined-up picture of activity across the sector  </li> </ul> <p>We envisage use-cases from:</p> <ul> <li>Local authority data and performance teams</li> <li>Children\u2019s social care service managers and strategic leads</li> <li>Academic researchers and national analysts</li> <li>Project leads, developers and architects working in CSC data or digital delivery</li> </ul> <p>We see this as a collaborative mapping tool, developed potentially with input from local authority teams, analysts, service leads, academic partners, and national bodies.</p>"},{"location":"dev-index_Pre-AlisonChgs/#plan","title":"Plan","text":"<ul> <li>Interactive network map: Navigate to Network to view entities, relationships, and systems as a live graph</li> <li>Structured data records: Underpinning the map is a growing library of structured YAML records, aligned to a SCCM concept framework(BSI as PAS 182) that describe:</li> <li>Tools and systems (e.g. PATCH, Validator 903)</li> <li>Frameworks and inspections (e.g. Ofsted ILACS, JTAI)</li> <li>Relationships and service models</li> <li>Rules, plans, events and guidance</li> <li>Searchable resource: The search page enables you to explore the structured data model directly.</li> <li>This is separate from the standard MkDocs search (top-right), which only covers page text within this site.</li> <li>The CSC knowledge search indexes structured YAML content as well as <code>.md</code>, <code>.pdf</code>, <code>.py</code>, <code>.js</code> and <code>.html</code> files, and supports keyword relevance, match scoring, and metadata extraction.</li> <li>(in dev)The search index|scope currently takes a data sample direct from local authority web sites. At the moment this is throttled to ~10, but with the potential to extract simplistic reference resource(s) directly from all ~153</li> <li>(in dev)The search index|scope aims to scrape from relevant CSC public data sources in order to increase the tool's search scope. This could schedule indexing of relevant documents or data sources from defined .gov or .edu sites.   </li> <li>Documentation hub: Local documentation from D2I projects(Git repos) is also live-indexed to provide technical context</li> </ul>"},{"location":"dev-index_Pre-AlisonChgs/#how-is-this-structured","title":"How is this structured?","text":"<p>The aim was to align records in this tool with the Smart City Concept Model (SCCM), an open framework for describing public service ecosystems. Every entity towards the documented network(diagram) is represented as a YAML file, defined at the top level via SCCM concept types. Note: This does mean that for LA colleagues some language/grammer use might appear disconnected, incl. minor such as ORGANZATION vs ORGANISATION :</p> <p>e.g. - <code>@type: AGENT</code> \u2013 people, teams, or organisations - <code>@type: SERVICE</code> \u2013 a system, service or tool - <code>@type: EVENT</code> \u2013 events such as inspections, launches, reviews - <code>@type: RULE</code>, <code>@type: PLAN</code>, <code>@type: COLLECTION</code> \u2013 policy elements, datasets or strategies - <code>@type: RELATIONSHIP</code> \u2013 links between entities (e.g. oversight, supply, influence)</p> <p>YAML files are validated, searchable, and designed to be easier to contribute to as they're more human readable than other structured data forms (e.g. JSON, CSV... )</p> <p>Note: Further SCCM allignment examples under Possible SCCM Mapping to CSC Eco-System</p>"},{"location":"dev-index_Pre-AlisonChgs/#how-can-i-get-involved","title":"How can I get involved?","text":"<p>This project is being developed with and for the sector. We welcome:</p> <ul> <li>Feedback and suggestions on what\u2019s useful or missing(or broken)</li> <li>Contributions of local projects or documentation</li> <li>Ideas for how the tool could better support the sector</li> </ul> <p>To contribute or get involved, please contact the Data to Insight team or fork from/visit the GitHub repo.</p>"},{"location":"dev-index_Pre-AlisonChgs/#foundations-and-inspiration","title":"Foundations and Inspiration","text":"<p>This tool builds on the thinking behind platforms like the Children\u2019s Services Network and grounded in open modelling approaches like the already mentioned Smart City Concept Model.</p> <p>It is designed to be lightweight, transparent, and openly extensible \u2014 enabling others to adopt or adapt it for their own contexts.</p>"},{"location":"dev-index_Pre-AlisonChgs/#soft-systems-conceptual-mapping","title":"Soft Systems Conceptual Mapping","text":"<p>System of Interest </p> <p>Shared|public data platform and ecosystem used within Children\u2019s Social Care to connect people|LA colleagues, data, tools, and services</p> <p>Purpose </p> <p>To enable shared sector understanding, validation, discovery, and collaboration between local authorities, tool development, projects and other initiatives in CSC</p> <p>Worldview (Weltanschauung) </p> <p>Fragmented data landscapes transformed into a collaborative, open ecosystem using lightweight, transparent structures like SCCM, JSON, YAML + MkDocs</p> <p>Owner(s) </p> <p>Likely data platform stewards: D2I, local authority data teams, ecosystem developers</p> <p>Environment (External Constraints) </p> <p>GitHub Pages (no backend), data security and ethics, evolving standards, distributed maintenance, changing ecosystem, frameworks and statuatory guidance, browser-only deployments</p>"},{"location":"dev-index_Pre-AlisonChgs/#whats-next","title":"What\u2019s next?","text":"<ul> <li>Ongoing expansion of linked tools, rules, and frameworks</li> <li>Live|scheduled scrapes from key web resources or published docs/framesworks</li> <li>Search and filter interface (in beta, but aiming to implement network diagram filters)</li> <li>Option for local teams to submit structured entries or link live repositories</li> <li>Export options for integration into other data tools</li> </ul> <p>Thanks for the interest in CSC Knowledge Base Network We hope it supports your work, and welcome your feedback as we continue to improve and expand it.</p>"},{"location":"dev-index_Pre-AlisonChgs/#possible-sccm-mapping-to-csc-eco-system","title":"Possible SCCM Mapping to CSC Eco-System","text":"SCCM Concept (Category) Suggested example(s) (in progress) Community South East fostering cluster Documentation CSC Independent Review Events Children\u2019s Social Care Review, ILACS Inspections, Public Inquiries Organization Data to Insight, LIIA Persons Organisational/sector tools linked where consent given or public record Plans Kinship Care Strategy, Children\u2019s Social Care National Framework Relationships LA-1 \u2194 Supports \u2194 SSD Tests, DfE \u2194 Pilots \u2194 API Data Flows Rules Statutory Guidance, Keeping Children Safe in Education 2025 Sector Tools PATCH, ChAT Services"},{"location":"dev-istand_submission/","title":"CSC Map of the World Award submission","text":"<p>Notes towards a possible Jan 2026-27 iStand project submission. </p>"},{"location":"dev-istand_submission/#name-of-the-initiative-or-project","title":"Name of the initiative or project","text":"<p>Children's Social Care Map of the World (CSC Map of the World)</p>"},{"location":"dev-istand_submission/#briefly-describe-the-initiative-including-aims-and-objectives","title":"Briefly describe the initiative, including aims and objectives","text":"<p>Children's Social Care Map of the World is an open source knowledge base and network map for Children's Social Care. Offering the potential for a single, structured view of key CSC documents, tools, services, guidance, projects, people and data services that are currently spread across many unconnected sources, reports and key individuals within the sector.</p> <p>The project builds|visualises a graph of related entities and relationships that represents the children's services data ecosystem at scale. Each node in that network, e.g an organisation, service, event, framework, plan, rule, dataset or relationship, is defined as a human readable YAML file aligned with the SAVVI Smart City Concept Model (SCCM, BSI PAS 182). This gives a clear, shared language for describing public service activity and makes the underlying model easy to reuse and extend in other contexts.</p> <p>Underneath, and in combination with the graph is a compact, optimised corpus that combines content from multiple Children's Social Care source types. This includes Git repositories and technical documentation, published reports and statutory guidance, sector tools and frameworks, direct contributions from local teams and carefully scoped web scrapes of CSC relevant sites. Text is extracted once, split into coherent chunks, embedded as numerical vectors, then stored in standard Parquet tables and FAISS indexes. The result is a focused, ring fenced search resource that can be used online or offline without shipping large original PDFs or Word files.</p> <p>Because these artefacts are small and self contained, the main web platform runs entirely as a static GitHub Pages site with no database and no proprietary backend. Everything is held inside Git in open formats. Any local authority or sector stakeholder can clone the repository, start from the shared corpus, add their own documents and stand up a localised Map of the World using standard open source tools.</p> <p>In Discovery|Alpha phase the initiative has three initial objectives.</p> <ul> <li>Provide an up to date picture of who is doing what in Children's Social Care data and digital, including visible and hidden connections between workstreams, people, frameworks and tools.</li> <li>Reduce reliance on tacit knowledge by turning informal awareness, for example who knows a similar project elsewhere, into an explicit, navigable map of relationships, tags and shared contributors.</li> <li>Prove that a sector wide systems model and search corpus can be held in extensible YAML and compact artefacts in a way that is practical to adopt, adapt and extend by local authorities and partners.</li> </ul> <p>The work supports digital innovation, data and collaboration in public services by treating the map and corpus as shared infrastructure that other tools and projects can build on.</p>"},{"location":"dev-istand_submission/#what-are-the-key-achievements","title":"What are the key achievements","text":"<p>CSC Map of the World has delivered concrete technical and sector achievements, despite being early in its lifecycle.</p> <ul> <li>A live public site shows Children's Social Care organisations, plans, events, frameworks, rules and services as an interactive network. Users can filter by concept type, search by name, tag or free text and keep neighbouring nodes visible to see context. This moves the sector from one off diagrams and slide packs to a shared, living systems view that anyone can revisit.</li> <li>A robust SCCM based schema has been implemented in practice. Every element is stored as a YAML file with a clear <code>@type</code> such as AGENT, SERVICE, EVENT, RULE, PLAN, COLLECTION or RELATIONSHIP. This has already been used to describe national frameworks, inspections, tools and local initiatives, demonstrating that a single concept model can stretch across a diverse ecosystem while remaining understandable and maintainable.</li> <li>A reusable corpus and search layer has been built and documented. The Map of the World corpus is packaged as a small set of artefacts, for example Parquet tables of chunked text and metadata, quantised vector tables and FAISS indexes. Quantisation reduces vector storage by roughly a factor of four without a noticeable impact on search quality. Local authorities can start from this shared pack, add their own PDFs, Word documents or text files, and append to the index. Unchanged documents are skipped automatically using content hashes, so builds are incremental and repeatable.</li> <li>Guidance has been published for local reuse. A short, practical note explains how local colleagues can reuse the corpus, what each file does and how to add their own documents without rebuilding everything. This lowers the skills barrier, because teams do not need to understand FAISS or vector maths to benefit from the work.</li> <li>The project has created sector facing assets around the prototype, including an infographic that explains the concept, a soft systems map of the system of interest and a submission route that lets others propose new nodes and relationships using the same YAML schema.</li> </ul> <p>Key challenges and how they were addressed.</p> <ul> <li>Fragmented information. The initiative tackled this by agreeing a single schema and map for the sector and by indexing material from multiple types of source, for example Git repos, guidance PDFs and web content from an initial sample of local authority sites.</li> <li>Hosting constraints. The decision to run entirely on GitHub Pages, with no database, was turned into a design strength by separating heavy preprocessing from the public site and by using compact artefacts only.</li> <li>Storage and performance. The team introduced chunking, quantisation and incremental builds so that search quality remains high while storage and runtime stay manageable on standard hardware.</li> <li>Skills and capacity. Plain English reuse guidance and simple contribution routes allow analysts and service colleagues to add value without needing to become machine learning specialists.</li> </ul> <p>Early impact is already visible in how colleagues understand the landscape. They can see projects, people and resources that share tags, themes or contributors, rather than relying on chance conversations. This reduces duplicate effort, highlights natural collaborations and makes it easier to spot gaps. As more nodes and relationships are added, the same infrastructure can support more systematic efficiency gains by becoming the reference layer that dashboards, benchmarks or research projects draw on.</p>"},{"location":"dev-istand_submission/#how-innovative-is-your-initiative","title":"How innovative is your initiative","text":"<p>We believe that this is a ground breaking initiative for Children's Social Care and for public service data more broadly, in both what it models and how it is delivered. Previous similar work in this area have created visual sector mapping, but have never been able to provide the supporting and more useful meta-data layers, nor offer an underlying full search option. These crucial elements shift beyond just any visual representation.  transformative as a valuable tool </p> <ol> <li> <p>System wide modelling rather than isolated projects.    By using SCCM aligned YAML for every organisation, service, framework, rule, plan and relationship, the project provides a single, extensible way to model public service effort, resources, key people and technology stacks. That model can be forked and adapted by others, rather than being locked inside proprietary software or static reports.</p> </li> <li> <p>A focused corpus rather than generic web or AI search.    The Map of the World corpus is tightly scoped to Children's Social Care and related domains, with explicit source boundaries and licensing metadata. It does not try to index everything, it focuses on authoritative, sector relevant material. Content is chunked and embedded with modern sentence level models, then stored in compact Parquet and FAISS artefacts. This gives fast, targeted retrieval that respects sector framing and context. It also allows offline and local use, for example inside a secure council environment, which general search engines and most AI tools cannot offer.</p> </li> <li> <p>Advanced methods packaged for realistic local use.    The technical pipeline uses methods usually associated with large platforms, for example content hashing, uint8 quantisation and approximate nearest neighbour search, but packages them so that local teams can actually adopt them. The heavy processing happens once, upstream or locally, and the public site only ships small JSON and YAML files. This keeps the footprint low while still supporting rich search and visualisation.</p> </li> <li> <p>Git native and vendor neutral from the start.    The data model, documentation, configuration and derived artefacts live in public repositories, with clear folder structures and manifest files. Anyone can inspect the history of changes, fork the project or run their own pipelines. Upstream contributions from open source developers improve the core tools and schema. Downstream, local authorities can add their own documents and nodes, or export the corpus to underpin their own dashboards, reports or internal tools.</p> </li> <li> <p>Foundations for future sector specific AI.    Because the corpus is curated, ring fenced and well described, it could form part of a domain specific retrieval layer or training set for future Children's Social Care language models, subject to governance and ethics. The current focus is on retrieval, mapping and navigation rather than on building an LLM, but the foundations mean that, if the sector chooses to explore that route, the right building blocks are already in place.</p> </li> </ol>"},{"location":"dev-istand_submission/#what-are-the-key-learning-points","title":"What are the key learning points","text":"<p>Several clear learning points have emerged that are relevant beyond this project.</p> <ul> <li>Designing for static, open infrastructure has been a positive constraint. Knowing that the public site must run with no server side code and no database forced disciplined decisions about data structures, file sizes and indexing. The outcome is a system that is simple to host and easy to mirror, which is important for resilience and for low resource environments.</li> <li>Conceptual clarity needs to come early. Explicitly using SCCM concepts such as AGENT, SERVICE, EVENT, PLAN, RULE, COLLECTION and RELATIONSHIP prevented a lot of downstream confusion. It also highlighted where Children's Social Care specific ideas stretch the base model, giving useful feedback for future standards work.</li> <li>Incremental, artefact based design has proved essential. By identifying documents with hashes and recording build state in small manifest files, the pipeline can skip unchanged material and only process what is new. This keeps runtimes manageable and makes it realistic for local teams to reuse the shared pack. It also makes it easier to maintain a clear audit trail from original documents to derived artefacts.</li> <li>Governance and scope need as much attention as technology. Even when working with public content, there are important questions about what to index, how often to update and how to communicate the purpose of the map. The project has deliberately started with a limited sample and a focus on reference style resources, with a view to co designing wider roll out and governance with the sector.</li> <li>There is strong value in making tacit knowledge explicit. Many of the most useful connections in Children's Social Care exist in people's heads, for example who worked on a previous pilot, where a particular tool is already in use or which framework overlaps with a new initiative. Mapping entities, tags and shared contributors into a searchable graph makes that knowledge visible to others and less dependent on individual memory.</li> </ul> <p>In terms of replicability and scale, the pattern is straightforward to copy. Any domain that can be described using SCCM like concepts and stored as YAML files can adopt a similar approach. Others can reuse both the schema and the corpus pipeline. The technology stack is standard and low cost, so the main work for new adopters is curation and governance, not software build.</p>"},{"location":"dev-istand_submission/#additional-comments","title":"Additional comments","text":"<p>CSC Map of the World builds on and connects to a wider set of Data to Insight projects that support data quality, pipelines and analytics in Children's Social Care. This provides an immediate community of practice, shared infrastructure and potential contributors.</p> <p>The project has put effort into communication as well as code. The infographic, soft systems map and reuse notes are written in plain English and are aimed at senior leaders, practitioners and analysts, not only at developers. This has already made it easier to have practical conversations about where the map can help, what it should and should not include and how local teams might feed into it.</p> <p>Most importantly, the initiative is explicitly positioned as with and for the sector. It is an evolving shared platform rather than a finished product. As more local authorities and partners contribute nodes, relationships and documents, the map will become a richer picture of Children's Social Care activity, and the corpus will become a more powerful shared search resource. Because it is open, portable and built on standard formats, the value does not depend on a single supplier or team. It can grow, branch and be reused in many different ways over time.</p> <p>In summary, the key strengths of CSC Map of the World are.</p> <ul> <li>Impact, turning fragmented and tacit knowledge into a single, navigable map and search corpus that supports better decisions and reduces duplicate effort.</li> <li>Innovation, combining SCCM aligned modelling, compact search artefacts and static hosting to deliver something that is new in this sector yet realistic for local authorities to adopt.</li> <li>Scalability and replicability, using open formats and Git native workflows so that any interested authority or partner can reuse the approach, extend the corpus and adapt the map to their own context.</li> </ul>"},{"location":"dev-motw-reuse-guide-local-authorities/","title":"Reusing -Map of the World- corpus locally","text":"<p>A short guide for local authority colleagues</p> <p>Explains how you can start from the shared corpus and add your own documents without rebuilding everything. It also lists the files the (pre-processing)Jupyter notebook creates and where they live when running locally.</p>"},{"location":"dev-motw-reuse-guide-local-authorities/#what-lasdata-colleagues-can-do","title":"What LAs/data colleagues can do","text":"<ul> <li>Start from the shared corpus - and add your own PDFs, Word documents, or text files  </li> <li>Append only what is new - Unchanged documents are skipped automatically  </li> <li>Keep storage small - we store compact Parquet tables and a FAISS index, not the original PDFs  </li> <li>Reuse the corpus - in your own analysis tools. Parquet and FAISS are standard, portable formats</li> </ul>"},{"location":"dev-motw-reuse-guide-local-authorities/#what-is-in-the-shared-pack","title":"What is in the shared pack","text":"<p>Place these in your project folder:</p> <pre><code>csc_artifacts/\n  state.json                 # records which documents were processed and with which settings\n  chunks/                    # per-document Parquet of chunked text + metadata\n  vectors/                   # per-document Parquet of vectors (uint8 by default)\n  motw_index.faiss           # FAISS index (HNSW) built over all vectors\n# optional\n  motw_chunks.parquet        # combined text+meta (only if materialised)\n  motw_vectors.parquet       # combined text+vectors (only if materialised)\n</code></pre> <p>You provide your own documents into:</p> <pre><code>data_published/              # put your PDFs, .docx, or .txt here\n</code></pre> <p>We avoid storing the PDFs in Git (inconsistent formatting and heavier file sizes). Keep them local or in object storage if you need an archive.</p>"},{"location":"dev-motw-reuse-guide-local-authorities/#what-the-notebook-creates","title":"What the notebook creates","text":"<p>When you run the notebook, it will create or update:</p> <ul> <li><code>csc_artifacts/state.json</code> - manifest with hashes, sizes, chunk counts, and build settings  </li> <li><code>csc_artifacts/chunks/&lt;doc_id&gt;.parquet</code> - chunked text and metadata for each document  </li> <li><code>csc_artifacts/vectors/&lt;doc_id&gt;.parquet</code> - the matching vectors for each document. Stored as <code>embedding_q</code> (uint8) + note on how to dequantise  </li> <li><code>csc_artifacts/motw_index.faiss</code> - the FAISS search index  </li> <li>Optionally, <code>csc_artifacts/motw_chunks.parquet</code> and <code>csc_artifacts/motw_vectors.parquet</code> if you turn on materialisation</li> </ul> <p>Each document is identified by the first 16 hex chars of its SHA-256 hash. This keeps the build incremental and avoids duplicates.</p>"},{"location":"dev-motw-reuse-guide-local-authorities/#two-main-ways-to-run","title":"Two main ways to run","text":""},{"location":"dev-motw-reuse-guide-local-authorities/#1fresh-build","title":"1.Fresh build","text":"<p>Use when you are starting from scratch or want a clean rebuild.</p> <ul> <li>The notebook scans <code>data_published/</code>, extracts text, chunks it, embeds it, writes per-document Parquet, and builds a fresh FAISS index.  </li> <li>It writes a new <code>state.json</code> so the next/future runs are incremental</li> </ul>"},{"location":"dev-motw-reuse-guide-local-authorities/#2append-your-new-files","title":"2.Append your new files","text":"<p>Use when you are starting from the shared pack and want to add your own documents.</p> <ul> <li>Put your documents into <code>data_published/</code> </li> <li>The notebook compares hashes against <code>state.json</code>. Unchanged documents are skipped  </li> <li>New documents are processed and appended to the existing FAISS index  </li> <li>If any existing document has changed, the notebook rebuilds the index from the per-document Parquet to keep it correct</li> </ul>"},{"location":"dev-motw-reuse-guide-local-authorities/#other-options","title":"Other options","text":"<ul> <li>Rebuild from Parquet only - if you have the per-document Parquet but no FAISS file, the notebook can rebuild the index without touching PDFs  </li> <li>Materialise a single-file Parquet - if you prefer a single file, turn on materialisation to write <code>motw_chunks.parquet</code> and <code>motw_vectors.parquet</code>. This is optional because the per-document layout already supports incremental updates  </li> <li>Upload to object storage - if you set the Cloudflare R2 variables, the notebook can upload updated artefacts after a run</li> </ul>"},{"location":"dev-motw-reuse-guide-local-authorities/#changes-and-deletions","title":"Changes and deletions","text":"<ul> <li>Changed documents - if the content of a document changes, its hash changes. The notebook reprocesses that document and rebuilds the FAISS index so your search stays correct  </li> <li>Deleted documents - remove the matching files in <code>csc_artifacts/chunks/</code> and <code>csc_artifacts/vectors/</code> and delete the entry in <code>state.json</code>, then run the notebook to rebuild the FAISS index</li> </ul>"},{"location":"dev-motw-reuse-guide-local-authorities/#did-it-work","title":"Did it work?","text":"<p>You should see a summary like:</p> <pre><code>Docs: 120  Read errors: 0  Empty: 0\nChars: 14,200,000  Chunks: 95,400\n[Embedding] 00:02:15s\nWrite/Update FAISS...\nrows (meta, vectors, index): 95,400, 95,400, 95,400\nDone.\n</code></pre> <ul> <li>The counts for meta, vectors, and the FAISS index should match  </li> <li>If you only added new files, you should see a note about appending to the index rather than rebuilding</li> </ul>"},{"location":"dev-motw-reuse-guide-local-authorities/#why-this-might-enable-your-las-data-work","title":"Why this might enable your LA's data work","text":"<ul> <li>Standard formats - Parquet can be read by Pandas, Polars, DuckDB, Spark and more. FAISS is a common vector index  </li> <li>Local-first - you can work entirely on your machine. No need to upload PDFs  </li> <li>Incremental by design - starting from the shared pack means you only build what is new, then carry on where we left off</li> </ul>"},{"location":"dev-recap_dev_diary/","title":"MapOfTheWorld development log v4.2","text":"<p>Short notes so others can see journey. Focus on pivots and trade offs. Static GitHub Pages, no server. Graph must load fast.</p>"},{"location":"dev-recap_dev_diary/#2026-01-14","title":"2026-01-14","text":"<ul> <li>Rethinking what data points 'should' be included in the yml files. Need to establish baselines here - forum collab/collectively?  </li> <li>Some further work adding primarily new org objects, with some connected relations (JB)  </li> <li>Fixed non-disappearing/persistent info panel on full-network page  </li> <li>centralised the info panel formatting... trying to remove some of the inconsistencies seen between the 3 instances of this  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-12-02","title":"2025-12-02","text":"<ul> <li>Back on project after long gap, many log entries missing  </li> <li>Fixed explorer search not firing  </li> <li>Stopped explorer showing full results list after selection, now list hides once node picked  </li> <li>Explorer info panel now pulls full data from <code>node_details.json</code>, not just minimal fields  </li> <li>Fixed duplicated edges when pressing add 1 hop repeatedly, now track bidirectional edge index  </li> <li>Added 2 hops option and made explorer buttons more visible with extra CSS  </li> <li>Alison refreshed intro text and infographic for landing page  </li> <li>Added content to <code>data_source_optimisation.md</code> </li> <li>Realised back end flow now fuzzy in head, note to write short idiot guide for self, including external notebook, <code>search_index.json</code>, <code>state.json</code>, <code>csc_artifacts</code>, <code>docs/data</code>, parquet and faiss refresh  </li> <li>Timed <code>network_fullscreen.md</code> graph load, roughly 50 seconds off peak, plan small warning on page  </li> <li>Updated <code>setup.sh</code> and <code>devcontainer.json</code> so Python and <code>mkdocs serve</code> work in Codespaces without manual venv setup  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-11-28","title":"2025-11-28","text":"<ul> <li>Converted public intro copy for MapOfTheWorld into markdown  </li> <li>Added image include for infographic on main project page  </li> <li>Committed staged docs changes from feature branch into <code>main</code> </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-11-overview","title":"2025-11 overview","text":"<ul> <li>Reviewed JSON size and limits, keep only minified JSON in <code>docs/data</code> for deploy  </li> <li>Keep pretty JSON locally for debugging and diffs  </li> <li>Standardised base URL safe fetch pattern using <code>new URL(relPath, document.baseURI)</code> </li> <li>Documented split JSON contract and local rebuild steps for contributors  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-10-overview","title":"2025-10 overview","text":"<ul> <li>Reached v4.2, search and graph load feel stable, payload trimmed  </li> <li>Removed client side GitHub API calls for feedback, now ship <code>docs/data/feedback-items.json</code> </li> <li>Locked label policy. Edge labels hidden until zoom threshold, above roughly 5k nodes labels at load too risky  </li> <li>Added preload hook so demo pages can embed <code>window.MOTW.graphStd</code>, default still fetch from <code>docs/data</code> for caching  </li> <li>Tweaked scraper filenames, stripped stopwords from stems, optional <code>-u&lt;hash&gt;</code> for collision control, no double underscores, avoid noisy topic prefixes  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-09-overview","title":"2025-09 overview","text":"<ul> <li>Adopted split JSON pattern  </li> <li><code>docs/data/graph_data.json</code> minimal nodes and edges for Cytoscape  </li> <li><code>docs/data/crosswalk.json</code> ids, slugs, page URLs  </li> <li><code>docs/data/degree.json</code> for seeded staged load and ordering  </li> <li><code>docs/data/related_nodes.json</code> optional suggestions  </li> <li><code>docs/data/search_index.json</code> compact index for site search  </li> <li>Kept node fields tight, mainly <code>slug</code>, <code>tags</code>, <code>summary</code>, <code>search_blob</code> </li> <li>Added Choices.js type filter and context mode checkbox to keep neighbours visible while filtering  </li> <li>Moved heavy processing out of GitHub Actions, now build JSON locally in Python then commit, Pages only serves static files  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-08-overview","title":"2025-08 overview","text":"<ul> <li>Switched fully to SCCM aligned YAML as source of truth  </li> <li>Types include ORGANIZATION, SERVICE, EVENT, PLAN, RULE, RESOURCE, PERSON, RELATIONSHIP  </li> <li>Wrote Python builders to emit small artefacts only  </li> <li><code>docs/data/graph_data.json</code> for render  </li> <li><code>docs/data/crosswalk.json</code> for lookups  </li> <li>Pretty versus minified toggle via <code>GRAPH_MINIFY</code> </li> <li>Replaced hover card with side panel version 2, lighter DOM work, clearer field bag  </li> <li>Improved search, free text plus <code>tag:</code> and <code>type:</code> operators, debounced input, URL state persisted  </li> <li>Edges start in haystack style for speed, switch to bezier when zoomed in  </li> <li>Edge labels off by default, show only after useful zoom, labels at load killed frame rate  </li> <li>Isolated degree 0 nodes placed in compact grid along bottom, keeps layout stable  </li> <li>Staged loader, show high degree organisations first, then stream more nodes and edges in batches, status chip shows progress  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-08-06","title":"2025-08-06","text":"<ul> <li>Revisited SAVVI model and checked fit against SCCM  </li> <li>Decided to stay with SCCM for MapOfTheWorld  </li> <li>Scoped dynamic filter for RELATIONSHIP Source field by source type, planned helper script change  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-08-05","title":"2025-08-05","text":""},{"location":"dev-recap_dev_diary/#input-form-logic-with-yaml-export","title":"Input form logic with YAML export","text":"<ul> <li>Finalised static HTML form for new MapOfTheWorld entries  </li> <li>Form supports <code>@type</code> values from SCCM model, including ORGANIZATION, SERVICE, EVENT, PLAN, COLLECTION, PERSON, RESOURCE, RELATIONSHIP  </li> <li>Each type has its own field section, shown or hidden when type changes  </li> <li>RELATIONSHIP block includes source type dropdown so <code>relationship_type</code> options filter correctly  </li> <li>Added grey placeholder examples for multi value fields, for example collaborators, related entities, interests, delivery partners  </li> <li>Placeholders stripped from YAML output if unchanged  </li> <li>On <code>@type</code> change, YAML output box cleared so entries do not bleed across  </li> </ul>"},{"location":"dev-recap_dev_diary/#implementation-notes","title":"Implementation notes","text":"<ul> <li>Form uses plain HTML, CSS and vanilla JavaScript plus <code>js-yaml</code> for YAML serialisation  </li> <li>Served directly from repo, fully static, no build step, matches GitHub Pages model  </li> <li>YAML output fits Git based workflow, contributors can download file or later raise pull request  </li> </ul>"},{"location":"dev-recap_dev_diary/#problems-encountered","title":"Problems encountered","text":"<ul> <li>Type specific sections initially not appearing, only ORGANIZATION and RELATIONSHIP worked, fixed by applying <code>toggleTypeSections()</code> on load and on change  </li> <li>Generate YAML button originally acted like form submit and cleared content, fixed by preventing default submit behaviour  </li> <li>Some type specific fields missing from YAML, reintroduced full per type mapping when building object  </li> <li>Placeholder values leaked into YAML, fixed with <code>cleanList()</code> and <code>cleanTextList()</code> utilities to strip placeholder text  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-08-01","title":"2025-08-01","text":""},{"location":"dev-recap_dev_diary/#mailto-links-devlog-structure-public-dev-log","title":"Mailto links, devlog structure, public dev log","text":"<ul> <li>Revised mailto link for suggest improvement or fix, easier for direct feedback  </li> <li>Defined structure for this dev log, hybrid of dev log and changelog  </li> <li>Skipped tags and automation for now, markdown fast enough to edit by hand  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-07-overview","title":"2025-07 overview","text":"<ul> <li>Set aim, searchable visual map of people, projects, rules, services, hosted as static site  </li> <li>Early build used single large JSON with everything, worked up to roughly two thousand nodes then stalled, memory churn and long time to first paint  </li> <li>Chose SCCM aligned YAML for core data, decided against Parquet on Pages, keep Parquet and faiss local only  </li> <li>Submission form matured, type specific fields and RELATIONSHIP block with dynamic options  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-07-31","title":"2025-07-31","text":""},{"location":"dev-recap_dev_diary/#cytoscape-class-filters-and-stlite-graph-fixes","title":"Cytoscape class filters and stlite graph fixes","text":"<ul> <li>Found bug where class based filters did not apply on page load  </li> <li>Root cause, class assignment lagged layout render, simple <code>setTimeout</code> after layout fixed it  </li> <li>Fixed mismatch in hard coded colours and class names in legend, ORGANIZATION nodes no longer show grey due to casing drift  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-07-30","title":"2025-07-30","text":""},{"location":"dev-recap_dev_diary/#static-legend-and-sources-listing","title":"Static legend and sources listing","text":"<ul> <li>Dynamic legend proved fragile when class names drifted from type names  </li> <li>Rebuilt legend as static block with known entity types such as ORGANIZATION, SERVICE, PERSON with fixed colours  </li> <li>Filter defaults now apply on load and legend starts collapsed  </li> <li>Designed Python script to scan <code>data_yml</code> and other folders and build <code>docs/sources.md</code>, table per source type, ready for later refinement  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-07-29","title":"2025-07-29","text":""},{"location":"dev-recap_dev_diary/#yaml-errors-and-edge-failures-in-cytoscape-build","title":"YAML errors and edge failures in Cytoscape build","text":"<ul> <li>Hit YAML error during <code>admin-build_cytoscape_json.py</code>, single malformed file broke whole build  </li> <li>Noticed edges skipped because of missing nodes, traced to ID naming mismatches  </li> <li>Rebuilt broken relationship YAML from working template, fixed missing edges  </li> <li>Reinforced need for stricter YAML validation around relationships  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-07-22-to-2025-07-26","title":"2025-07-22 to 2025-07-26","text":""},{"location":"dev-recap_dev_diary/#external-data-inclusion-and-web-scraping","title":"External data inclusion and web scraping","text":"<ul> <li>Decided to include external web content, for example PDFs, guidance, reports  </li> <li>Designed approach to store scraped artefacts under <code>data_web/</code> with YAML metadata for indexing  </li> <li>Early blockers, inconsistent text extraction from some GOV.UK PDFs and odd header parsing in <code>.txt</code> and <code>.pdf</code> sources  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-07-19","title":"2025-07-19","text":""},{"location":"dev-recap_dev_diary/#pivot-from-stlite-to-mkdocs-with-custom-js","title":"Pivot from stlite to MkDocs with custom JS","text":"<ul> <li>Tried several iterations of interactive graphs in stlite, repeated issues with modules and layout timing  </li> <li>Cytoscape.js often failed to load or broke quietly because of Pyodide and import limits  </li> <li>stlite felt too fragile for graph exploration with filters, tooltips, legend controls  </li> <li>Decided to move to MkDocs as primary frontend and embed custom HTML plus JavaScript components instead  </li> <li>Static MkDocs base gave better search integration, reusable components and cleaner GitHub Pages hosting  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-07-18","title":"2025-07-18","text":""},{"location":"dev-recap_dev_diary/#migration-to-stlite-for-browser-hosting","title":"Migration to stlite for browser hosting","text":"<ul> <li>Earlier step shifted from standard Streamlit to stlite so app could run in browser on GitHub Pages  </li> <li>Removed unsupported modules such as <code>pyvis</code>, <code>Path(__file__)</code> usage and Parquet I/O  </li> <li>Rebuilt visualisation to use JSON plus embedded Cytoscape.js in HTML  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-07-17","title":"2025-07-17","text":""},{"location":"dev-recap_dev_diary/#parquet-removal-and-browser-based-frontend","title":"Parquet removal and browser based frontend","text":"<ul> <li>Adapted original Streamlit app to run fully in browser  </li> <li>Dropped Parquet loading because of browser limitations  </li> <li>Replaced with pre generated JSON at <code>data/index_data.json</code>, loaded over HTTP  </li> <li>Moved from Python based visual tools to Cytoscape.js for performance and portability  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-07-16","title":"2025-07-16","text":""},{"location":"dev-recap_dev_diary/#sccm-node-type-update-for-yaml","title":"SCCM node type update for YAML","text":"<ul> <li>Standardised entity types to quoted form such as <code>@type: 'PERSON'</code> to avoid YAML parsing quirks  </li> <li>Change rippled through validators and graph builder  </li> <li>Highlighted need for stronger YAML schema checks later  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-07-14","title":"2025-07-14","text":""},{"location":"dev-recap_dev_diary/#fixing-schema-to-graph-disconnects","title":"Fixing schema to graph disconnects","text":"<ul> <li>Found broken links and dangling edges from mismatched service and organisation IDs  </li> <li>Rewrote edge building logic to verify both subject and object exist before creating edge  </li> <li>Added clearer error messages when relationships skipped  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-07-09","title":"2025-07-09","text":""},{"location":"dev-recap_dev_diary/#visual-and-navigational-structure","title":"Visual and navigational structure","text":"<ul> <li>Reorganised <code>/docs/</code> folder to avoid old MkDocs sprawl  </li> <li>Added grouped navigation for tools, scrapes and thematic areas such as Early Help, SEND, Benchmarking  </li> <li>Refined script that auto generates <code>mkdocs.yml</code> navigation from folder structure  </li> </ul>"},{"location":"dev-recap_dev_diary/#2025-07-03","title":"2025-07-03","text":""},{"location":"dev-recap_dev_diary/#csv-and-contact-data-integration","title":"CSV and contact data integration","text":"<ul> <li>Added merge step for contact lists from different sources, for example Wix exports and curated CSVs  </li> <li>Normalised email domains and lowercased fields  </li> <li>Started mapping people entities for wider ecosystem graph  </li> </ul>"},{"location":"dev-recap_dev_diary/#what-i-tried-that-did-not-work","title":"What I tried that did not work","text":"<ul> <li>Single giant JSON with full metadata, slow parse, slow paint, brittle caching  </li> <li>Edge labels enabled at load, browser struggled once graph neared five thousand nodes  </li> <li>Hover cards on mouse move, too many DOM changes and jitter on large graphs  </li> <li>Live GitHub API calls from client, rate limits and unauth flows, unreliable on Pages  </li> <li>Embedding full text for search inside graph JSON, payload size exploded, hurt first paint  </li> <li>Parquet on Pages, not supported, kept only as local build artefact  </li> <li>Loading all nodes and edges at once, long white screen and no progress signals  </li> <li>Bezier edges at initial zoom, layout cost too high when zoomed out  </li> </ul>"},{"location":"dev-recap_dev_diary/#what-works-now","title":"What works now","text":"<ul> <li>Split JSON, only bytes needed for current task  </li> <li>Staged loading seeded by degree, fast first paint then gradual reveal  </li> <li>Labels off until zoom, bezier edges after zoom, smoother interaction  </li> <li>Side panel version 2, type agnostic field bag, fewer re renders  </li> <li>Search with operators and URL state, easier sharing of views  </li> <li>Context mode keeps neighbours during filtering, supports sense making  </li> <li>Local Python pipeline writes minified deploy assets plus optional pretty debug files  </li> <li>Static JSON for feedback and lists, no client GitHub API  </li> </ul>"},{"location":"dev-recap_dev_diary/#files-and-purpose","title":"Files and purpose","text":"<ul> <li><code>data_externally_processed/</code> - Precomputed search corpus from PDFs etc, feeds semantic search  </li> <li><code>data_externally_processed/motw_chunks.parquet</code> - Text chunks table, one row per chunk, used to reconstruct context windows  </li> <li><code>data_externally_processed/motw_index.faiss</code> - Vector index, fast nearest neighbour search over <code>motw_vectors</code> </li> <li><code>data_externally_processed/motw_vectors.parquet</code> - Embeddings table, one row per chunk, mirrors <code>motw_chunks</code> row ids  </li> <li> <p><code>data_externally_processed/search_index.json</code> - Lightweight search index export, usable by static site or potentialy API tools  </p> </li> <li> <p><code>data_repos/</code> - Upstream Git repos pulled in, extra raw corpora for search and mapping  </p> </li> <li><code>data_web/</code> - External source configs, reproducible scraping   </li> <li><code>data_yml/</code> - Core SCCM metadata, single source of truth for graph nodes and details  </li> <li> <p><code>docs/data/</code> - Build outputs for front end JavaScript, not hand edited in normal flow </p> </li> <li> <p><code>docs/data/adjacency.json</code> - Node adjacency map, used for neighbour lookups and context mode hints  </p> </li> <li><code>docs/data/crosswalk.json</code> - Id and slug to page path mapping, joins YAML ids to MkDocs pages and URLs  </li> <li><code>docs/data/csc_artifacts/</code> - Packaged semantic search artefacts for public site or LA notebooks  </li> <li><code>docs/data/csc_artifacts/motw_chunks.parquet</code> - Deployed chunk table, used by external notebooks or future on device search  </li> <li><code>docs/data/csc_artifacts/motw_index.faiss</code> - Deployed ANN index, supports offline or local vector queries  </li> <li><code>docs/data/csc_artifacts/motw_vectors.parquet</code> - Deployed embeddings, aligned with chunks, future friendly for new tools  </li> <li> <p><code>docs/data/csc_artifacts/state.json</code> - Metadata for chunks, vectors and index, shapes, versions, build state  </p> </li> <li> <p><code>docs/data/degree.json</code> - Node degree per id, used for sizing or dimming nodes, supports optimisation and QA  </p> </li> <li><code>docs/data/graph_data.json</code> - Full node and edge payload for standard graph view, richest graph version  </li> <li><code>docs/data/graph_data.lite.json</code> - Reduced graph payload, trimmed fields or nodes for faster load and lighter demos  </li> <li><code>docs/data/graph_search_index.json</code> - Search index focused on graph nodes, drives graph explorer search UI  </li> <li><code>docs/data/lite_index.json</code> - Tiny index for lite graph, quick lookup of ids, slugs and basic labels  </li> <li><code>docs/data/node_details.json</code> - Per node detail blob, side panel reads from here instead of YAML at runtime  </li> <li><code>docs/data/related_nodes.json</code> - Precomputed related suggestions, powers show related and context recommendations  </li> <li><code>docs/data/search_index.json</code> - Site wide search index, complements MkDocs default, used by custom search tooling  </li> <li><code>docs/data/source_nodes.dict.json</code> - Source to nodes mapping in dictionary shape, handy for scripts and analysis  </li> <li><code>docs/data/source_nodes.json</code> - Canonical mapping of sources to node lists, used by optimisation and QA views  </li> <li> <p><code>docs/data/source_nodes.list.json</code> - List shaped variant of source to nodes mapping, easier to scan during dev  </p> </li> <li> <p><code>docs/data/feedback-items.json</code> - Static feedback items for site, replaces live GitHub API calls  </p> </li> <li> <p><code>docs/data/graph_data.json</code> - Renderable graph core, shared across main network views  </p> </li> <li><code>scripts/*.py</code> - Local build scripts, drive YAML validation, graph build and index generation  </li> </ul>"},{"location":"dev-recap_dev_diary/#build-pipeline-local","title":"Build pipeline, local","text":"<ol> <li>Read YAML sources for all types and validate fields  </li> <li>Generate graph structures, compute degree, neighbours and crosswalk  </li> <li>Build search index, tokenise, normalise, trim to compact form  </li> <li>Write minified JSON to <code>docs/data</code>, optionally write pretty copies for review  </li> <li>Run local smoke test, start MkDocs preview, check first paint and zoom behaviour  </li> <li>Commit JSON artefacts, scripts and docs, push to GitHub, Pages serves static site  </li> </ol>"},{"location":"dev-recap_dev_diary/#performance-notes","title":"Performance notes","text":"<ul> <li>Aim for first content visible fast, show small seed subgraph before full graph  </li> <li>Keep JSON small, strip unused fields, prefer integer ids and short keys  </li> <li>Use cache friendly paths under <code>docs/data</code> </li> <li>Avoid heavy synchronous work on client, defer expensive transforms until user zooms in  </li> <li>Prefer haystack edges at low zoom, switch to bezier when detail matters  </li> <li>Never show thousands of labels at once  </li> </ul>"},{"location":"dev-recap_dev_diary/#lessons-learned","title":"Lessons learned","text":"<ul> <li>Design for static hosting from start, no hidden servers  </li> <li>Split concerns, separate files for render, search and lookups  </li> <li>Build locally where tools are rich, publish only what browsers need  </li> <li>Test with large graphs early, set label thresholds using data  </li> <li>Keep UX simple, side panel better than hover flood, context mode helps exploration  </li> </ul>"},{"location":"dev-recap_dev_diary/#next","title":"Next","text":"<ul> <li>Optional worker to pre compute layouts for very large subgraphs  </li> <li>Light theming pass for accessibility, including high contrast toggle  </li> <li>More examples in docs, plus short how to videos  </li> </ul>"},{"location":"explore/","title":"CSC Network Search First (Pre-Alpha)","text":"<p>Conceptual visualisation of related CSC data work, tools, projects, guidance, published reports, relevant Github repos, notable LA and Gov public cloud content and key people, inclusive of a searchable supporting information sub-layer and graph node meta-data. </p> <p>This map and the supporting data is in developmental pre-alpha build state. Data and structures are part of ongoing development and subject to daily change.</p> Search: Keep neighbours (append) Clear all"},{"location":"graph_filtering_guidance/","title":"Graph Filtering Guide","text":"<p>Make the most of the network graph using type filters, a search box, and an optional context mode that reveals nearby connections. This page explains how it works and gives practical examples.</p>"},{"location":"graph_filtering_guidance/#quick-start","title":"Quick start","text":"<ul> <li>Type filter (chips/select): show only Organizations, Events, Plans, etc.</li> <li>Search box: find nodes by name, tags, summary text, or slug (we index these into a <code>search_blob</code> for fast matching).</li> <li>Context mode: optionally keep neighbours of matches visible so you can see how results connect.</li> <li>Shareable views: your current filter state is encoded in the URL (types + query), so you can copy the link and share what you\u2019re seeing.</li> <li>Legend counts: the legend shows live totals for visible nodes by type.</li> <li>Reset: click Reset view to clear filters and fit the graph.</li> </ul>"},{"location":"graph_filtering_guidance/#how-filtering-works","title":"How filtering works","text":"<p>Filtering is intersection-based:</p> <ul> <li>The type filter (e.g. Organizations + Events) selects what kinds of nodes are considered.</li> <li>The search box narrows this further to nodes whose text matches your query.</li> <li>The result is Type AND Search.</li> </ul> <p>Edges remain visible only if both endpoints are visible. This offers scope to reduce less relevant showing, keep the visualisation cleaner and relevant.</p>"},{"location":"graph_filtering_guidance/#what-text-is-searchable","title":"What text is searchable?","text":"<p>Each node has a lightweight <code>search_blob</code> built at publish time, which includes: - Name/label - Tags - Summary/description (shortened) - Slug (path-like identifier) - Type (e.g. <code>organization</code>, <code>event</code>)</p> <p>The search matches plain words anywhere in that blob.</p>"},{"location":"graph_filtering_guidance/#debounced-typing","title":"Debounced typing","text":"<p>When you type in the search box, the filter doesn\u2019t run on every keystroke. It waits ~150 ms for a short pause before applying the filter. </p> <p>Why we've done this: As we scale up the visualised network(graph), filtering can get expensive (show/hide many nodes, recompute edges, update legend). Debouncing reduces unnecessary work and makes your searches smooth(er) but importantly reduces our background processing overheads - making your interactions more rapid. </p> <p>Example: typing <code>ilacs</code> would normally trigger 5 full filter runs. With debounce, you typically get just 1\u20132 runs total.</p>"},{"location":"graph_filtering_guidance/#context-mode-keep-neighbours-of-matches-visible","title":"Context mode (keep neighbours of matches visible)","text":"<p>If Context mode is ON, then when your search finds some nodes, we also show their immediate neighbours and the edges between them. That gives you a mini ego-network around each hit, which we think offers better network exploration/search relevance.</p> <p>Example: search <code>tag:ilacs</code>. You\u2019ll see the nodes that are tagged <code>ilacs</code>, plus the organizations that run them, related events, or connected plans\u2014so you understand each hit in context.</p> <p>Turn it OFF when you want a strict, minimal view showing only the direct matches.</p>"},{"location":"graph_filtering_guidance/#simple-search-operators-power-users","title":"Simple search operators (power users)","text":"<p>You can mix operators with plain keywords to express intent quickly:</p> <ul> <li><code>type:org</code> \u2014 show organizations (same as picking Organizations in the type filter).</li> <li><code>tag:ilacs</code> \u2014 show nodes with the tag <code>ilacs</code>.</li> <li><code>type:event tag:training</code> \u2014 events tagged <code>training</code>.</li> <li><code>tag:ilacs charity</code> \u2014 nodes tagged <code>ilacs</code> and whose text contains <code>charity</code>.</li> <li><code>type:org type:event</code> \u2014 organizations or events (operator values are ORed).</li> </ul> <p>Rules: - Within the same operator (e.g., multiple <code>type:</code>), values are ORed. - Across different buckets (type, tag, and plain text), conditions are ANDed. - Text tokens (non-operator words) must all appear somewhere in the node\u2019s <code>search_blob</code>.</p> <p>Aliases: <code>type:org</code>, <code>type:organisation</code>, and <code>type:organization</code> are treated the same.</p>"},{"location":"graph_filtering_guidance/#combining-filters-examples","title":"Combining filters: examples","text":"<ul> <li> <p>Only organizations with ILACS in the text   Type filter: Organizations; Search: <code>ilacs</code> Shows only orgs whose name/summary/tags include \u201cilacs\u201d.</p> </li> <li> <p>Any node tagged ILACS that mentions Ofsted   Type filter: All; Search: <code>tag:ilacs ofsted</code> Matches the ILACS tag AND the word \u201cofsted\u201d appears in the node\u2019s text.</p> </li> <li> <p>Events or Organizations related to training   Type filter: All; Search: <code>type:event type:org training</code> Matches if the node is an Event OR Organization, AND contains \u201ctraining\u201d.</p> </li> <li> <p>Strict list without neighbours   Toggle Context mode OFF; Search: <code>tag:procurement framework</code> Only nodes that match will display (no neighbours). Useful for tight lists.</p> </li> </ul>"},{"location":"graph_filtering_guidance/#url-state-and-sharing","title":"URL state and sharing","text":"<p>When you change filters or search, the page updates the URL with your current types and query. Copy the URL to share your exact view with someone else, including the existing zoom/pan context.</p>"},{"location":"graph_filtering_guidance/#legend-counts","title":"Legend counts","text":"<p>The legend shows the number of visible nodes per type. Counts update as you filter so you can see how your query impacts the mix.</p>"},{"location":"graph_filtering_guidance/#resetting","title":"Resetting","text":"<p>Use the Reset view button to: - Clear the type filter and search query - Remove match highlighting - Fit the graph to the viewport</p>"},{"location":"graph_filtering_guidance/#troubleshooting-tips","title":"Troubleshooting tips","text":"<ul> <li>Nothing shows up? Clear the search box, check the type chips, or hit Reset view.</li> <li>Too much disappears when I type? Turn Context mode ON to keep neighbours of hits visible.</li> <li>I can\u2019t find a node I know exists. Try broader terms, check spelling, or search by <code>tag:</code> if you know one.</li> <li>Sharing a view. Copy the page URL after you\u2019ve set filters; it contains your state.</li> </ul> <p>This graph is built from YAML entities (Organization, Event, Plan, etc.) with extra build-time fields for search (slug, search blob). The filters you see operate entirely in the browser for quick, privacy-friendly exploration.</p>"},{"location":"network/","title":"CSC Network SCCM Filter (Pre-Alpha)","text":"<p>Conceptual visualisation of related CSC data work, tools, projects, guidance, published reports, relevant Github repos, notable LA and Gov public cloud content and key people, inclusive of a searchable supporting information sub-layer and graph node meta-data. </p> <p>This map and the supporting data is in developmental pre-alpha build state. Data and structures are part of ongoing development and subject to daily change.</p> Search: Keep neighbours (context) Filter by node type(s): Organizations Plans Events Services Reset View Quick tips for filtering <p>Free text matches the node\u2019s name, tags, and summary.</p> <ul> <li><code>tag:&lt;word&gt;</code> \u2014 match nodes with that tag (e.g. <code>tag:ilacs</code>)</li> <li><code>type:&lt;kind&gt;</code> \u2014 restrict by type (<code>type:org</code>, <code>type:plan</code>, <code>type:event</code>, <code>type:service</code>)</li> <li>Combine terms: <code>tag:ilacs type:org</code> (all terms must match)</li> <li>Context mode: keeps neighbours of matches visible for exploration</li> <li>Filters + search intersect (both must match)</li> <li>Share state: copy the URL (types and query persist in the hash)</li> </ul> <p>Examples</p> <ul> <li><code>ilacs</code> \u2014 any node mentioning \u201cilacs\u201d</li> <li><code>tag:children_services</code> \u2014 nodes tagged \u201cchildren_services\u201d</li> <li><code>type:org dfe</code> \u2014 organisation nodes mentioning \u201cdfe\u201d</li> <li><code>tag:data_tools type:service</code> \u2014 services tagged \u201cdata_tools\u201d</li> </ul> <p></p>"},{"location":"network_fullscreen/","title":"CSC Network Full (Pre-Alpha)","text":""},{"location":"sccm_relation_types/","title":"CSC Network Relationships","text":"<p>This is a reference page detailing the scope of relationship types defined within the Smart City Concept Model (SCCM) as applied to this tools Children's Social Care CSC Network Diagram. These relationships are reproduced directly from the Smart City Concept Model definitions, but shown here for each relevant object type towards additional clarity on the derived network diagram (generated from the core YAML definitions within this tool).</p> Service type relationships SubjectRelationshipObject ServicecontainsService ServiceinfluencedByObjective ServiceprovidedByAgent ServiceresponsibilityOfAgent ServiceserviceImplementsMethodMethod ServiceusedByCommunity ServicesubjectOfAgreement ServicecontainedInService ServicecontainedInFunction ServiceraisesCase ServiceusesResourceResource ServicehasRuleRule Event type relationships SubjectRelationshipObject EventatPlacePlace EventhasOutcomeState EventcontainedInCase EventcontainedInAccount EventhasRoleFromItem EventhasOutcomeDecision EventeventPlannedInPlan Plan type relationships SubjectRelationshipObject PlancontainsPlan PlanhasTargetTarget PlaninfluencedByObjective PlanplanDerivedFromMethodMethod PlanplanForEventEvent PlanplanForCaseCase PlancontainedInPlan PlanplanOfAgent PlanusesResourceResource Community type relationships SubjectRelationshipObject CommunitycontainsCommunity CommunitycontainedInCommunity CommunityusesService Organization type relationships SubjectRelationshipObject OrganizationcontainsOrganization OrganizationcontainedInOrganization OrganizationhasMemberPerson Person type relationships SubjectRelationshipObject PersonmemberOfOrganization Rule type relationships SubjectRelationshipObject RuleruleForService Resource type relationships SubjectRelationshipObject ResourceresourceForService ResourceresourceForPlan ResourceresourceOfAgent Collection type relationships SubjectRelationshipObject CollectioncollectionContainsItem CollectioncollectionDefinedByAgent Agent type relationships SubjectRelationshipObject AgenthasObject AgenthasAbstract AgenthasAgreementAgreement AgenthasObjectiveObjective AgenthasPlanPlan AgenthasResourceResource AgenttakesDecisionDecision AgentusesItem AgentmakesAssumptionAssumption AgentdefinesCollectionCollection AgentownsAccount AgentprovidesService AgentresponsibleForService"},{"location":"search/","title":"Search","text":"Search the CSC Network Knowledge Base <p> Reference detail of the search index scope and the search strategy applied here.</p> <p>JavaScript is required to use search function.</p>"},{"location":"search_pipeline/","title":"Search Strategy","text":""},{"location":"search_pipeline/#full-text-search-strategy-optimised-for-static-hosting","title":"Full-Text Search Strategy (Optimised for Static Hosting)","text":"<p>To support search across the contents of thousands of documents and varied sources (including PDFs) within our MkDocs-based site, we're exploring approaches that can simulate full-text search while remaining scalable, performant, and static-host compatible (i.e. no backend/db, fully Git contained).</p>"},{"location":"search_pipeline/#main-goals","title":"Main Goals","text":"<ul> <li>Allow users to search document content (not just titles)</li> <li>Ensure performance remains fast even as thousands of documents/sources added</li> <li>Avoid full-text duplication or payload bloat in both frontend and backend(to avoid hitting Git/responsiveness limits)</li> <li>Keep compatibility with static site hosting (e.g. GitHub Pages, <code>list.js</code>, <code>lunr.js</code>, Mkdocs)</li> </ul>"},{"location":"search_pipeline/#optimisation-strategy","title":"Optimisation Strategy","text":"<p>Rather than store the entire text of every PDF in the search index (slower and rapidly bloated), we're exploring approaches based around:</p> <ol> <li>Extract Text from PDFs </li> <li>Using <code>pdfplumber</code>, each document\u2019s text extracted and cleaned</li> <li> <p>Unicode characters like curly quotes, dashes, ellipses are normalised for consistency</p> </li> <li> <p>Generate Excerpts </p> </li> <li>Shorter <code>excerpt</code>s are created from (near)first meaningful paragraph (trying to ignore TOCs and headings)</li> <li> <p>This then contributes to search results and graph visualisation tooltips</p> </li> <li> <p>Lemmatise and Tokenise Keywords </p> </li> <li>Words are lemmatised (e.g. running, ran, runs \u2192 run) using <code>nltk</code></li> <li> <p>Thus creating/aiming for a usable compressed keyword representation of each document</p> </li> <li> <p>Apply Document Frequency Filtering </p> </li> <li>Use <code>CountVectorizer</code> from <code>scikit-learn</code>:<ul> <li>Remove overly common terms (in progress but approximately: appear in &gt;85% of docs)</li> <li>Remove rare noise terms (appear in &lt;2 docs)</li> </ul> </li> <li> <p>This results in signal-rich keywords per document</p> </li> <li> <p>Final Search Index Generation </p> </li> <li>Each optimised document (content) is then represented in <code>docs/search_index.json</code> with:<ul> <li><code>doc_id</code>, <code>name</code>, <code>excerpt</code>, <code>url</code>, <code>tags</code>, and optimised <code>keywords</code></li> </ul> </li> <li>At runtime, search results display:<ul> <li>Matched titles</li> <li>Decoded excerpts</li> <li>A prioritised and randomly sampled subset of keywords (max 20) to avoid repetition or alphabetical bias</li> </ul> </li> </ol>"},{"location":"search_pipeline/#note-ongoing-work-towards-the-above-is-a-core-under-pinning-part-of-the-d2i-backend-work-towards-an-efficient-csc-map","title":"Note: Ongoing work towards the above is a core, under-pinning part of the D2I backend work towards an efficient CSC Map.","text":""},{"location":"search_pipeline/#optional-archive-parquet","title":"Optional Archive (Parquet)","text":"<p>For potential archival or later ML processing if needed, a full <code>.parquet</code> file is made available  (<code>admin_scripts/docs_index.parquet</code>) with complete text and metadata (disabled by default to keep project lightweight during development, but a variable flag can bve set in the py index build script to generate this for interested LA colleagues who might be able to further use it).</p>"},{"location":"search_pipeline/#key-libraries-used","title":"Key Libraries Used","text":"Purpose Tool PDF text extraction <code>pdfplumber</code> Text cleanup <code>re</code>, <code>unicodedata</code>, <code>DOMParser</code> (JS) Lemmatisation &amp; stopwords <code>nltk</code> Frequency filtering <code>scikit-learn</code> (<code>CountVectorizer</code>) Output formats <code>json</code>, <code>pandas</code>, <code>parquet</code> <p>This setup we think ensures search is (acceptably)fast, useful, and scalable \u2014 and that the project can grow without sacrificing performance or frontend simplicity. We're in the process of scaling this up for more complete/realistic testing alongside a cyclic review approach. </p>"},{"location":"sources/","title":"Data Sources","text":"<p>_To add transparency to the search in particular, below is the current scope of the input data/sources. Some of the labelling comes direct/dynamically off the sources themselves and may therefore be inconsistent. _</p>"},{"location":"sources/#sccm-aligned-yaml-metadata-55-sources","title":"SCCM-aligned YAML Metadata (~55 sources)","text":""},{"location":"sources/#events","title":"events","text":"Source File Type Word Count Last Refreshed childrens_social_care_in_england_2025 .yaml - 02/12/2025 childrens_social_care_review .yaml - 02/12/2025 childrens_wellbeing_schools_bill_2024_25 .yaml - 02/12/2025 nvest1 .yaml - 02/12/2025 nvest2 .yaml - 02/12/2025 nvest3 .yaml - 02/12/2025"},{"location":"sources/#organizations","title":"organizations","text":"Source File Type Word Count Last Refreshed adcs .yaml - 02/12/2025 coram .yaml - 02/12/2025 csdug .yaml - 02/12/2025 data_to_insight .yaml - 02/12/2025 department_for_education .yaml - 02/12/2025 east_sussex_county_council .yaml - 22/12/2025 essex_county_council .yaml - 02/12/2025 hertfordshire_county_council .yaml - 02/12/2025 knowsley_council .yaml - 02/12/2025 lancaster_university .yaml - 02/12/2025 lga .yaml - 02/12/2025 research_in_practice .yaml - 02/12/2025 social_finance .yaml - 02/12/2025 test_0001 .yaml - 22/12/2025 test_0002 .yaml - 22/12/2025 test_0003 .yaml - 22/12/2025 test_0004 .yaml - 22/12/2025 test_0005 .yaml - 22/12/2025 test_0006 .yaml - 22/12/2025 test_0007 .yaml - 22/12/2025 test_0008 .yaml - 22/12/2025 test_0009 .yaml - 22/12/2025 test_0010 .yaml - 22/12/2025 test_0011 .yaml - 22/12/2025 test_0012 .yaml - 22/12/2025 test_0013 .yaml - 22/12/2025 test_0014 .yaml - 22/12/2025 test_0015 .yaml - 22/12/2025 test_0016 .yaml - 22/12/2025 test_0017 .yaml - 22/12/2025 test_0018 .yaml - 22/12/2025 test_0019 .yaml - 22/12/2025 test_0020 .yaml - 22/12/2025"},{"location":"sources/#plans","title":"plans","text":"Source File Type Word Count Last Refreshed childrens_social_care_national_framework .yaml - 02/12/2025 ddsf .yaml - 22/12/2025 ffp_programme_guide_2025 .yaml - 02/12/2025 national_kinship_care_strategy .yaml - 02/12/2025 nvest .yaml - 02/12/2025"},{"location":"sources/#resources","title":"resources","text":"Source File Type Word Count Last Refreshed ddsf2a_social_workers_and_cms_constraints .yaml - 02/12/2025"},{"location":"sources/#rules","title":"rules","text":"Source File Type Word Count Last Refreshed childrens_act_1989 .yaml - 02/12/2025 cin_census_data_requirements .yaml - 02/12/2025 keeping_children_safe_in_education_2025 .yaml - 02/12/2025"},{"location":"sources/#services","title":"services","text":"Source File Type Word Count Last Refreshed centre_of_excellence .yaml - 02/12/2025 d2i_apprenticeships .yaml - 02/12/2025 d2i_data_validators .yaml - 02/12/2025 d2i_excel_toolkit_maintenance .yaml - 02/12/2025 east_sussex_county_council_childrens_services_send .yaml - 22/12/2025 single_point_of_advice .yaml - 22/12/2025 standard_safeguarding_dataset .yaml - 22/12/2025"},{"location":"sources/#public-web-data-6-sources","title":"Public Web Data (~6 sources)","text":""},{"location":"sources/#data_web","title":"data_web","text":"Source File Type Word Count Last Refreshed adcsorguk-adcs-safeguarding-page .txt 843 02/12/2025 barnetgovuk-barnet-resource .txt 310 02/12/2025 bathnesgovuk-bath-and-north-east-somerset-resource .txt 720 02/12/2025 childrensservicesnetwork-childrens-services-network .txt 28 02/12/2025 govuk-department-for-education .txt 1510 02/12/2025 lbbdgovuk-barking-and-dagenham-resource .txt 317 02/12/2025"}]}